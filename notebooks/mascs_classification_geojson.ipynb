{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title \n",
    "\n",
    "**Automated surface mapping via unsupervised learning and classification of Mercury Visible--Near-Infrared reflectance spectra**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T14:24:33.808982Z",
     "iopub.status.busy": "2022-12-20T14:24:33.808666Z",
     "iopub.status.idle": "2022-12-20T14:24:33.814773Z",
     "shell.execute_reply": "2022-12-20T14:24:33.814260Z",
     "shell.execute_reply.started": "2022-12-20T14:24:33.808957Z"
    }
   },
   "source": [
    "## Abstract\n",
    "\n",
    "In this work we apply unsupervised learning techniques for  dimensionality reduction and clustering to remote sensing  hyperspectral Visible-Near Infrared (VNIR) reflectance spectra  datasets of the planet Mercury obtained by the MErcury Surface, Space  ENvironment, GEochemistry, and Ranging (MESSENGER) mission.\n",
    "This  approach produces cluster maps, which group different regions of the  surface based on the properties of their spectra as inferred during  the learning process.\n",
    "While results depend on the choice of model  parameters and available data, comparison to expert-generated geologic  maps shows that some clusters correspond to expert-mapped classes such  as smooth plains on Mercury.\n",
    "These automatically generated maps can  serve as a starting point or comparison for traditional methods of  creating geologic maps based on spectral patterns.\n",
    "The code and data  used in this work is available as python jupyter notebook on the  github public repository  [MESSENGER-Mercury-Surface-Cassification-Unsupervised_DLR](https://github.\n",
    "com/epn-ml/MESSENGER-Mercury-Surface-Cassification-Unsupervised_DLR)[^1] funded by the European Union's Horizon 2020 grant No 871149.\n",
    "\n",
    "Authors:\n",
    "- Mario D'Amore$^1$\n",
    "- Sebastiano Padovan$^{1,2,3}$\n",
    "\n",
    "Affiliations : \n",
    "\n",
    "-  $^1$German Aerospace Center (DLR), Rutherfordstraße 2, 12489 Berlin,Germany\n",
    "-  $^2$EUMETSAT, Eumetsat Allee 1, 64295 Darmstadt, Germany\n",
    "-  $^3$WGS, Berliner Allee 47, 64295 Darmstadt, Germany\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T14:30:53.678822Z",
     "iopub.status.busy": "2022-12-20T14:30:53.678517Z",
     "iopub.status.idle": "2022-12-20T14:30:53.692968Z",
     "shell.execute_reply": "2022-12-20T14:30:53.692251Z",
     "shell.execute_reply.started": "2022-12-20T14:30:53.678798Z"
    }
   },
   "source": [
    "## [Introduction](#sec:4b.intro)\n",
    "\n",
    "The sheer amount of data returned by scientific missions aimed at\n",
    "exploring the solar system and observing exoplanets in recent decades\n",
    "overwhelms classical methods to explore and discover important\n",
    "scientific aspects of the target body. As an example, the Mercury data\n",
    "return for Mariner 10 was less than 100 MB, while MESSENGER delivered\n",
    "about 23 TB. Future missions are expected to exceed this limit. In\n",
    "addition, there is a trend of increasing complexity in the data itself,\n",
    "e.g., going from the of Mariner-10 to the hyperspectral datasets\n",
    "expected from BepiColombo. This situation clearly indicates that some\n",
    "form of automated analysis would be beneficial, provided it is able to\n",
    "save time without a loss of the information content of the data.\n",
    "\n",
    "Keeping the focus on hyperspectral remote sensing data, the typical\n",
    "approach for analysing this kind of data is to model the observed\n",
    "radiation with a forward radiative model[like Hapke, as in\n",
    "@Hamilton2005] or attempt to reproduce the observed radiation by setting\n",
    "up relevant samples in a laboratory setting using chemical and/or\n",
    "geomorphological context information.[e.g., @Helbert2013] Complex\n",
    "forward models that are able to take into account the relevant physics\n",
    "are typically computationally intensive and difficult to use to\n",
    "investigate the very large parameter space covered by hyperspectral\n",
    "data. This consideration is even more relevant for laboratory\n",
    "investigations : physical simulation needs the target to be physically\n",
    "fabricated, hence more and and more parameters means more experiments\n",
    "and more time. Models need computational power to be calculated in a\n",
    "reasonable amount of time, but could be distributed on several machines\n",
    "to overcome this limitation. This workaround is not effective for\n",
    "laboratory experiment, because most only few places meets of the\n",
    "environment needed for space sample simulation, like high-vacuum,\n",
    "-temperature, -radiation and so on.\n",
    "\n",
    "Without a way to efficiently and rapidly explore large amounts of\n",
    "complex data, it is likely that valuable information will be missed in\n",
    "large hyperspectral data sets.\n",
    "\n",
    "Geological maps are the gold standard for remote planetary surface\n",
    "studies, but producing them is an extremely time-consuming task. This\n",
    "process can suffer from user bias and typically only uses a few data\n",
    "points (e.g., 3-channel images) to describe different units. For\n",
    "example,[@Denevi2009] mapped the distribution and extent of major\n",
    "terrain types of Mercury using MESSENGER Mercury Dual Imaging System\n",
    "(MDIS) camera observations of Mercury. While the camera has 11 spectral\n",
    "bands, the maps typically used for the terrain differentiation are RGB,\n",
    "where 3 representative spectral bands are mapped onto the three image\n",
    "color channels.\n",
    "\n",
    "Geomorphological maps take in account additional features like surface\n",
    "roughness and crater density as a proxy for the age, where the\n",
    "correlation between age and crater density are derived from\n",
    "models.[e.g., @blandCraterCounting2003; @kerrWhoCanRead2006] Automated\n",
    "techniques are becoming more common in planetary science applications,\n",
    "as this books testifies, and the aim of this chapter is to illustrate\n",
    "how to apply unsupervised learning techniques to remote sensing data.\n",
    "This approach requires minimal user interaction and yields\n",
    "scientifically interesting products like classification maps that can be\n",
    "directly compared with geomorphological maps and models. We present an\n",
    "analysis of spectral reflectance data of Mercury's surface collected by\n",
    "the Mercury Atmospheric and Surface Composition Spectrometer (MASCS)\n",
    "instrument during orbital observations of the NASA MESSENGER mission\n",
    "between 2011 and 2015.[@McClintock2007] MASCS is a three sensor point\n",
    "spectrometer with a spectral coverage from 200 nm to 1450 nm. After a\n",
    "brief overview of the instrument and its significance for the\n",
    "investigation of Mercury (section\n",
    "[2](#sec:4b.mercury_mascs){reference-type=\"ref\"\n",
    "reference=\"sec:4b.mercury_mascs\"}), we will illustrate how we extract\n",
    "and resample the data to a format useful for our ML application (section\n",
    "[3](#sec:4b.dataprep){reference-type=\"ref\"\n",
    "reference=\"sec:4b.dataprep\"}). Then we show how to compress the data\n",
    "(section\n",
    "[4.1](#sec:4b.dimensionality_reduction_ica){reference-type=\"ref\"\n",
    "reference=\"sec:4b.dimensionality_reduction_ica\"}), how to project them\n",
    "to a lower number of dimensions (section\n",
    "[4.2](#sec:4b.manifold_learning){reference-type=\"ref\"\n",
    "reference=\"sec:4b.manifold_learning\"}), and finally, how to group\n",
    "\"similar\" data points together to discover salient spectral classes and\n",
    "their distribution on the surface. We conclude in section\n",
    "[4.4](#sec:4b.conclusion){reference-type=\"ref\"\n",
    "reference=\"sec:4b.conclusion\"} by providing a basic comparison of the\n",
    "result of the discovered spectral class distribution with maps of the\n",
    "surface of Mercury obtained using classical methods, in order to provide\n",
    "a first assessment of the machine learning techniques presented here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the case, i.e. [Bandfield2000](#ref-Bandfield2000) did found spectral classes on Mars and\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T14:30:53.678822Z",
     "iopub.status.busy": "2022-12-20T14:30:53.678517Z",
     "iopub.status.idle": "2022-12-20T14:30:53.692968Z",
     "shell.execute_reply": "2022-12-20T14:30:53.692251Z",
     "shell.execute_reply.started": "2022-12-20T14:30:53.678798Z"
    }
   },
   "source": [
    "## [Mercury and the MASCS instrument](#sec:4b.mercury_mascs)\n",
    "\n",
    "Surface mineralogy and composition are important indicators of the past\n",
    "of a planetary body, since they provide hints about the processes that\n",
    "formed and altered the crust, which is largely the result of the\n",
    "interior evolution. For example, the possibility of identifying specific\n",
    "mineral assemblage like metamorphic rocks, which are known to form in\n",
    "specific pressure and temperature conditions, would provide indications\n",
    "on the physical processes occurring in the subsurface that produced\n",
    "those rocks and later transported the rocks to the surface.[e.g.,\n",
    "@namurSilicateMineralogySurface2017] Similarly, observations of hydrated\n",
    "minerals can be interpreted as indicating the possible past presence of\n",
    "water, as in the case of Mars.[@meslinSoilDiversityHydration2013]\n",
    "\n",
    "While some investigations have been published on Mercury's surface\n",
    "mineralogy,[e.g.,\n",
    "@e.vanderkaadenGeochemistryMineralogyPetrology2017; @namurSilicateMineralogySurface2017; @Vilas2016a; @Sprague2009]\n",
    "its link to the endogenous (e.g., mantle convection) and exogenous\n",
    "(e.g., impacts) processes that operated during the history of the planet\n",
    "is still difficult to elucidate.[e.g.,\n",
    "@padovanImpactinducedChangesSource2017] A relevant example is the\n",
    "geological features known as hollows, discovered on the surface of\n",
    "Mercury in MESSENGER data. Hollows are rimless depressions with flat\n",
    "floors, surrounded by halos of high-albedo material, and typically found\n",
    "in clusters.[@blewett2011hollows] Given this evidence, their formation\n",
    "mechanism likely includes the loss of volatile material through one or\n",
    "more processes such as sublimation, space weathering, outgassing, or\n",
    "pyroclastic flow. Hollows are associated with a particular spectral\n",
    "signature in MESSENGER's MDIS camera,[@Vilas2016a] but a specific\n",
    "spectral signature in spectrometer data could not be identified due to\n",
    "the coarse spatial resolution of the spectrometer. Overall, the only\n",
    "clear inference based on VNIR spectra obtained by the MASCS instrument\n",
    "is that Mercury's surface shows little variation, displaying no distinct\n",
    "spectral features except for the possible indication of sulfide\n",
    "mineralogy within the hollows.[@Vilas2016a]\n",
    "\n",
    "MASCS consists of a small Cassegrain telescope with an effective focal\n",
    "length of 257 mm and a 50-mm aperture that simultaneously feeds an\n",
    "UltraViolet and Visible Spectrometer (UVVS) and a Visible and InfraRed\n",
    "Spectrograph (VIRS) channel. VIRS is a fixed concave grating\n",
    "spectrograph with a focal length of 210 mm, equipped with a beam\n",
    "splitter that simultaneously disperses the light onto a 512-element\n",
    "sensor (VIS, 300--1050 nm) and a 256-element infrared sensor array (NIR,\n",
    "850--1450 nm). Data obtained by MASCS covers almost the entire surface\n",
    "of Mercury. The spatial resolution is highly latitude dependent due to\n",
    "the very elliptical orbit of the spacecraft, but a reference value\n",
    "$\\sim5$ km. This low spatial resolution is a trade-off for higher\n",
    "spectral resolution and more spectral channels compared to the imaging\n",
    "instruments (i.e., the MDIS).\n",
    "\n",
    "The NIR sensor is characterized by 3 -- 5 times lower signal-to-noise\n",
    "ratios (SNRs) than the VIS detector and does not add significant\n",
    "information to the VIS sensor in our tests. NIR measurement cann be\n",
    "linked and corrected to match corresponding VIS measurements\n",
    "following.[However, see @Besse2015 for a successful VIS/NIR cross\n",
    "correction.] The biggest obstacle is that the the most accurate\n",
    "photometric corrections is only available for the VIS channel.[see.\n",
    "@domingueAnalysisMESSENGERMASCS2019; @domingueAnalysisMESSENGERMASCS2019a]\n",
    "We then analysed only data from VIS channel, that is enough for the sake\n",
    "of illustrating unsupervised learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography(#sec:bibliography)\n",
    "\n",
    "<a target=\"_self\" href=\"#ref-Bandfield2000\">¶</a>\n",
    "Bandfield, JL, VE Hamilton, and PR Christensen. \"A Global View of\n",
    "Martian Surface Compositions from MGS-TES.\" *Science* 287, no. March\n",
    "(2000): 1626--1630. doi:[fjh6x2](https://doi.org/fjh6x2).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {#ref-Besse2015 .csl-entry role=\"doc-biblioentry\"}\n",
    "Besse, S., A. Doressoundiram, and J. Benkhoff. \"Spectroscopic Properties\n",
    "of Explosive Volcanism Within the Caloris Basin with MESSENGER\n",
    "Observations.\" *Journal of Geophysical Research: Planets* 120, no. 12\n",
    "(December 2015): 2102--2117.\n",
    "doi:[10.1002/2015JE004819](https://doi.org/10.1002/2015JE004819).\n",
    ":::\n",
    "\n",
    "::: {#ref-blandCraterCounting2003 .csl-entry role=\"doc-biblioentry\"}\n",
    "Bland, Phil. \"Crater Counting.\" *Astronomy & Geophysics* 44, no. 4\n",
    "(August 2003): 4.21--4.21. doi:[dsw66x](https://doi.org/dsw66x).\n",
    ":::\n",
    "\n",
    "::: {#ref-blewett2011hollows .csl-entry role=\"doc-biblioentry\"}\n",
    "Blewett, D. T., N. L. Chabot, B. W. Denevi, C. M. Ernst, J. W. Head, N.\n",
    "R. Izenberg, S. L. Murchie, et al. \"Hollows on Mercury: MESSENGER\n",
    "Evidence for Geologically Recent Volatile-Related Activity.\" *Science*\n",
    "333, no. 6051 (September 2011): 1856--1859.\n",
    "doi:[d8hhvw](https://doi.org/d8hhvw).\n",
    ":::\n",
    "\n",
    "::: {#ref-coenenUnderstandingUMAP2019a .csl-entry role=\"doc-biblioentry\"}\n",
    "Coenen, Andy, and Adam Pearce. \"Understanding UMAP.\"\n",
    "https://pair-code.github.io/understanding-umap/, 2019.\n",
    ":::\n",
    "\n",
    "::: {#ref-Denevi2013 .csl-entry role=\"doc-biblioentry\"}\n",
    "Denevi, Brett W., Carolyn M. Ernst, Heather M. Meyer, Mark S. Robinson,\n",
    "Scott L. Murchie, Jennifer L. Whitten, James W. Head, et al. \"The\n",
    "Distribution and Origin of Smooth Plains on Mercury.\" *Journal of\n",
    "Geophysical Research: Planets* 118, no. 5 (May 2013): 891--907.\n",
    "doi:[10.1002/jgre.20075](https://doi.org/10.1002/jgre.20075).\n",
    ":::\n",
    "\n",
    "::: {#ref-Denevi2009 .csl-entry role=\"doc-biblioentry\"}\n",
    "Denevi, Brett W., Mark S. Robinson, David T. Blewett, Deborah L.\n",
    "Domingue, James W. Head III, Timothy J. McCoy, Ralph L. McNutt Jr.,\n",
    "Scott L. Murchie, and Sean C. Solomon. \"MESSENGER Global Color\n",
    "Observations: Implications for the Composition and Evolution of\n",
    "Mercury's Crust.\" In *Lunar and Planetary Science Conference*, 1--2,\n",
    "2009.\n",
    ":::\n",
    "\n",
    "::: {#ref-domingueAnalysisMESSENGERMASCS2019 .csl-entry role=\"doc-biblioentry\"}\n",
    "Domingue, Deborah L., Mario D'Amore, Sabrina Ferrari, Jörn Helbert, and\n",
    "Noam R. Izenberg. \"Analysis of the MESSENGER MASCS Photometric Targets\n",
    "Part I: Photometric Standardization for Examining Spectral Variability\n",
    "Across Mercury's Surface.\" *Icarus* 319 (February 2019): 247--263.\n",
    "doi:[gh3dp5](https://doi.org/gh3dp5).\n",
    ":::\n",
    "\n",
    "::: {#ref-domingueAnalysisMESSENGERMASCS2019a .csl-entry role=\"doc-biblioentry\"}\n",
    "---------. \"Analysis of the MESSENGER MASCS Photometric Targets Part II:\n",
    "Photometric Variability Between Geomorphological Units.\" *Icarus* 319\n",
    "(February 2019): 140--246. doi:[gh3dp6](https://doi.org/gh3dp6).\n",
    ":::\n",
    "\n",
    "::: {#ref-donohoHessianEigenmapsLocally2003 .csl-entry role=\"doc-biblioentry\"}\n",
    "Donoho, David L., and Carrie Grimes. \"Hessian Eigenmaps: Locally Linear\n",
    "Embedding Techniques for High-Dimensional Data.\" *Proceedings of the\n",
    "National Academy of Sciences* 100, no. 10 (May 2003): 5591--5596.\n",
    "doi:[cnjc4z](https://doi.org/cnjc4z).\n",
    ":::\n",
    "\n",
    "::: {#ref-e.vanderkaadenGeochemistryMineralogyPetrology2017 .csl-entry role=\"doc-biblioentry\"}\n",
    "E. Vander Kaaden, Kathleen, Francis M. McCubbin, Larry R. Nittler,\n",
    "Patrick N. Peplowski, Shoshana Z. Weider, Elizabeth A. Frank, and\n",
    "Timothy J. McCoy. \"Geochemistry, Mineralogy, and Petrology of Boninitic\n",
    "and Komatiitic Rocks on the Mercurian Surface: Insights into the\n",
    "Mercurian Mantle.\" *Icarus* 285 (March 2017): 155--168.\n",
    "doi:[gg3j22](https://doi.org/gg3j22).\n",
    ":::\n",
    "\n",
    "::: {#ref-Hamilton2005 .csl-entry role=\"doc-biblioentry\"}\n",
    "Hamilton, Victoria E., Harry Y. McSween, and Bruce Hapke. \"Mineralogy of\n",
    "Martian Atmospheric Dust Inferred from Thermal Infrared Spectra of\n",
    "Aerosols.\" *Journal of Geophysical Research* 110, no. E12 (2005): 1--11.\n",
    "doi:[bhsb7j](https://doi.org/bhsb7j).\n",
    ":::\n",
    "\n",
    "::: {#ref-hastieElementsStatisticalLearning2009 .csl-entry role=\"doc-biblioentry\"}\n",
    "Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. *The Elements of\n",
    "Statistical Learning: Data Mining, Inference, and Prediction, Second\n",
    "Edition*. Second. Springer Series in Statistics. New York:\n",
    "Springer-Verlag, 2009.\n",
    "doi:[10.1007/978-0-387-84858-7](https://doi.org/10.1007/978-0-387-84858-7).\n",
    ":::\n",
    "\n",
    "::: {#ref-Helbert2013 .csl-entry role=\"doc-biblioentry\"}\n",
    "Helbert, Jörn, Alessandro Maturilli, Mario D'Amore, and M. D'Amore.\n",
    "\"Visible and Near-Infrared Reflectance Spectra of Thermally Processed\n",
    "Synthetic Sulfides as a Potential Analog for the Hollow Forming\n",
    "Materials on Mercury.\" *Earth and Planetary Science Letters* 369--370\n",
    "(May 2013): 233--238. doi:[gbddt9](https://doi.org/gbddt9).\n",
    ":::\n",
    "\n",
    "::: {#ref-hyvarinenIndependentComponentAnalysis2000 .csl-entry role=\"doc-biblioentry\"}\n",
    "Hyvärinen, A., and E. Oja. \"Independent Component Analysis: Algorithms\n",
    "and Applications.\" *Neural Networks* 13, no. 4 (June 2000): 411--430.\n",
    "doi:[cx35gq](https://doi.org/cx35gq).\n",
    ":::\n",
    "\n",
    "::: {#ref-kerrWhoCanRead2006 .csl-entry role=\"doc-biblioentry\"}\n",
    "Kerr, Richard A. \"Who Can Read the Martian Clock?\" *Science* 312, no.\n",
    "5777 (May 2006): 1132--1133. doi:[b6v8tt](https://doi.org/b6v8tt).\n",
    ":::\n",
    "\n",
    "::: {#ref-leeNonlinearDimensionalityReduction2007 .csl-entry role=\"doc-biblioentry\"}\n",
    "Lee, John A., and Michel Verleysen. *Nonlinear Dimensionality\n",
    "Reduction*. Information Science and Statistics. New York:\n",
    "Springer-Verlag, 2007.\n",
    "doi:[10.1007/978-0-387-39351-3](https://doi.org/10.1007/978-0-387-39351-3).\n",
    ":::\n",
    "\n",
    "::: {#ref-maatenVisualizingDataUsing2008 .csl-entry role=\"doc-biblioentry\"}\n",
    "Maaten, Laurens van der, and Geoffrey Hinton. \"Visualizing Data Using\n",
    "t-SNE.\" *Journal of Machine Learning Research* 9, no. 86 (2008):\n",
    "2579--2605.\n",
    ":::\n",
    "\n",
    "::: {#ref-Maturilli2014a .csl-entry role=\"doc-biblioentry\"}\n",
    "Maturilli, A., J. Helbert, J. M. St. John, J. W. Head, W. M. Vaughan, M.\n",
    "D'Amore, M. Gottschalk, and S. Ferrari. \"Komatiites as Mercury Surface\n",
    "Analogues: Spectral Measurements at PEL.\" *Earth and Planetary Science\n",
    "Letters* 398 (2014). doi:[gg3jwp](https://doi.org/gg3jwp).\n",
    ":::\n",
    "\n",
    "::: {#ref-McClintock2007 .csl-entry role=\"doc-biblioentry\"}\n",
    "McClintock, William E., and Mark R. Lankton. \"The Mercury Atmospheric\n",
    "and Surface Composition Spectrometer for the MESSENGER Mission.\" *Space\n",
    "Science Reviews* 131, no. 1--4 (2007): 481--521.\n",
    "doi:[btc6f6](https://doi.org/btc6f6).\n",
    ":::\n",
    "\n",
    "::: {#ref-mcinnesUMAPUniformManifold2018 .csl-entry role=\"doc-biblioentry\"}\n",
    "McInnes, Leland. \"UMAP: Uniform Manifold Approximation and Projection\n",
    "for Dimension Reduction Umap 0.5 Documentation.\"\n",
    "https://umap-learn.readthedocs.io/en/latest/index.html, 2018.\n",
    ":::\n",
    "\n",
    "::: {#ref-mcinnesUMAPUniformManifold2020 .csl-entry role=\"doc-biblioentry\"}\n",
    "McInnes, Leland, John Healy, and James Melville. \"UMAP: Uniform Manifold\n",
    "Approximation and Projection for Dimension Reduction.\" *arXiv:1802.03426\n",
    "\\[Cs, Stat\\]* (September 2020). <https://arxiv.org/abs/1802.03426>.\n",
    ":::\n",
    "\n",
    "::: {#ref-meslinSoilDiversityHydration2013 .csl-entry role=\"doc-biblioentry\"}\n",
    "Meslin, P.-Y., O. Gasnault, O. Forni, S. Schröder, A. Cousin, G. Berger,\n",
    "S. M. Clegg, et al. \"Soil Diversity and Hydration as Observed by ChemCam\n",
    "at Gale Crater, Mars.\" *Science* 341, no. 6153 (September 2013).\n",
    "doi:[f3sdqx](https://doi.org/f3sdqx).\n",
    ":::\n",
    "\n",
    "::: {#ref-namurSilicateMineralogySurface2017 .csl-entry role=\"doc-biblioentry\"}\n",
    "Namur, Olivier, and Bernard Charlier. \"Silicate Mineralogy at the\n",
    "Surface of Mercury.\" *Nature Geoscience* 10, no. 1 (January 2017):\n",
    "9--13. doi:[f9r3qp](https://doi.org/f9r3qp).\n",
    ":::\n",
    "\n",
    "::: {#ref-nasaPDSPDS3Standards2009 .csl-entry role=\"doc-biblioentry\"}\n",
    "NASA. \"PDS: PDS3 Standards Reference.\"\n",
    "https://pds.nasa.gov/datastandards/pds3/standards/, 2009.\n",
    ":::\n",
    "\n",
    "::: {#ref-Nittler2011 .csl-entry role=\"doc-biblioentry\"}\n",
    "Nittler, L. R., R. D. Starr, S. Z. Weider, T. J. McCoy, W. V. Boynton,\n",
    "D. S. Ebel, C. M. Ernst, et al. \"The Major-Element Composition of\n",
    "Mercury's Surface from MESSENGER X-Ray Spectrometry.\" *Science* 333, no.\n",
    "6051 (September 2011): 1847--1850. doi:[bps3b8](https://doi.org/bps3b8).\n",
    ":::\n",
    "\n",
    "::: {#ref-nittlerGlobalMajorelementMaps2020 .csl-entry role=\"doc-biblioentry\"}\n",
    "Nittler, Larry R., Elizabeth A. Frank, Shoshana Z. Weider, Ellen\n",
    "Crapster-Pregont, Audrey Vorburger, Richard D. Starr, and Sean C.\n",
    "Solomon. \"Global Major-Element Maps of Mercury from Four Years of\n",
    "MESSENGER X-Ray Spectrometer Observations.\" *Icarus* (February 2020):\n",
    "113716. doi:[ggm5sv](https://doi.org/ggm5sv).\n",
    ":::\n",
    "\n",
    "::: {#ref-padovanImpactinducedChangesSource2017 .csl-entry role=\"doc-biblioentry\"}\n",
    "Padovan, Sebastiano, Nicola Tosi, Ana-Catalina Plesa, and Thomas Ruedas.\n",
    "\"Impact-Induced Changes in Source Depth and Volume of Magmatism on\n",
    "Mercury and Their Observational Signatures.\" *Nature Communications* 8,\n",
    "no. 1 (December 2017): 1945. doi:[gcn9p2](https://doi.org/gcn9p2).\n",
    ":::\n",
    "\n",
    "::: {#ref-Peplowski2016 .csl-entry role=\"doc-biblioentry\"}\n",
    "Peplowski, Patrick N., Rachel L. Klima, David J. Lawrence, Carolyn M.\n",
    "Ernst, Brett W. Denevi, Elizabeth A. Frank, John O. Goldsten, Scott L.\n",
    "Murchie, Larry R. Nittler, and Sean C. Solomon. \"Remote Sensing Evidence\n",
    "for an Ancient Carbon-Bearing Crust on Mercury.\" *Nature Geoscience* 9,\n",
    "no. 4 (March 2016): 273--276.\n",
    "doi:[10.1038/ngeo2669](https://doi.org/10.1038/ngeo2669).\n",
    ":::\n",
    "\n",
    "::: {#ref-roweisNonlinearDimensionalityReduction2000 .csl-entry role=\"doc-biblioentry\"}\n",
    "Roweis, Sam T., and Lawrence K. Saul. \"Nonlinear Dimensionality\n",
    "Reduction by Locally Linear Embedding.\" *Science* 290, no. 5500\n",
    "(December 2000): 2323--2326. doi:[cbws2r](https://doi.org/cbws2r).\n",
    ":::\n",
    "\n",
    "::: {#ref-ruixuSurveyClusteringAlgorithms2005 .csl-entry role=\"doc-biblioentry\"}\n",
    "Rui Xu, and D. Wunsch. \"Survey of Clustering Algorithms.\" *IEEE\n",
    "Transactions on Neural Networks* 16, no. 3 (May 2005): 645--678.\n",
    "doi:[c3pfgf](https://doi.org/c3pfgf).\n",
    ":::\n",
    "\n",
    "::: {#ref-Sprague2009 .csl-entry role=\"doc-biblioentry\"}\n",
    "Sprague, A. L., K. L. Donaldson Hanna, R. W. H. Kozlowski, J Helbert, A\n",
    "Maturilli, J. B. Warell, and J. L. Hora. \"Spectral Emissivity\n",
    "Measurements of Mercury's Surface Indicate Mg- and Ca-Rich Mineralogy,\n",
    "K-Spar, Na-Rich Plagioclase, Rutile, with Possible Perovskite, and\n",
    "Garnet.\" *Planetary and Space Science* 57, no. 3 (March 2009): 364--383.\n",
    "doi:[fbd9jq](https://doi.org/fbd9jq).\n",
    ":::\n",
    "\n",
    "::: {#ref-tenenbaumGlobalGeometricFramework2000 .csl-entry role=\"doc-biblioentry\"}\n",
    "Tenenbaum, Joshua B., Vin de Silva, and John C. Langford. \"A Global\n",
    "Geometric Framework for Nonlinear Dimensionality Reduction.\" *Science*\n",
    "290, no. 5500 (December 2000): 2319--2323.\n",
    "doi:[cz8wgk](https://doi.org/cz8wgk).\n",
    ":::\n",
    "\n",
    "::: {#ref-Vasavada1999 .csl-entry role=\"doc-biblioentry\"}\n",
    "Vasavada, a. \"Near-Surface Temperatures on Mercury and the Moon and the\n",
    "Stability of Polar Ice Deposits.\" *Icarus* 141, no. 2 (October 1999):\n",
    "179--193. doi:[b9fhjd](https://doi.org/b9fhjd).\n",
    ":::\n",
    "\n",
    "::: {#ref-Vilas2016a .csl-entry role=\"doc-biblioentry\"}\n",
    "Vilas, Faith, Deborah L. Domingue, Jörn Helbert, Mario D'Amore,\n",
    "Alessandro Maturilli, Rachel L. Klima, Karen R. Stockstill-Cahill, et\n",
    "al. \"Mineralogical Indicators of Mercury's Hollows Composition in\n",
    "MESSENGER Color Observations.\" *Geophysical Research Letters* 43, no. 4\n",
    "(February 2016): 1450--1456. doi:[gg3j2v](https://doi.org/gg3j2v).\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: {#footnotes .section .footnotes .footnotes-end-of-document role=\"doc-endnotes\"}\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "1.  ::: {#fn1}\n",
    "    Like Hapke, as in Hamilton, McSween, and Hapke, \"Mineralogy of\n",
    "    Martian Atmospheric Dust Inferred from Thermal Infrared Spectra of\n",
    "    Aerosols.\"[↩︎](#fnref1){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "2.  ::: {#fn2}\n",
    "    E.g., Helbert et al., \"Visible and Near-Infrared Reflectance Spectra\n",
    "    of Thermally Processed Synthetic Sulfides as a Potential Analog for\n",
    "    the Hollow Forming Materials on Mercury.\"[↩︎](#fnref2){.footnote-back\n",
    "    role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "3.  ::: {#fn3}\n",
    "    Denevi et al., \"MESSENGER Global Color\n",
    "    Observations.\"[↩︎](#fnref3){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "4.  ::: {#fn4}\n",
    "    E.g., Bland, \"Crater Counting\"; Kerr, \"Who Can Read the Martian\n",
    "    Clock?\"[↩︎](#fnref4){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "5.  ::: {#fn5}\n",
    "    McClintock and Lankton, \"The Mercury Atmospheric and Surface\n",
    "    Composition Spectrometer for the MESSENGER\n",
    "    Mission.\"[↩︎](#fnref5){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "6.  ::: {#fn6}\n",
    "    E.g., Namur and Charlier, \"Silicate Mineralogy at the Surface of\n",
    "    Mercury.\"[↩︎](#fnref6){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "7.  ::: {#fn7}\n",
    "    Meslin et al., \"Soil Diversity and Hydration as Observed by ChemCam\n",
    "    at Gale Crater, Mars.\"[↩︎](#fnref7){.footnote-back\n",
    "    role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "8.  ::: {#fn8}\n",
    "    E.g., E. Vander Kaaden et al., \"Geochemistry, Mineralogy, and\n",
    "    Petrology of Boninitic and Komatiitic Rocks on the Mercurian\n",
    "    Surface\"; Namur and Charlier, \"Silicate Mineralogy at the Surface of\n",
    "    Mercury\"; Vilas et al., \"Mineralogical Indicators of Mercury's\n",
    "    Hollows Composition in MESSENGER Color Observations\"; Sprague et\n",
    "    al., \"Spectral Emissivity Measurements of Mercury's Surface Indicate\n",
    "    Mg- and Ca-Rich Mineralogy, K-Spar, Na-Rich Plagioclase, Rutile,\n",
    "    with Possible Perovskite, and Garnet.\"[↩︎](#fnref8){.footnote-back\n",
    "    role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "9.  ::: {#fn9}\n",
    "    E.g., Padovan et al., \"Impact-Induced Changes in Source Depth and\n",
    "    Volume of Magmatism on Mercury and Their Observational\n",
    "    Signatures.\"[↩︎](#fnref9){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "10. ::: {#fn10}\n",
    "    Blewett et al., \"Hollows on Mercury.\"[↩︎](#fnref10){.footnote-back\n",
    "    role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "11. ::: {#fn11}\n",
    "    Vilas et al., \"Mineralogical Indicators of Mercury's Hollows\n",
    "    Composition in MESSENGER Color\n",
    "    Observations.\"[↩︎](#fnref11){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "12. ::: {#fn12}\n",
    "    Ibid.[↩︎](#fnref12){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "13. ::: {#fn13}\n",
    "    However, see Besse, Doressoundiram, and Benkhoff, \"Spectroscopic\n",
    "    Properties of Explosive Volcanism Within the Caloris Basin with\n",
    "    MESSENGER Observations\" for a successful VIS/NIR cross\n",
    "    correction.[↩︎](#fnref13){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "14. ::: {#fn14}\n",
    "    See. Domingue et al., \"Analysis of the MESSENGER MASCS Photometric\n",
    "    Targets Part I\"; Domingue et al., \"Analysis of the MESSENGER MASCS\n",
    "    Photometric Targets Part II.\"[↩︎](#fnref14){.footnote-back\n",
    "    role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "15. ::: {#fn15}\n",
    "    NASA, \"PDS.\"[↩︎](#fnref15){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "16. ::: {#fn16}\n",
    "    We found a GDAL bug when reading 8 bytes real values that is solved\n",
    "    for version [≥]{.math .inline}2.3.0 after our report to the\n",
    "    developer. See <https://trac.osgeo.org/gdal/wiki/Release/2.3.0-News>\n",
    "    and use this version or higher when manipulating MASCS\n",
    "    data.[↩︎](#fnref16){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "17. ::: {#fn17}\n",
    "    PostgreSQL is a relational database management that controls the\n",
    "    creation, integrity, maintenance and use of a\n",
    "    database[↩︎](#fnref17){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "18. ::: {#fn18}\n",
    "    PostGIS adds support for geographic objects in geographic\n",
    "    information system and extends the database language with functions\n",
    "    to create and manipulate geographic objects. PostGIS follows the\n",
    "    Simple Features for SQL specification from the Open Geospatial\n",
    "    Consortium (OGC).[↩︎](#fnref18){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "19. ::: {#fn19}\n",
    "    MASCS VIS data have different wavelength sampling and part of the\n",
    "    global Mercury campaign had different spectral\n",
    "    binning.[↩︎](#fnref19){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "20. ::: {#fn20}\n",
    "    Domingue et al., \"Analysis of the MESSENGER MASCS Photometric\n",
    "    Targets Part I\"; Domingue et al., \"Analysis of the MESSENGER MASCS\n",
    "    Photometric Targets Part II.\"[↩︎](#fnref20){.footnote-back\n",
    "    role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "21. ::: {#fn21}\n",
    "    Hastie, Tibshirani, and Friedman, *The Elements of Statistical\n",
    "    Learning*; Hyvärinen and Oja, \"Independent Component\n",
    "    Analysis.\"[↩︎](#fnref21){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "22. ::: {#fn22}\n",
    "    Tenenbaum, Silva, and Langford, \"A Global Geometric Framework for\n",
    "    Nonlinear Dimensionality Reduction\"; Roweis and Saul, \"Nonlinear\n",
    "    Dimensionality Reduction by Locally Linear Embedding\"; Donoho and\n",
    "    Grimes, \"Hessian Eigenmaps\"; Maaten and Hinton, \"Visualizing Data\n",
    "    Using t-SNE\"; McInnes, Healy, and Melville,\n",
    "    \"UMAP.\"[↩︎](#fnref22){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "23. ::: {#fn23}\n",
    "    Lee and Verleysen, *Nonlinear Dimensionality\n",
    "    Reduction*.[↩︎](#fnref23){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "24. ::: {#fn24}\n",
    "    Roweis and Saul, \"Nonlinear Dimensionality Reduction by Locally\n",
    "    Linear Embedding.\"[↩︎](#fnref24){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "25. ::: {#fn25}\n",
    "    McInnes, Healy, and Melville, \"UMAP.\"[↩︎](#fnref25){.footnote-back\n",
    "    role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "26. ::: {#fn26}\n",
    "    Ibid.[↩︎](#fnref26){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "27. ::: {#fn27}\n",
    "    https://umap-learn.readthedocs.io/en/latest/how_umap_works.html[↩︎](#fnref27){.footnote-back\n",
    "    role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "28. ::: {#fn28}\n",
    "    McInnes, Healy, and Melville, \"UMAP.\"[↩︎](#fnref28){.footnote-back\n",
    "    role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "29. ::: {#fn29}\n",
    "    McInnes, \"UMAP.\"[↩︎](#fnref29){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "30. ::: {#fn30}\n",
    "    The interactive tutorial \\\"Understanding UMAP\\\" give some insight in\n",
    "    how the hyperparameters influence UMAP. See [ (Coenen and Pearce,\n",
    "    \"Understanding UMAP\")]{.citation\n",
    "    cites=\"coenenUnderstandingUMAP2019a\"} and\n",
    "    https://pair-code.github.io/understanding-346umap[↩︎](#fnref30){.footnote-back\n",
    "    role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "31. ::: {#fn31}\n",
    "    see for example \\\"Selecting the number of clusters with silhouette\n",
    "    analysis on KMeans clustering\\\"\n",
    "    <https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html>[↩︎](#fnref31){.footnote-back\n",
    "    role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "32. ::: {#fn32}\n",
    "    Rui Xu and Wunsch, \"Survey of Clustering\n",
    "    Algorithms.\"[↩︎](#fnref32){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "33. ::: {#fn33}\n",
    "    Vasavada, \"Near-Surface Temperatures on Mercury and the Moon and the\n",
    "    Stability of Polar Ice Deposits.\"[↩︎](#fnref33){.footnote-back\n",
    "    role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "34. ::: {#fn34}\n",
    "    Denevi et al., \"The Distribution and Origin of Smooth Plains on\n",
    "    Mercury.\"[↩︎](#fnref34){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "35. ::: {#fn35}\n",
    "    Ibid.[↩︎](#fnref35){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "36. ::: {#fn36}\n",
    "    Bandfield, Hamilton, and Christensen, \"A Global View of Martian\n",
    "    Surface Compositions from MGS-TES.\"[↩︎](#fnref36){.footnote-back\n",
    "    role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "37. ::: {#fn37}\n",
    "    Maturilli et al., \"Komatiites as Mercury Surface\n",
    "    Analogues.\"[↩︎](#fnref37){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "38. ::: {#fn38}\n",
    "    Nittler et al., \"Global Major-Element Maps of Mercury from Four\n",
    "    Years of MESSENGER X-Ray Spectrometer\n",
    "    Observations.\"[↩︎](#fnref38){.footnote-back role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "39. ::: {#fn39}\n",
    "    Peplowski et al., \"Remote Sensing Evidence for an Ancient\n",
    "    Carbon-Bearing Crust on Mercury.\"[↩︎](#fnref39){.footnote-back\n",
    "    role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "40. ::: {#fn40}\n",
    "    Nittler et al., \"The Major-Element Composition of Mercury's Surface\n",
    "    from MESSENGER X-Ray Spectrometry\"; Nittler et al., \"Global\n",
    "    Major-Element Maps of Mercury from Four Years of MESSENGER X-Ray\n",
    "    Spectrometer Observations.\"[↩︎](#fnref40){.footnote-back\n",
    "    role=\"doc-backlink\"}\n",
    "    :::\n",
    "\n",
    "41. ::: {#fn41}\n",
    "    https://github.com/epn-ml/MESSENGER-Mercury-Surface-Cassification-Unsupervised_DLR[↩︎](#fnref41){.footnote-back\n",
    "    role=\"doc-backlink\"}\n",
    "    :::\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class=\"unnumbered\" id=\"sec:bibliography\">Bibliography</h2>\n",
    "<div id=\"refs\" class=\"references csl-bib-body hanging-indent\"\n",
    "role=\"doc-bibliography\">\n",
    "<div id=\"ref-Bandfield2000\" class=\"csl-entry\" role=\"doc-biblioentry\">\n",
    "Bandfield, JL, VE Hamilton, and PR Christensen. <span>“A <span>Global\n",
    "View</span> of <span>Martian Surface Compositions</span> from\n",
    "<span>MGS</span>-<span>TES</span>.”</span> <em>Science</em> 287, no.\n",
    "March (2000): 1626–1630. doi:<a\n",
    "href=\"https://doi.org/fjh6x2\">fjh6x2</a>.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports\n",
    "\n",
    "Generic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:29:29.298095Z",
     "start_time": "2020-02-04T12:29:23.562215Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import fiona\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg') # non interactive\n",
    "#%matplotlib qt # for not-notebook\n",
    "%matplotlib inline\n",
    "from   matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "matplotlib.rcParams['xtick.labelsize'] = 16\n",
    "matplotlib.rcParams['ytick.labelsize'] = 16\n",
    "matplotlib.rcParams['axes.titlesize'] = 24\n",
    "matplotlib.rcParams['axes.labelsize'] = 20\n",
    "matplotlib.rcParams['font.size'] = 16\n",
    "\n",
    "pd.set_option('display.width',150)\n",
    "pd.set_option('display.max_colwidth',150)\n",
    "pd.set_option('display.max_rows',150)\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:29:29.298095Z",
     "start_time": "2020-02-04T12:29:23.562215Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datashader as ds\n",
    "import holoviews as hv\n",
    "import holoviews.operation.datashader as hds\n",
    "import hvplot.pandas\n",
    "# hv.extension('bokeh','matplotlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define auxiliary functions & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:29:29.376411Z",
     "start_time": "2020-02-04T12:29:29.367525Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_path = pathlib.Path('..')                     # local base path\n",
    "input_data_path = base_path / 'data/processed'  # input data location \n",
    "out_figure_path = base_path / 'reports/figures' # output location <- CHANGE THIS TO YOUR LIKING\n",
    "out_models_path = base_path / 'models' # output location <- CHANGE THIS TO YOUR LIKING\n",
    "\n",
    "print(f'{base_path=}')\n",
    "print(f'{input_data_path=}')\n",
    "print(f'{out_models_path=}')\n",
    "\n",
    "\n",
    "# this globally saves all generated plot with save_plot defind below\n",
    "save_plots_bool = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:29:29.341273Z",
     "start_time": "2020-02-04T12:29:29.305706Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a figure\n",
    "def make_map(in_data,alpha,norm,interpolation=None):\n",
    "    '''\n",
    "    outdf_gdf.loc[spectral_df_nona_index,'R'] = spectral_df[in_wav]\n",
    "    '''\n",
    "    \n",
    "    outdf_gdf.loc[spectral_df_nona_index,'R'] = in_data\n",
    "\n",
    "    map_crs = ccrs.PlateCarree(central_longitude=0.0)\n",
    "    fig, ax = plt.subplots(nrows=1,ncols=1,figsize=[18,18],subplot_kw={'projection': map_crs})\n",
    "    background = ax.imshow(img,\n",
    "                            cmap=plt.cm.gray,\n",
    "                            extent=img_extent,\n",
    "                            origin='upper',\n",
    "                            transform=ccrs.PlateCarree(central_longitude=0.0)\n",
    "                          );\n",
    "    # Here we are using the numpy reshape because we now the final image shape: it is not always the case!!\n",
    "    # This is FASTER then Geopandas.GeoDataFrame.plot!!!!\n",
    "    im = ax.imshow(outdf_gdf.sort_index()['R'].values.reshape(360,180).T,\n",
    "               interpolation= interpolation,\n",
    "               extent= data_img_extent,\n",
    "               cmap=plt.cm.Spectral_r,\n",
    "               transform=ccrs.PlateCarree(central_longitude=0.0),\n",
    "               origin='upper',\n",
    "               alpha=alpha,\n",
    "               # vmax=0.065,\n",
    "               norm=norm,\n",
    "                  );\n",
    "    return im\n",
    "\n",
    "def save_plot(out_file,\n",
    "              output_dir = out_figure_path,\n",
    "              dpi=150,\n",
    "              out_format='jpg',\n",
    "              save=False):\n",
    "    ''' helper function to save previous plot\n",
    "    '''\n",
    "    \n",
    "    out_path = output_dir / (out_file +f'.{out_format}' )\n",
    "    if save :\n",
    "        plt.savefig(out_path,dpi=dpi)\n",
    "        return (f'Saving image to {out_path}')\n",
    "    else:\n",
    "        return (f'NOT saving image to {out_path}')  \n",
    "\n",
    "def df_shader(in_data,**kwargs):\n",
    "    '''\n",
    "    accept input : spectral_df[in_wav] \n",
    "    add it to outdf_gdf:\n",
    "    outdf_gdf.loc[spectral_df_nona_index,'R'] = spectral_df[in_wav]\n",
    "    \n",
    "    returns shader(hv.Points(outdf_gdf.loc[spectral_df_nona_index,'R']))\n",
    "    '''\n",
    "    vdims = 'R'\n",
    "    kdims=[('x','longitude'),('y','latitude')]\n",
    "    outdf_gdf.loc[spectral_df_nona_index,'R'] = in_data\n",
    "\n",
    "    return shader(hv.Points(outdf_gdf.loc[spectral_df_nona_index,['x','y','R']],kdims=kdims,vdims=vdims).opts(),**kwargs).opts()\n",
    "\n",
    "\n",
    "def df_rasterer(in_data,**kwargs):\n",
    "    '''\n",
    "    accept input : spectral_df[in_wav] \n",
    "    add it to outdf_gdf:\n",
    "    outdf_gdf.loc[spectral_df_nona_index,'R'] = spectral_df[in_wav]\n",
    "    \n",
    "    returns shader(hv.Points(outdf_gdf.loc[spectral_df_nona_index,'R']))\n",
    "    '''\n",
    "    vdims = 'R'\n",
    "    kdims=[('x','longitude'),('y','latitude')]\n",
    "    outdf_gdf.loc[spectral_df_nona_index,'R'] = in_data\n",
    "\n",
    "    return rasterer(hv.Points(outdf_gdf.loc[spectral_df_nona_index,['x','y','R']],kdims=kdims,vdims=vdims).opts(),**kwargs).opts()\n",
    "\n",
    "\n",
    "def shader(ppoints,aggregator=ds.mean(),x_sampling=1,y_sampling=1,cmap=plt.cm.Spectral_r, dynamic=True):\n",
    "    return hds.shade(\n",
    "                hds.rasterize(ppoints,\n",
    "                              aggregator=aggregator,\n",
    "                              x_sampling=x_sampling,\n",
    "                              y_sampling=y_sampling),\n",
    "                              cmap=cmap,\n",
    "                              dynamic=dynamic,\n",
    "                              )\n",
    "\n",
    "def rasterer(ppoints,aggregator=ds.mean(),x_sampling=2,y_sampling=2,dynamic=True):\n",
    "    return hds.rasterize(ppoints,\n",
    "                         aggregator=aggregator,\n",
    "                         x_sampling=x_sampling,\n",
    "                         y_sampling=y_sampling,\n",
    "                         dynamic=dynamic,\n",
    "                        )\n",
    "\n",
    "\n",
    "def colorbar_img_shader(in_data,cmap=plt.cm.Spectral_r,**kwargs):\n",
    "    \n",
    "    raster = df_rasterer(in_data,**kwargs).opts(alpha=1,colorbar=True,cmap=cmap)\n",
    "    \n",
    "    return hv.Overlay([raster,\n",
    "                        hds.shade(raster,cmap=cmap,group='datashaded'),\n",
    "                        background.opts(cmap='Gray',alpha=0.5),\n",
    "                      ])\n",
    "# .collate()\n",
    "\n",
    "def find_nearest_in_array(\n",
    "                 value,\n",
    "                 array,\n",
    "                 return_value=False):\n",
    "    \"\"\"Find nearest value in a numpy array\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    value : value to search for\n",
    "    array : array to search in, should be sorted.\n",
    "    return_value : bool, if to return the actual values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "##TODO add sorting check/option to sort ? make sense?\n",
    "    import numpy as np\n",
    "\n",
    "    closest_index = (np.abs(array - value)).argmin()\n",
    "\n",
    "    if value > np.nanmax(array):\n",
    "        import warnings\n",
    "        warnings.warn(f'value > np.nanmax(array) : {value} > {np.nanmax(array)}')\n",
    "\n",
    "    if value < np.nanmin(array):\n",
    "        import warnings\n",
    "        warnings.warn(f'value < np.nanmin(array) : {value} < {np.nanmax(array)}')\n",
    "\n",
    "    if not return_value: \n",
    "        return closest_index\n",
    "    else:\n",
    "        return closest_index, array[closest_index]\n",
    "\n",
    "\n",
    "def get_mascs_geojson(file, gzipped=True):\n",
    "    \"\"\"return a geopandas.DataFrame from a geojson, could be compressed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    file : path of geojson compressed file to geopandas dataframe\n",
    "    gzipped : bool, if compressed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    geopandas.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    import geopandas as gpd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import gzip\n",
    "    from geopandas import GeoDataFrame\n",
    "\n",
    "    if gzipped:\n",
    "        # get the compressed geojson\n",
    "        with gzip.GzipFile(file, 'r') as fin:\n",
    "            geodata = json.loads(fin.read().decode('utf-8'))\n",
    "    else:\n",
    "        # get the uncompressed geojson\n",
    "        with open(file, 'r') as fin:\n",
    "            geodata = json.load(fin)\n",
    "\n",
    "    import shapely\n",
    "    # extract geometries\n",
    "    geometries = [shapely.geometry.Polygon(g['geometry']['coordinates'][0]) for g in geodata['features']]\n",
    "    # extract id\n",
    "    ids = [int(g['id']) for g in geodata['features']]\n",
    "    # generate GeoDataFrame\n",
    "    out_gdf = gpd.GeoDataFrame(data=[g['properties'] for g in geodata['features']], geometry=geometries, index=ids).sort_index()\n",
    "    # cast arrays to numpy\n",
    "    if 'array' in out_gdf:\n",
    "        out_gdf['array'] = out_gdf['array'].apply(lambda x: np.array(x).astype(float))\n",
    "\n",
    "    return out_gdf\n",
    "\n",
    "#CRS from  https://github.com/Melown/vts-registry/blob/master/registry/registry/srs.json\n",
    "mercury_crs = {\n",
    "    \"geographic-dmercury2000\": {\n",
    "        \"comment\": \"Geographic, DMercury2000 (iau2000:19900)\",\n",
    "        \"srsDef\": \"+proj=longlat +a=2439700 +b=2439700 +no_defs\",\n",
    "        \"type\": \"geographic\"\n",
    "    },\n",
    "    \"geocentric-dmercury2000\": {\n",
    "        \"comment\": \"Geocentric, Mercury\",\n",
    "        \"srsDef\": \"+proj=geocent +a=2439700 +b=2439700 +lon_0=0 +units=m +no_defs\",\n",
    "        \"type\": \"cartesian\"\n",
    "    },\n",
    "    \"eqc-dmercury2000\": {\n",
    "        \"comment\": \"Equidistant Cylindrical, DMercury2000 (iau2000:19911)\",\n",
    "        \"srsDef\": \"+proj=eqc +lat_ts=0 +lat_0=0 +lon_0=0 +x_0=0 +y_0=0 +a=2439700 +b=2439700 +units=m +no_defs\",\n",
    "        \"type\": \"projected\"\n",
    "    },\n",
    "    \"merc-dmercury2000\": {\n",
    "        \"comment\": \"Mercator, DMercury2000 (iau2000:19974)\",\n",
    "        \"srsDef\": \"+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +a=2439700 +b=2439700 +units=m +no_defs\",\n",
    "        \"type\": \"projected\"\n",
    "    },\n",
    "    \"steren-dmercury2000\": {\n",
    "        \"comment\": \"Polar Sterographic North, DMercury2000 (iau2000:19918)\",\n",
    "        \"srsDef\": \"+proj=stere +lat_0=90 +lat_ts=90 +lon_0=0 +k=1 +x_0=0 +y_0=0 +a=2439700 +b=2439700 +units=m +no_defs\",\n",
    "        \"type\": \"projected\"\n",
    "    },\n",
    "    \"steres-dmercury2000\": {\n",
    "        \"comment\": \"Polar Stereographic South, DMercury2000 (iau2000:19920)\",\n",
    "        \"srsDef\": \"+proj=stere +lat_0=-90 +lat_ts=-90 +lon_0=0 +k=1 +x_0=0 +y_0=0 +a=2439700 +b=2439700 +units=m +no_defs\",\n",
    "        \"type\": \"projected\"\n",
    "}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:29:29.387455Z",
     "start_time": "2020-02-04T12:29:29.381055Z"
    },
    "tags": []
   },
   "source": [
    "## Load Data\n",
    "\n",
    "Data are too big to be included in this repo, user can find it on [Zenodo](https://zenodo.org/record/7433033) at [https://zenodo.org/record/7433033](https://zenodo.org/record/7433033).\n",
    "\n",
    "Download the datafile `grid_2D_-180_+180_-90_+90_1deg_st_median_photom_iof_sp_2nm.geojson.gz` in  `data/processed` with some variation of\n",
    "\n",
    "```bash\n",
    "curl https://zenodo.org/record/7433033/files/grid_2D_0_360_-90_%2B90_1deg_st_median_photom_iof_sp_2nm.png --output data/processed/grid_2D_-180_+180_-90_+90_1deg_st_median_photom_iof_sp_2nm.geojson.gz\n",
    "```\n",
    "\n",
    "This is a preview of the data cube from Zenodo.\n",
    "\n",
    "![Preview od the data cube from Zenodo](https://zenodo.org/api/iiif/v2/c98bb0bc-cfa1-449e-94f7-95f9d074543e:f4cc114a-fac9-42b8-b5a8-380901fe8dba:grid_2D_0_360_-90_%2B90_1deg_st_median_photom_iof_sp_2nm.png/full/750,/0/default.png)\n",
    "\n",
    "Create the wavelenghts array and its helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:29:29.387455Z",
     "start_time": "2020-02-04T12:29:29.381055Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the wavelenghts array\n",
    "wav_grid_2nm = np.arange(260,1052,2)\n",
    "\n",
    "# define find_nearest, with wav_grid_2nm as default array\n",
    "find_nearest = lambda x : find_nearest_in_array(x,wav_grid_2nm)\n",
    "\n",
    "# this is to index an array based on wav_grid_2nm\n",
    "wavelenght = 415\n",
    "print(f'         wavelenght = {wavelenght:5d} <-- number we search for')\n",
    "print(f'find_nearest({wavelenght:5d}) = {find_nearest(wavelenght):5d} <-- index of {wavelenght} in wav_grid_2nm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:29:29.399368Z",
     "start_time": "2020-02-04T12:29:29.394878Z"
    },
    "execution": {
     "iopub.execute_input": "2022-05-24T08:38:14.625138Z",
     "iopub.status.busy": "2022-05-24T08:38:14.624914Z",
     "iopub.status.idle": "2022-05-24T08:38:14.628307Z",
     "shell.execute_reply": "2022-05-24T08:38:14.627826Z",
     "shell.execute_reply.started": "2022-05-24T08:38:14.625122Z"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Select input data. From MASCS documentation :  \n",
    "\n",
    "```\n",
    "PHOTOM_IOF_SPECTRUM_DATA : \n",
    "   Derived column of photometrically normalized \n",
    "   reflectance-at-sensor spectra. One row per spectrum. NIR spectrum has up \n",
    "   to 256 values (depending on binning and windowing), VIS has up to 512. \n",
    "   Reflectance is a unitless parameter. Reflectance from saturated pixels, \n",
    "   or binned pixels with one saturated element, are set to 1e32. PER \n",
    "   SPECTRUM column.\"\n",
    "\n",
    "IOF_SPECTRUM_DATA : \n",
    "   DESCRIPTION = \"Derived column of reflectance-at-sensor spectra. One \n",
    "   row per spectrum. NIR spectrum has up to 256 values (depending on binning \n",
    "   and windowing), VIS has up to 512. Reflectance is a unitless parameter. \n",
    "   Reflectance from saturated pixels, or binned pixels with one saturated \n",
    "   element, are set to 1e32. PER SPECTRUM column.\"\n",
    "```\n",
    "\n",
    "define the datafile with filename structure:\n",
    "\n",
    "```\n",
    "[description from database]_[function applied to the spectra for each pixel]_[data array used]\n",
    "    [description from database] = grid_2D_0_360_-90_+90\n",
    "    [function applied to the spectra for each pixel] = avg or st_median\n",
    "    [data array used] = iof_sp_2nm or photom_iof_sp_2nm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:29:29.399368Z",
     "start_time": "2020-02-04T12:29:29.394878Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_data_name = 'grid_2D_-180_+180_-90_+90_1deg_st_median_photom_iof_sp_2nm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are in a gzipped geojson to reduce size, but geopandas doesn't like it.\n",
    "\n",
    "The function below accept a path and return a GeoDataFrame.\n",
    "\n",
    "An optional `gzipped[=True default]` keywords take care of compressed geojson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:29:47.380230Z",
     "start_time": "2020-02-04T12:29:29.404990Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "outdf_gdf = get_mascs_geojson( input_data_path / (input_data_name+'.geojson.gz'), gzipped=True)\n",
    "# outdf_gdf = get_mascs_geojson( input_data_path / (input_data_name+'.geojson.gz'), gzipped=True, cast_to_numeric=False)\n",
    "\n",
    "# this is to be sure that the cells are ordered in natural way == reshape with numpy\n",
    "outdf_gdf = outdf_gdf.set_index('natural_index',drop=True).sort_index()\n",
    "import fiona.crs\n",
    "\n",
    "# set Mercury Lat/Lon as crs\n",
    "outdf_gdf.crs = fiona.crs.from_string(mercury_crs['geographic-dmercury2000']['srsDef'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unravel spectral reflectance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:29:58.859511Z",
     "start_time": "2020-02-04T12:29:47.385237Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create wavelenghts columns: this create empy columns with np.nan (nice!)\n",
    "# use a separate df, because mixed types columns are crazy. and buggy\n",
    "spectral_df = pd.DataFrame(index=outdf_gdf.index,columns = wav_grid_2nm).fillna(np.nan)\n",
    "## assign single wavelenght to columns, only where array vectors len !=0\n",
    "spectral_df.loc[outdf_gdf['array'].apply(lambda x : len(x)) != 0, wav_grid_2nm] = np.stack(outdf_gdf.loc[outdf_gdf['array'].apply(lambda x : len(x)) != 0,'array'], axis=0).astype(np.float64)\n",
    "## drop array column\n",
    "outdf_gdf.drop(columns=['array'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:13.478705Z",
     "start_time": "2020-02-04T12:29:58.864249Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create x and y cols = lon and lat\n",
    "outdf_gdf['x'] = outdf_gdf.apply(lambda x: x['geometry'].centroid.x , axis=1)\n",
    "outdf_gdf['y'] = outdf_gdf.apply(lambda x: x['geometry'].centroid.y , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:19.913044Z",
     "start_time": "2020-02-04T12:30:13.491986Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop outlier : this clean up further noisy data, instrumental effect, etc\n",
    "print(spectral_df.shape)\n",
    "low = .02\n",
    "high = .999\n",
    "quant_df = spectral_df.quantile([low, high])\n",
    "spectral_df =  spectral_df[spectral_df >= 0].apply(lambda x: x[(x>quant_df.loc[low,x.name]) &\\\n",
    "                                       (x < quant_df.loc[high,x.name])], axis=0)\\\n",
    "                                       \n",
    "print(spectral_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:19.923399Z",
     "start_time": "2020-02-04T12:30:19.917834Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cut to stop_wav\n",
    "# iloc doesn't support int columns indexing!!!!\n",
    "start_wav = 268 # below all NaN\n",
    "stop_wav  = 975 # above a bump in NaN \n",
    "\n",
    "spectral_df = spectral_df.iloc[:,find_nearest(start_wav):find_nearest(stop_wav)+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:20.736248Z",
     "start_time": "2020-02-04T12:30:19.928238Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count nan\n",
    "print(f'{spectral_df.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:20.736248Z",
     "start_time": "2020-02-04T12:30:19.928238Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2,figsize=(19,1.5))\n",
    "\n",
    "spectral_df.isna().sum(axis=0).plot(ax=axs[0]);\n",
    "spectral_df.isna().sum(axis=1).plot(ax=axs[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:04:34.277075Z",
     "start_time": "2020-02-04T13:04:14.474084Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "print(spectral_df.shape, spectral_df.dropna(axis=0,how='any').shape)\n",
    "\n",
    "display(spectral_df.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:04:34.277075Z",
     "start_time": "2020-02-04T13:04:14.474084Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# whole data distribution \n",
    "fig, axarr = plt.subplots(nrows=1, ncols=2,figsize=(19,2))\n",
    "sns.histplot(spectral_df.dropna(how='any').values.flatten(), ax= axarr[0],bins = 255,alpha=0.4, kde=True,edgecolor='none');\n",
    "sns.histplot(spectral_df.dropna(how='any').values.flatten(), ax= axarr[1],bins = 255,alpha=0.4, kde=True,edgecolor='none',log_scale=(False, True),);\n",
    "\n",
    "save_plot('whole_data_distribution_seaborn',save=save_plots_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:22.611537Z",
     "start_time": "2020-02-04T12:29:21.322Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = spectral_df.dropna(how='any').values.T\n",
    "\n",
    "from skimage import exposure\n",
    "\n",
    "# Equalization : paramterless\n",
    "plt.figure(figsize=[24,8]);\n",
    "plt.imshow(exposure.equalize_hist(img),\n",
    "           interpolation='bicubic',\n",
    "           aspect='auto',\n",
    "           cmap=plt.cm.Spectral_r);\n",
    "# [Colorbar Tick Labelling Demo — Matplotlib 3.1.2 documentation](https://matplotlib.org/3.1.1/gallery/ticks_and_spines/colorbar_tick_labelling_demo.html)\n",
    "cbar = plt.colorbar(ticks=[0.01, 0.5, 1], orientation='vertical')\n",
    "cbar.ax.set_yticklabels(\n",
    "    np.around([np.percentile(img,0.01), np.median(img), np.nanmax(img)],decimals=3)\n",
    "    );\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_plot('spectrogram',save=save_plots_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:04:34.533701Z",
     "start_time": "2020-02-04T13:04:34.282887Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keep the old name? naa\n",
    "spectral_df_nona_index = spectral_df.dropna(how='any').index\n",
    "\n",
    "print(f'        outdf_gdf : {outdf_gdf.shape}')\n",
    "print(f'      spectral_df : {spectral_df.shape}')\n",
    "print(f'spectral_df_nonan : {spectral_df_nona_index.shape}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:04:34.569542Z",
     "start_time": "2020-02-04T13:04:34.538642Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define 2 wavelenghts and calculate something \n",
    "in_wav = 450\n",
    "en_wav = 1050\n",
    "\n",
    "idx_in = find_nearest(in_wav)\n",
    "idx_en = find_nearest(en_wav)\n",
    "\n",
    "outdf_gdf.loc[spectral_df_nona_index,'refl'] = spectral_df[in_wav]\n",
    "\n",
    "print(f'(wav[{idx_in}],wav[{idx_en}]) = ({wav_grid_2nm[idx_in]}, {wav_grid_2nm[idx_en]}) \\nspectral[:,idx_in:idx_en].shape : {spectral_df.loc[:,in_wav:en_wav].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:04:34.580107Z",
     "start_time": "2020-02-04T13:04:34.575398Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate rows & cols from grid properties files, assuming regular grid\n",
    "rows, cols = 360, 180\n",
    "rows_half_step, cols_half_step = 1, 1\n",
    "\n",
    "data_img_extent = [-180.0, 180.0, -90.0, 90.0]\n",
    "\n",
    "# extent = [outdf_gdf.total_bounds[i] for i in [0,2,1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:22.626542Z",
     "start_time": "2020-02-04T12:29:21.337Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specific wavelengths data distribution \n",
    "\n",
    "fig, axarr = plt.subplots(nrows=1, ncols=1,figsize=(20,4))\n",
    "\n",
    "plot_wav = 300\n",
    "ax = sns.histplot(spectral_df[plot_wav].dropna(),ax= axarr,bins = 255,stat=\"density\", alpha=0.4, kde=True,edgecolor='none');\n",
    "ax.set_xlim([0.005,0.075])\n",
    "plot_wav = 700\n",
    "sns.histplot(spectral_df[plot_wav].dropna(),ax= axarr,bins = 255,stat=\"density\", alpha=0.4, kde=True,edgecolor='none');\n",
    "plot_wav = 900\n",
    "sns.histplot(spectral_df[plot_wav].dropna(),ax= axarr,bins = 255,stat=\"density\", alpha=0.4, kde=True,edgecolor='none');\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_plot('global_300nm_700nm_900nm_distribution', out_format='png',save=save_plots_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:04:39.721458Z",
     "start_time": "2020-02-04T13:04:39.578146Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "# from scipy import misc\n",
    "import imageio\n",
    "from skimage import transform \n",
    "# read background image\n",
    "img = imageio.imread( input_data_path / '1280x640_20120330_monochrome_basemap_1000mpp_equirectangular.png')\n",
    "img_extent = (-180, 180, -90, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:04:39.993323Z",
     "start_time": "2020-02-04T13:04:39.837695Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "\n",
    "from PIL import Image, ImageEnhance\n",
    "kdims=[('x','latitude'),('y','longitude')]\n",
    "\n",
    "backimage = Image.open( input_data_path / '1280x640_20120330_monochrome_basemap_1000mpp_equirectangular.png')\n",
    "## from help :  as four-tuple defining the (left, bottom, right and top) edges.\n",
    "hv_img_extent = (-180, -90,180, 90)\n",
    "\n",
    "background = hv.Image(\n",
    "            np.array(backimage),\n",
    "            bounds=hv_img_extent,\n",
    "            kdims=kdims,\n",
    "            group='backplane',\n",
    "            ).opts(cmap='Gray',clone=False)\n",
    "\n",
    "# # show the background image\n",
    "# background.options(width=800,height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "map_crs = ccrs.PlateCarree(central_longitude=0.0)\n",
    "fig, ax = plt.subplots(nrows=1,ncols=1,figsize=[18,18],subplot_kw={'projection': map_crs})\n",
    "background = ax.imshow(img,\n",
    "                        cmap=plt.cm.gray,\n",
    "                        extent=img_extent,\n",
    "                        origin='upper',\n",
    "                        transform=ccrs.PlateCarree(central_longitude=0.0)\n",
    "                      );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:22.641152Z",
     "start_time": "2020-02-04T12:29:21.354Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('matplotlib')\n",
    "hv.output(fig='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_shader((spectral_df[970]-spectral_df[270])/700.)#.opts(fig_inches=4, aspect=2,fig_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#     vdims = 'R'\n",
    "#     kdims=[('x','longitude'),('y','latitude')]\n",
    "#     outdf_gdf.loc[spectral_df_nona_index,'R'] = in_data\n",
    "\n",
    "#     return shader(hv.Points(outdf_gdf.loc[spectral_df_nona_index,['x','y','R']],kdims=kdims,vdims=vdims).opts(),**kwargs).opts()\n",
    "\n",
    "spectral_df[970]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:22.641152Z",
     "start_time": "2020-02-04T12:29:21.354Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spectral slope R[970]-R[270] / 270 -970\n",
    "(background.opts(cmap='Gray',alpha=0.5)*\\\n",
    " df_shader((spectral_df[970]-spectral_df[270])/700.,cmap=plt.cm.Spectral_r).opts(interpolation='bilinear',alpha=0.5)).\\\n",
    "    opts(\n",
    "    fig_inches=4, aspect=2,fig_size=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:22.648536Z",
     "start_time": "2020-02-04T12:29:21.363Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('matplotlib')\n",
    "plot_wav = 700\n",
    "\n",
    "out = colorbar_img_shader(spectral_df[plot_wav])\n",
    "_ = out.DynamicMap.II.opts(interpolation='None',aspect=1.8,fig_size=200,alpha=0.7)\n",
    "out\n",
    "# hv.save(out,out_figure_path / '1b_mascs_700nm_refl.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:22.652101Z",
     "start_time": "2020-02-04T12:29:21.367Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "background.opts(cmap='Gray')*df_shader(spectral_df[plot_wav],cmap=plt.cm.inferno).opts(height=600,width=1000,alpha=0.7)\n",
    "\n",
    "out = colorbar_img_shader(spectral_df[plot_wav])\n",
    "_ = out.DynamicMap.II.opts(height=400,width=800,alpha=0.75)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T12:50:03.942765Z",
     "start_time": "2020-02-05T12:50:03.753437Z"
    },
    "lines_to_next_cell": 2,
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# small_data == spectral_df[spectral_df_nonan]\n",
    "X = spectral_df.loc[spectral_df_nona_index]\n",
    "\n",
    "print(f'        outdf_gdf : {outdf_gdf.shape}')\n",
    "print(f'      spectral_df : {spectral_df.shape}')\n",
    "print(f'spectral_df_nonan : {spectral_df_nona_index.shape}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction\n",
    "\n",
    "[2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 0.20.2 documentation](https://scikit-learn.org/stable/modules/decomposition.html#decompositions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:04:52.140269Z",
     "start_time": "2020-02-04T13:04:52.136044Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from sklearn import model_selection\n",
    "from sklearn import pipeline, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T09:42:52.968069Z",
     "start_time": "2020-01-30T09:42:44.955965Z"
    },
    "hidden": true,
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Principal components analysis : which is the data dimensionality?\n",
    "\n",
    "# n_components = spectral_df.shape[1]//2\n",
    "# pca = decomposition.PCA(n_components=n_components)\n",
    "pca = decomposition.PCA(0.95)\n",
    "pca.fit(X)\n",
    "n_components = pca.n_components_\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "print('X.shape               : {}\\n'\n",
    "      'X_pca.shape           : {}\\n'\n",
    "      'pca.components_.shape : {}'.format(X.shape, X_pca.shape, pca.components_.shape))\n",
    "\n",
    "print(\"              variance       var_ratio      cum_var_ratio\")\n",
    "for i in range(n_components):\n",
    "    print(\"Component %2s: %12.10f   %12.10f   %12.10f\" % (i, pca.explained_variance_[i], pca.explained_variance_ratio_[i], np.cumsum(pca.explained_variance_ratio_)[i]))\n",
    "\n",
    "\n",
    "# As we can see, only the 2 first components are useful\n",
    "# pca.n_components = 2\n",
    "# small_data_pca = pca.fit_transform(small_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### PCA residual error estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T11:32:04.860423Z",
     "start_time": "2020-01-23T11:14:22.223170Z"
    },
    "hidden": true,
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca_error = []\n",
    "\n",
    "cmp_index = np.linspace(0, pca.n_components_, num=pca.n_components_//2, endpoint=True, dtype=int)\n",
    "cmp_index[-1] -= 1\n",
    "\n",
    "for i in cmp_index:\n",
    "# for i in range(pca.n_components):\n",
    "    print(\"Component %2s: %12.10f   %12.10f   %12.10f\" % (i, pca.explained_variance_[i], pca.explained_variance_ratio_[i], np.cumsum(pca.explained_variance_ratio_)[i]))\n",
    "    img = (X-np.dot(X_pca[:,:i],pca.components_[:i])-X.mean()).values\n",
    "    pca_error.append({\n",
    "        'min': img.min(),\n",
    "        'max': img.max(),\n",
    "        'mean' : img.mean(),\n",
    "        'median' : np.median(img),\n",
    "        'std' : np.std(img),\n",
    "        'pca_scores' : np.mean(model_selection.cross_val_score(decomposition.PCA(n_components=n_components).fit(X), X,cv=2))\n",
    "    })\n",
    "    print(pca_error[-1])\n",
    "\n",
    "pca_errors_df = pd.DataFrame.from_dict(pca_error)\n",
    "pca_errors_df['delta'] = pca_errors_df['max']-pca_errors_df['min']\n",
    "pca_errors_df['explained_variance'] = pca.explained_variance_[cmp_index]\n",
    "pca_errors_df['explained_variance_ratio'] = pca.explained_variance_ratio_[cmp_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T14:15:03.216655Z",
     "start_time": "2020-01-23T14:15:03.149687Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca_errors_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T15:13:01.109953Z",
     "start_time": "2020-01-23T15:13:01.000537Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import hvplot.pandas\n",
    "a = (pca_errors_df[['std']]).hvplot().opts(width=1200,height=400)\n",
    "b = (pca_errors_df['delta']).hvplot().opts(width=1200,height=400)\n",
    "c = pca_errors_df['pca_scores'].hvplot().opts(width=1200,height=400)\n",
    "d = pca_errors_df[['max','min']].hvplot().opts(width=1200,height=400)\n",
    "\n",
    "c\n",
    "# ((a+b+c)*hv.HLine(10000*(0.005*0.005)).opts(color='red')).cols(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T14:17:27.025978Z",
     "start_time": "2020-01-23T14:17:07.789533Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from skimage import exposure\n",
    "img = (X-np.dot(X_pca,pca.components_)-X.mean()).values\n",
    "\n",
    "plt.figure(figsize=[20,8]);\n",
    "plt.imshow(exposure.equalize_hist(img,nbins=512),\n",
    "           aspect='auto',\n",
    "           interpolation='bilinear',\n",
    "           cmap=plt.cm.Spectral_r,\n",
    "           extent = [X.columns.min(),X.columns.max(),  X.index.min(), X.index.max()],\n",
    "          );\n",
    "# cbar = plt.colorbar(ticks=[0.01, 0.5, 1], orientation='vertical')\n",
    "# cbar.ax.set_yticklabels(\n",
    "#     np.around([np.percentile(img,0.01), np.median(img), np.nanmax(img)],decimals=3)\n",
    "#     );\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T14:17:32.664128Z",
     "start_time": "2020-01-23T14:17:27.032431Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-20T13:49:04.069103Z",
     "iopub.status.busy": "2022-12-20T13:49:04.068876Z",
     "iopub.status.idle": "2022-12-20T13:49:05.160376Z",
     "shell.execute_reply": "2022-12-20T13:49:05.159672Z",
     "shell.execute_reply.started": "2022-12-20T13:49:04.069089Z"
    },
    "hidden": true,
    "tags": []
   },
   "source": [
    "recontruct initial data with choosen PCA components  and look at the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T14:17:32.664128Z",
     "start_time": "2020-01-23T14:17:27.032431Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff_img = (X-np.dot(X_pca,pca.components_)-X.mean())\n",
    "fig, axs = plt.subplots(nrows=2,ncols=2,figsize=[20,8])\n",
    "\n",
    "ax = axs.flatten()\n",
    "\n",
    "ax[0].set_title('max')\n",
    "diff_img.max().plot(ax=ax[0])\n",
    "\n",
    "ax[1].set_title('median')\n",
    "diff_img.median().plot(ax=ax[1])\n",
    "\n",
    "ax[2].set_title('min')\n",
    "diff_img.min().plot(ax=ax[2])\n",
    "\n",
    "ax[3].set_title('std')\n",
    "diff_img.std().plot(ax=ax[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T16:27:28.819556Z",
     "start_time": "2020-01-23T16:27:16.088047Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hv.extension('matplotlib')\n",
    "# plot_wav = 500\n",
    "# out = colorbar_img_shader((X-np.dot(X_pca,pca.components_)-X.mean())[plot_wav])\n",
    "# _ = out.DynamicMap.II.opts(interpolation='bicubic',aspect=2,fig_size=400,alpha=0.7)\n",
    "# out\n",
    "\n",
    "# spectral_df.columns.min(),np.quantile(spectral_df.columns,0.25),np.quantile(spectral_df.columns,0.75),spectral_df.columns.max()\n",
    "# (268, 444, 797.5, 974)\n",
    "\n",
    "hv.extension('matplotlib')\n",
    "    \n",
    "NdLayout = hv.NdLayout(\n",
    "            {plot_wav: df_shader( (X-np.dot(X_pca,pca.components_)-X.mean())[plot_wav],x_sampling=2,y_sampling=2).\\\n",
    "            opts(interpolation='bicubic',aspect=2,alpha=0.8)\n",
    "                 for ind,plot_wav in enumerate([270, 470, 770, 970])}\n",
    "            ,kdims='Wav')\n",
    "\n",
    "(background.opts(cmap='Gray',alpha=1)*NdLayout.cols(2)).opts(fig_size=200,tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### PCA visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T09:44:51.703714Z",
     "start_time": "2020-01-30T09:44:50.924263Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "components_shift = 0\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "ax = plt.subplot()\n",
    "ax.plot(np.arange(pca.n_components_-components_shift)+components_shift, pca.explained_variance_ratio_[components_shift:])\n",
    "ax.plot(np.arange(pca.n_components_-components_shift)+components_shift, pca.explained_variance_ratio_[components_shift:], \"r.\")\n",
    "ax.set_title(\"PCA explained variance\")\n",
    "ax.set_yscale('log')\n",
    "# ax.set_xlim([components_shift-1, pca.n_components_])\n",
    "# ax.set_ylim([np.min(pca.explained_variance_ratio_),np.max(pca.explained_variance_ratio_)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.Curve(\n",
    "                   (np.arange(pca.n_components_-components_shift)+components_shift, pca.explained_variance_ratio_[components_shift:]),\n",
    "                   kdims='components',vdims='var. ratio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T15:11:54.548356Z",
     "start_time": "2020-01-30T15:11:53.888181Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "components_shift = 0\n",
    "hv.extension('bokeh')\n",
    "\n",
    "overlay = hv.Curve(\n",
    "                   (np.arange(pca.n_components_-components_shift)+components_shift, pca.explained_variance_ratio_[components_shift:]),\n",
    "                   kdims='components',vdims='var. ratio')\n",
    "\n",
    "overlay2 = hv.Curve(\n",
    "                    (np.arange(pca.n_components_-components_shift)+components_shift, np.cumsum(pca.explained_variance_ratio_[components_shift:])),\n",
    "                    kdims='components',vdims='var. cumsum')\n",
    "\n",
    "overlay3 = hv.Curve(\n",
    "                    (np.arange(pca.n_components_-components_shift)+components_shift, np.gradient(np.cumsum(pca.explained_variance_ratio_[components_shift:]))),\n",
    "                    kdims='components',vdims='gradient(var. cumsum)')\n",
    "\n",
    "overlay4 = hv.Curve(\n",
    "                    (np.arange(pca.n_components_-components_shift)+components_shift, np.gradient(np.gradient(np.cumsum(pca.explained_variance_ratio_[components_shift:])))) ,\n",
    "                    kdims='components',vdims='$gradient^2$(var. cumsum)' )\n",
    "\n",
    "\n",
    "layout = overlay+overlay2+overlay3+overlay4\n",
    "\n",
    "layout.opts(\n",
    "    hv.opts.Curve(line_width=3,height=500, width=600),\n",
    "    hv.opts.Points(alpha=0.5, size=10),\n",
    ").cols(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T16:27:29.814860Z",
     "start_time": "2020-01-23T16:27:28.824873Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "\n",
    "vdims = ['~Reflectance']\n",
    "kdims=['Wavelenght (um)']\n",
    "\n",
    "width=1100\n",
    "height=500\n",
    "\n",
    "shift = 0.3\n",
    "overlay_dict = {'PCA.{}'.format(ind):hv.Curve((spectral_df.columns.to_numpy(),cmp + shift*(ind+1)),vdims=vdims,kdims=kdims) for ind,cmp in \n",
    "                enumerate(preprocessing.MinMaxScaler().fit_transform(pca.components_.T).T[:4,:])}\n",
    "\n",
    "overlay_dict['Mean'] = hv.Curve((spectral_df.columns.to_numpy(),\n",
    "                  preprocessing.MinMaxScaler().fit_transform(spectral_df.mean().values.reshape(-1, 1)).squeeze()\n",
    "                                ),vdims=vdims,kdims=kdims).opts(line_width=1,line_dash='solid',color='black',alpha=1)\n",
    "\n",
    "# shift_dict = {'PCA.{}-shift'.format(ind):hv.HLine(0.5*ind).opts(\n",
    "#                 line_width=0.25,line_dash='dashed',color='black') for ind in range(4)}\n",
    "\n",
    "\n",
    "hv.NdOverlay(overlay_dict).opts(\n",
    "                hv.opts.Curve(line_width=4, show_grid=True),\n",
    "                hv.opts.NdOverlay(width=width,height=height,legend_cols=4,legend_position='bottom')\n",
    "                ) #* hv.NdOverlay(shift_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T14:18:52.438837Z",
     "start_time": "2020-01-23T14:18:50.705967Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "\n",
    "max_pca_comp = 4\n",
    "\n",
    "NdLayout = hv.NdLayout(\n",
    "            {ind: df_shader(cmp[:,np.newaxis],x_sampling=2,y_sampling=2).\\\n",
    "             options(height=350,width=550,alpha=0.8)\n",
    "                 for ind,cmp in enumerate(X_pca[:,:max_pca_comp].T)}\n",
    "            ,kdims='Component')\n",
    "\n",
    "background.opts(cmap='Gray',alpha=0.9)*NdLayout.cols(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T16:27:41.828831Z",
     "start_time": "2020-01-23T16:27:29.820210Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hv.extension('matplotlib')\n",
    "\n",
    "max_pca_comp = 16\n",
    "\n",
    "for ind,cmp in enumerate(X_pca[:,:max_pca_comp].T):\n",
    "    print(f'PCA.{ind:02} min:{cmp.min():10.5} , max:{cmp.max():10.5}, delta:{cmp.max()-cmp.min():10.5}')\n",
    "\n",
    "NdLayout = hv.NdLayout(\n",
    "            {ind: df_shader(cmp[:,np.newaxis],x_sampling=2,y_sampling=2).\\\n",
    "            opts(interpolation='bicubic',aspect=2,alpha=0.7)\n",
    "                 for ind,cmp in enumerate(X_pca[:,:max_pca_comp].T)}\n",
    "            ,kdims='Component')\n",
    "\n",
    "(background.opts(cmap='Gray',alpha=1)*NdLayout.cols(4)).opts(tight=True, vspace=0.01, hspace=0.01, fig_size=150).cols(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T16:27:44.151949Z",
     "start_time": "2020-01-23T16:27:41.834742Z"
    },
    "hidden": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "\n",
    "import itertools\n",
    "gridplot = {}\n",
    "for x,y in itertools.combinations(range(3), 2):\n",
    "    print(x,y,X_pca[:,[x,y]].shape)\n",
    "    gridplot[f'PCA.{x} vs PCA.{y}'] = hv.Points(X_pca[:,[x,y]],kdims=[f'PCA.{x}',f'PCA.{y}'])\n",
    "\n",
    "\n",
    "# # hds.dynspread(\n",
    "hds.datashade(hv.NdLayout(gridplot),aggregator=ds.count(),cmap=plt.cm.viridis, x_sampling=0.003, y_sampling=0.003).\\\n",
    "opts(height=1000,width=1200,tight=True).cols(3)\n",
    "# opts(fig_size=200,aspect_weight=True,tight=True).cols(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T10:51:49.591134Z",
     "start_time": "2020-01-24T10:51:49.565420Z"
    }
   },
   "source": [
    "#### ICA\n",
    "\n",
    "[2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 0.20.2 documentation](https://scikit-learn.org/stable/modules/decomposition.html#independent-component-analysis-ica)\n",
    "\n",
    "Calculate the reconstruction error for increasing number of ICA components :\n",
    "\n",
    "    print(np.concatenate((np.arange(1,5),np.arange(0,160,20)[1:])))\n",
    "    array([  1,   2,   3,   4,  20,  40,  60,  80, 100, 120, 140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T10:51:12.198839Z",
     "start_time": "2020-02-04T10:51:12.184046Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ica_rec_error_path = pathlib.Path(out_models_path / 'ica_rec_error_df.csv')\n",
    "\n",
    "# check if we already run and stored this \n",
    "if ica_rec_error_path.is_file():\n",
    "    # load reconstruction error\n",
    "    ica_rec_error_df = pd.read_csv(ica_rec_error_path, index_col='ICA components n.')\n",
    "else:\n",
    "    # calculate reconstruction error\n",
    "    ica_rec_error = {}\n",
    "    for ica_n_components in np.concatenate((np.arange(1,5),np.arange(0,160,20)[1:])) : \n",
    "    #     print(ica_n_components)\n",
    "        ica = decomposition.FastICA(n_components=ica_n_components,random_state=4)\n",
    "        S_  = ica.fit_transform(X)\n",
    "        # evaluate overall reconstruction error\n",
    "        ica_rec_error[ica_n_components] = np.std((X-ica.inverse_transform(S_)).values.flatten())\n",
    "        print(ica_n_components,ica_rec_error[ica_n_components])\n",
    "\n",
    "    ica_rec_error_df = pd.DataFrame.from_dict(ica_rec_error,orient='index')\n",
    "    ica_rec_error_df.index.name = 'ICA components n.'\n",
    "    ica_rec_error_df.columns = ['reconstruction error']\n",
    "    ica_rec_error_df.to_csv(ica_rec_error_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T10:48:14.491420Z",
     "start_time": "2020-02-04T10:48:14.118800Z"
    }
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "display(ica_rec_error_df.sort_index())\n",
    "import hvplot.pandas\n",
    "\n",
    "(hv.HLine(0.0015,group='line')*\\\n",
    " hv.VLine(4,group='line')*\\\n",
    " ica_rec_error_df.sort_index().hvplot()*\\\n",
    " ica_rec_error_df.sort_index().hvplot(kind='scatter')).\\\n",
    "opts(\n",
    "    hv.opts.Points(marker='circle',alpha=0.5),\n",
    "    hv.opts.Curve(color='red') \n",
    "     )\n",
    "\n",
    "# (hv.RGB(np.random.rand(10, 10, 4), group='A') * hv.RGB(np.random.rand(10, 10, 4), group='B')).opts(\n",
    "#     hv.opts.RGB('A', alpha=0.1), hv.opts.RGB('B', alpha=0.5)\n",
    "# )\n",
    "\n",
    "# # ica_rec_error_df\n",
    "# hv.extension('bokeh')\n",
    "\n",
    "# (ica_rec_error_df/X.min().min()).hvplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T11:21:09.373344Z",
     "start_time": "2020-02-04T11:21:08.120408Z"
    }
   },
   "outputs": [],
   "source": [
    "import hvplot.pandas\n",
    "hv.extension('matplotlib')\n",
    "\n",
    "main_plot = hv.render((hv.HLine(0.0015,group='line')*\\\n",
    " hv.VLine(4,group='line')*\\\n",
    " ica_rec_error_df.sort_index().hvplot()*\\\n",
    " ica_rec_error_df.sort_index().hvplot(kind='scatter')).\\\n",
    "opts(\n",
    "    hv.opts.Points(marker='circle',alpha=0.5),\n",
    "    hv.opts.Curve(color='red', fig_size=250, aspect=2) \n",
    "     ))\n",
    "\n",
    "inset = hv.render((hv.HLine(0.0015,group='line')*\\\n",
    " hv.VLine(4,group='line')*\\\n",
    " ica_rec_error_df.sort_index().hvplot()*\\\n",
    " ica_rec_error_df.sort_index().hvplot(kind='scatter')).\\\n",
    "opts(\n",
    "    hv.opts.Points(marker='circle',alpha=0.5),\n",
    "    hv.opts.Curve(color='red', fig_size=50, aspect=1) \n",
    "     ).options(xlim=(2.5, 5),ylim=(0.00144, 0.00165)))\n",
    "\n",
    "\n",
    "main_ax = main_plot.get_axes()[0]\n",
    "inset = inset.get_axes()\n",
    "\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes,mark_inset\n",
    "\n",
    "axin = inset_axes(main_ax, width='50%', height='50%', loc=1)\n",
    "# axin = main_ax.inset_axes([80, 0.001, 0.2, 0.5])\n",
    "\n",
    "# main_ax.inset_axes?\n",
    "main_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T11:26:24.283625Z",
     "start_time": "2020-02-04T11:26:23.643364Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes,mark_inset\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[12,6])\n",
    "ica_rec_error_df.sort_index().plot(ax = ax,label=False,marker='o',legend=False)\n",
    "ax.hlines(0.0015,xmin = ica_rec_error_df.index.min(), xmax = ica_rec_error_df.index.max(), color='red')\n",
    "ax.vlines(4,ymin = ica_rec_error_df.min(), ymax = ica_rec_error_df.max(), color='green')\n",
    "ax.set_ylim( [ ica_rec_error_df.min().values[0], ica_rec_error_df.max().values[0]] )\n",
    "\n",
    "axin = inset_axes(ax, width='60%', height='60%', loc=1)\n",
    "ica_rec_error_df.sort_index().plot(ax = axin,label=False,marker='o',legend=False)\n",
    "axin.hlines(0.0015,xmin = ica_rec_error_df.index.min(), xmax = ica_rec_error_df.index.max(), color='red')\n",
    "axin.vlines(4,ymin = ica_rec_error_df.min(), ymax = ica_rec_error_df.max(), color='green')\n",
    "\n",
    "axin.set_ylim([0.00144, 0.00165])\n",
    "axin.set_xlim([2.5, 5])\n",
    "axin.axes.get_xaxis().set_visible(False)\n",
    "axin.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "mark_inset(ax, axin, loc1=2, loc2=3, fc=\"gray\", alpha=0.3, ec=\"0.5\");\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "save_plot('ICA_reconstruction_error_zoom_included', out_format='png',save=save_plots_bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:50:36.298750Z",
     "start_time": "2020-02-04T13:50:30.137453Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute ICA\n",
    "\n",
    "ica_n_components= 4\n",
    "\n",
    "# for ica_n_components in range(2,16):\n",
    "ica = decomposition.FastICA(n_components=ica_n_components,random_state=4)\n",
    "S_ = ica.fit_transform(X)  # Reconstruct signals\n",
    "A_ = ica.mixing_           # Get estimated mixing matrix\n",
    "\n",
    "# coefficients matrix\n",
    "# S_ /= S_.std(axis=0)\n",
    "\n",
    "# vector components = signal\n",
    "# A_ -= A_.mean(axis=0)\n",
    "# X ~= S_ x A_.T\n",
    "\n",
    "print(f'ica_n_components : {ica_n_components}')\n",
    "print(f'X : {X.shape}')\n",
    "print(f'A_ : {A_.shape}')\n",
    "print(f'S_ : {S_.shape}')\n",
    "# print(f'square(sum(X-ICA^-1(X)) : {np.sum((X-ica.inverse_transform(S_))**2)}')\n",
    "# print(f'np.norm(X-ICA^-1(X),2) : {np.linalg.norm(X-ica.inverse_transform(S_),2)}')\n",
    "print(((X-ica.inverse_transform(S_)).max()-(X-ica.inverse_transform(S_)).min()).describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:05:46.562192Z",
     "start_time": "2020-02-04T13:05:38.803057Z"
    }
   },
   "outputs": [],
   "source": [
    "## reconstruction error at specific wav ma\n",
    "# h.extension('matplotlib')\n",
    "# plot_wav = 500\n",
    "# out = colorbar_img_shader((X-np.dot(X_pca,pca.components_)-X.mean())[plot_wav])\n",
    "# _ = out.DynamicMap.II.opts(interpolation='bicubic',aspect=2,fig_size=400,alpha=0.7)\n",
    "# out\n",
    "\n",
    "# spectral_df.columns.min(),np.quantile(spectral_df.columns,0.25),np.quantile(spectral_df.columns,0.75),spectral_df.columns.max()\n",
    "# (268, 444, 797.5, 974)\n",
    "\n",
    "hv.extension('matplotlib')\n",
    "    \n",
    "NdLayout = hv.NdLayout(\n",
    "#           difference maps\n",
    "            {plot_wav: df_shader( (X-ica.inverse_transform(S_))[plot_wav],x_sampling=1,y_sampling=1).\\\n",
    "#           only reoconstructed vectors maps\n",
    "#              {plot_wav: df_shader( ica.inverse_transform(S_)[:,plot_wav//2-X.columns[0]],x_sampling=2,y_sampling=2).\\\n",
    "\n",
    "            opts(interpolation='bicubic',aspect=2,alpha=0.8)\n",
    "                 for ind,plot_wav in enumerate([270, 470, 770, 970])}\n",
    "            ,kdims='Wav')\n",
    "\n",
    "(background.opts(cmap='Gray',alpha=1)*NdLayout.cols(2)).opts(fig_size=200,tight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:50:02.592603Z",
     "start_time": "2020-02-04T13:49:59.873165Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.sqrt(((X-ica.inverse_transform(S_)).max()-(X-ica.inverse_transform(S_))).apply(np.square).sum()/X.size).plot(figsize=[20,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:57:26.433936Z",
     "start_time": "2020-02-04T13:57:25.509592Z"
    }
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "\n",
    "vdims = ['Reflectance']\n",
    "kdims=['wavelenght (nm)']\n",
    "width=1100\n",
    "height=500\n",
    "\n",
    "overlay_dict = {f'ICA comp. n.{ind}':hv.Curve((spectral_df.columns.to_numpy(),cmp),vdims=vdims,kdims=kdims) for ind,cmp in enumerate(A_.T) }\n",
    "\n",
    "overlay_dict['Mean'] = hv.Curve((spectral_df.columns.to_numpy(),\n",
    "                  preprocessing.MinMaxScaler().fit_transform(spectral_df.mean().values.reshape(-1, 1)).squeeze()\n",
    "                                ),vdims=vdims,kdims=kdims).opts(line_width=1,line_dash='dashed',color='black',alpha=1)\n",
    "\n",
    "\n",
    "hv.NdOverlay(overlay_dict).opts(\n",
    "                hv.opts.Curve(line_width=4, show_grid=True),\n",
    "                hv.opts.NdOverlay(width=width,height=height)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T14:02:11.036577Z",
     "start_time": "2020-02-04T14:02:10.474330Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('matplotlib')\n",
    "\n",
    "overlay_dict['Mean'].opts(linewidth=1, color='black',alpha =0.25)\n",
    "\n",
    "out = hv.NdOverlay(overlay_dict).opts(\n",
    "                hv.opts.Curve(linewidth=4, show_grid=False),\n",
    "                hv.opts.NdOverlay(fig_size=500, aspect=2.5))\n",
    "out\n",
    "save_plot('ICA_components', out_format='png',save=save_plots_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:07:07.349955Z",
     "start_time": "2020-02-04T13:07:07.345921Z"
    }
   },
   "outputs": [],
   "source": [
    "# [python - Holoviews change datashader colormap - Stack Overflow](https://stackoverflow.com/a/59837074)\n",
    "from holoviews.plotting.util import process_cmap\n",
    "# [Colormaps — HoloViews 1.12.7 documentation](http://holoviews.org/user_guide/Colormaps.html)\n",
    "# process_cmap(\"Plasma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T14:52:54.608599Z",
     "start_time": "2020-02-04T14:52:49.677891Z"
    }
   },
   "outputs": [],
   "source": [
    "hv.extension('matplotlib')\n",
    "\n",
    "max_comp = ica_n_components\n",
    "\n",
    "# sampling = 2\n",
    "# cmp = S_[:,2]\n",
    "# out = colorbar_img_shader(cmp[:,np.newaxis],x_sampling=sampling,y_sampling=sampling)\n",
    "# _ = out.DynamicMap.II.opts(interpolation='bilinear',aspect=2,fig_size=400,alpha=1)\n",
    "# out\n",
    "\n",
    "for ind,cmp in enumerate(S_[:,:max_comp].T):\n",
    "    print(f'ICA.{ind:02} min:{cmp.min():10.5} , max:{cmp.max():10.5}, delta:{cmp.max()-cmp.min():10.5}')\n",
    "\n",
    "sampling = 1\n",
    "NdLayout = hv.NdLayout(\n",
    "            {ind: df_shader(cmp[:,np.newaxis],x_sampling=sampling,y_sampling=sampling).\\\n",
    "            opts(interpolation='bicubic',aspect=2,alpha=0.85)\n",
    "                for ind,cmp in enumerate(S_.T)}\n",
    "            ,kdims='ICA Component')\n",
    "\n",
    "out = (background.opts(cmap='Gray',alpha=1)*NdLayout.cols(4)).opts(tight=True, vspace=0.01, hspace=0.01, fig_size=300).cols(2)\n",
    "out\n",
    "# hv.save(out, out_figure_path / 'ICA_components_map.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T15:18:23.422514Z",
     "start_time": "2020-02-04T15:18:17.680624Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hv.extension('bokeh')\n",
    "hv.extension('matplotlib')\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data = S_,\n",
    "    columns=[f'ICA.{ica_c}' for ica_c in range(ica_n_components)]\n",
    ")\n",
    "\n",
    "# # limit to the two \"major components\"\n",
    "# df_ds = hv.Dataset(df[['ICA.1','ICA.2']])\n",
    "df_ds = hv.Dataset(df)\n",
    "\n",
    "sampling = (df.max()-df.min()).describe().mean()/250\n",
    "\n",
    "def local_datashade ( X,\n",
    "                     aggregator=ds.count(),\n",
    "                     cmap=plt.cm.Spectral_r,\n",
    "                     x_sampling=sampling,\n",
    "                     y_sampling=sampling,\n",
    "                     **kwargs):\n",
    "    return hds.datashade(X,aggregator=aggregator,cmap=cmap,x_sampling=x_sampling,y_sampling=y_sampling, **kwargs)\n",
    "\n",
    "point_grid = hv.operation.gridmatrix(df_ds, diagonal_operation=hv.operation.histogram.instance(num_bins=50) ).map(local_datashade, hv.Scatter)\n",
    "# .opts(hv.opts.RGB(interpolation='bilinear',aspect=1,fig_size=200))\n",
    "\n",
    "out = point_grid.opts(fig_size=300).opts(hv.opts.RGB(interpolation='bilinear',aspect=1))\n",
    "out\n",
    "# hv.save(out, out_figure_path / 'ICA_coefficients_gridplot_density_C1_C2.png', dpi=200)\n",
    "# hv.save(out, out_figure_path / 'ICA_coefficients_gridplot_density.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold Learning\n",
    "\n",
    "[2.2. Manifold learning — scikit-learn 0.22.1 documentation](https://scikit-learn.org/stable/modules/manifold.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T15:18:24.609499Z",
     "start_time": "2020-02-04T15:18:24.594017Z"
    }
   },
   "outputs": [],
   "source": [
    "[p.stem.split('_') for p in pathlib.Path('embedding/').glob('*.npy')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### TSNE embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# all data\n",
    "cached_tsne = pathlib.Path('tsne_embedding.npy')\n",
    "\n",
    "# # low correlation \n",
    "# threshold=0.985\n",
    "# X = CovarianceThreshold(threshold=threshold).fit_transform(spectral.values[:,idx_in:idx_en])\n",
    "# cached_tsne = pathlib.Path('tsne_embedding_lowcorr-{}.npy'.format(threshold))\n",
    "\n",
    "if cached_tsne.is_file():\n",
    "    print(f'Loading: {cached_tsne}')\n",
    "    X_tsne = np.load(cached_tsne)\n",
    "else:\n",
    "    from sklearn.manifold import TSNE\n",
    "    X_tsne = TSNE(n_components=2).fit_transform(X)\n",
    "    np.save(cached_tsne,X_tsne)    \n",
    "\n",
    "print(X.shape, X_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T16:09:42.569870Z",
     "start_time": "2020-01-16T16:09:42.491002Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hv.extension('matplotlib')\n",
    "\n",
    "vdims = ['label']\n",
    "kdims=['TSNE.0','TSNE.1']\n",
    "\n",
    "perplexities = [5, 30, 50, 100]\n",
    "\n",
    "### RANDOM SAMPPLING\n",
    "# randomize = np.random.choice(X_pca.shape[0], size=1000)\n",
    "# label=labels[randomize]\n",
    "# X = X_pca[randomize,:]\n",
    "\n",
    "overwrite=True\n",
    "label= None\n",
    "X = X_pca\n",
    "print(f'TSN X.shape : {X.shape}')\n",
    "\n",
    "filenameextra ='_pcacomponents-{}'.format(n_components)\n",
    "\n",
    "def get_tsne(perplexity=None,filename_extra=filenameextra, overwrite=False):\n",
    "    from sklearn.manifold import TSNE\n",
    "    cached_tsne = pathlib.Path('embedding/tsne_embedding_perplexity-{}{}.npy'.format(perplexity,filename_extra))\n",
    "    if cached_tsne.is_file():\n",
    "        print(f'Loading: {cached_tsne}')\n",
    "        x_tsne = np.load(cached_tsne)\n",
    "    else:\n",
    "        print(f'file not found: {cached_tsne} - Calculating!')\n",
    "        if overwrite:\n",
    "            x_tsne = TSNE(n_components=2).fit_transform(X)\n",
    "            np.save(cached_tsne,x_tsne)    \n",
    "        else:\n",
    "            print(f'overwrite set to {overwrite} stopping')\n",
    "    print('X.shape : {}, X_tsne.shape : {}, perplexity : {}'.format(X.shape, x_tsne.shape,perplexity))\n",
    "    return x_tsne\n",
    "\n",
    "\n",
    "def tsne_to_holocurve(*argv, **kwargs):\n",
    "    xtsne = get_tsne(perplexity=kwargs['perplexity'],filename_extra=kwargs['filename_extra'], overwrite=kwargs['overwrite'])\n",
    "    print(kwargs)\n",
    "    if 'label' in kwargs and kwargs.get('label') is not None:\n",
    "#         print('Label')\n",
    "#         print(np.unique(kwargs.get('label'),return_counts=True))\n",
    "        return hv.Scatter(np.hstack([xtsne ,kwargs.get('label')[:,np.newaxis]]),vdims=kwargs['vdims'],kdims=kwargs['kdims'])\n",
    "    else:\n",
    "        print('Nolabel')\n",
    "        return hv.Scatter(xtsne,kdims=kwargs['kdims'][0], vdims=kwargs['kdims'][1])\n",
    "    \n",
    "curve_dict = {p:tsne_to_holocurve(label=label,\n",
    "                                  perplexity=p,\n",
    "                                  overwrite=overwrite,\n",
    "                                  filename_extra=filenameextra,\n",
    "                                  vdims=vdims,\n",
    "                                  kdims=kdims)\n",
    "              for p in perplexities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T15:55:22.409670Z",
     "start_time": "2020-01-16T15:55:22.379733Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "label= None\n",
    "min_size, max_size = 20, 30\n",
    "\n",
    "if label is not None:\n",
    "    print('labels = ',label.shape)\n",
    "    classes , s = np.unique(label, return_counts=True)\n",
    "    print('classes size:',dict(zip(classes,s)))\n",
    "    # marker size inverse proportional to population size\n",
    "    sizes = ((1-(s-s.min())/(s.max()-s.min()))*(max_size-min_size))+min_size\n",
    "    NdLayout = hv.NdLayout(curve_dict, kdims='perplexity').opts(hv.opts.Scatter(s= [dict(zip(classes,sizes)).get(l) for l in label]),hv.opts.NdLayout(fig_inches=6))\n",
    "    NdLayout.opts(hv.opts.Scatter(color=vdims[0]))\n",
    "else:\n",
    "    print('no labels = ')\n",
    "    NdLayout = hv.NdLayout(curve_dict, kdims='perplexity').opts(hv.opts.Scatter(s=min_size),hv.opts.NdLayout(fig_inches=6))\n",
    "\n",
    "NdLayout.opts(hv.opts.Scatter(alpha=0.25, cmap='Set1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T15:11:44.876035Z",
     "start_time": "2020-01-15T15:11:44.869922Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perplexity = 30\n",
    "# X_tsne = get_tsne(perplexity=perplexity,filename_extra=filenameextra, overwrite=False)\n",
    "X_tsne = curve_dict.get(perplexity).data[:,:-1]\n",
    "print('X.shape : {}, X_tsne.shape : {}, perplexity : {}'.format(X.shape, X_tsne.shape,perplexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UMAP embedding\n",
    "\n",
    "[Basic UMAP Parameters — umap 0.3 documentation](https://umap-learn.readthedocs.io/en/latest/parameters.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T14:54:59.376413Z",
     "start_time": "2020-02-04T14:54:57.471405Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import umap\n",
    "\n",
    "vdims = ['label']\n",
    "kdims=['UMAP.0','UMAP.1']\n",
    "\n",
    "X_embedd = X.values\n",
    "\n",
    "filenameextra ='_icacomponents-{}'.format(ica_n_components)\n",
    "\n",
    "### RANDOM SAMPPLING\n",
    "randomize = np.random.choice(X.shape[0], size=100)\n",
    "try:\n",
    "    label=labels[randomize]\n",
    "except NameError:\n",
    "    label=None\n",
    "# X_embedd = X_embedd[randomize,:]\n",
    "\n",
    "overwrite=True\n",
    "label= None\n",
    "\n",
    "# neighbors = np.arange(5,X_embedd.shape[0]//6,X_embedd.shape[0]//20) # 5 to a quarter of the data each 1/8 o the data\n",
    "neighbors     = [   100, 4000, 7000]\n",
    "min_distances = (0.0, 0.5, 0.99)\n",
    "\n",
    "print('X_embedd.shape : ',X_embedd.shape)\n",
    "print('     neighbors : ',neighbors)\n",
    "print(' min_distances : ',min_distances)\n",
    "\n",
    "import itertools\n",
    "print(list(itertools.product(neighbors,min_distances)))\n",
    "\n",
    "def get_umap(neighbors, mindistances,filename_extra=filenameextra, label=None, overwrite=False):\n",
    "\n",
    "    cached_umap = pathlib.Path('embedding/umap_embedding_neighbors-{}_mindist-{}{}.npy'.format(neighbors,mindistances,filename_extra))\n",
    "    if cached_umap.is_file():\n",
    "        print(f'Loading: {cached_umap}')\n",
    "        x_umap = np.load(cached_umap)\n",
    "    else:\n",
    "        print(f'file not found: {cached_umap} - Calculating!')\n",
    "        if overwrite: \n",
    "            x_umap = umap.UMAP(n_neighbors=neighbors, min_dist = mindistances).fit_transform(X_embedd)\n",
    "            np.save(cached_umap,x_umap)    \n",
    "        else:\n",
    "            print(f'overwrite set to {overwrite} stopping')\n",
    "    print('X_embedd.shape : {}, x_umap.shape : {}, neighbors : {}, min_dist: {}'.format(X_embedd.shape, x_umap.shape,neighbors, mindistances))\n",
    "    if label is not None:\n",
    "        return hv.Scatter(np.hstack([x_umap,label[:,np.newaxis]]),vdims=vdims,kdims=kdims)\n",
    "    else:\n",
    "        return hv.Scatter(x_umap)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T14:55:00.588391Z",
     "start_time": "2020-02-04T14:55:00.487630Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hv.extension('matplotlib')\n",
    "\n",
    "curve_dict_2D = {(n,d):get_umap(n,d,label=label, overwrite=overwrite) for n in neighbors for d in min_distances}\n",
    "\n",
    "gridspace = hv.GridSpace(curve_dict_2D, kdims=['neighbors, local > global structure', 'minimum distance in representation']).opts(hv.opts.Scatter(s=25,alpha=0.25),hv.opts.GridSpace(fig_inches=16))\n",
    "\n",
    "# if labels.any(): \n",
    "#     gridspace.opts(hv.opts.Scatter(cmap=plt.cm.Spectral_r,c='label'))\n",
    "\n",
    "# gridspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T15:24:55.538495Z",
     "start_time": "2020-02-04T15:24:54.253178Z"
    }
   },
   "outputs": [],
   "source": [
    "gridspace\n",
    "\n",
    "hv.extension('matplotlib')\n",
    "# hv.extension('bokeh')\n",
    "\n",
    "out = hds.dynspread(\n",
    "hds.datashade(gridspace,\n",
    "              aggregator=ds.count(),\n",
    "              cmap=plt.cm.Spectral_r,\n",
    "              x_sampling=0.25,\n",
    "              y_sampling=0.25,\n",
    "             ).\\\n",
    "opts(aspect=1,fig_size=70).opts(hv.opts.RGB(interpolation='bilinear')))\n",
    "# opts(height=1600,width=1600))\n",
    "out\n",
    "\n",
    "# hv.save(out, out_figure_path / f'UMAP_gridspace_ICA_{ica_n_components}components.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T15:24:57.743887Z",
     "start_time": "2020-02-04T15:24:57.730498Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neigh_mindist =  (4000, 0.99)\n",
    "\n",
    "# print('X.shape : {}, X_umap.shape : {}, (n_neighbors,min_dist) : {}'.format(X.shape, X_umap.shape,neigh_mindist))\n",
    "X_umap_scatter = get_umap(neighbors=neigh_mindist[0], mindistances=neigh_mindist[1],filename_extra=filenameextra, label=None, overwrite=True)\n",
    "\n",
    "X_umap = curve_dict_2D.get(neigh_mindist).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T15:24:59.150353Z",
     "start_time": "2020-02-04T15:24:58.697765Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "vdims = ['label']\n",
    "kdims=['UMAP.0','UMAP.1']\n",
    "\n",
    "hv.extension('matplotlib')\n",
    "# hv.extension('bokeh')\n",
    "hds.datashade(X_umap_scatter,\n",
    "              aggregator=ds.count(),\n",
    "              cmap=plt.cm.Spectral_r,\n",
    "              x_sampling=0.4,\n",
    "              y_sampling=0.3,\n",
    "             ).\\\n",
    "opts(interpolation='bilinear',aspect=1,fig_size=200)\n",
    "# opts(height=600,width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T15:25:09.309518Z",
     "start_time": "2020-02-04T15:25:08.421360Z"
    }
   },
   "outputs": [],
   "source": [
    "import pdir\n",
    "a = hds.rasterize(X_umap_scatter,aggregator=ds.count(),dynamic=False,x_sampling=0.4, y_sampling=0.3).data.to_dataframe()\n",
    "display(a.describe())\n",
    "\n",
    "ax = a.plot.hist(bins=45)\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T15:25:27.347442Z",
     "start_time": "2020-02-04T15:25:27.306610Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# data for classificatiom\n",
    "\n",
    "# X_classification = X_pca # PCA\n",
    "# X_classification = S_ # ICA\n",
    "# X_classification = W # NFM # really noisy/bad!!\n",
    "# X_classification = X_tsne # tsne embedding\n",
    "# X_classification = X_umap # umap embedding\n",
    "X_classification = z_mean\n",
    "n_classification_features = X_classification.shape[1]\n",
    "\n",
    "print('X_classification shape : ',X_classification.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T15:16:46.847222Z",
     "start_time": "2020-02-05T15:16:46.305828Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Scalers\n",
    "##########\n",
    "# scaler , preprocessing_type = preprocessing.StandardScaler().fit(X_classification)    , 'StandardScaler'\n",
    "# scaler , preprocessing_type = preprocessing.MinMaxScaler().fit(X_classification) , 'MinMaxScaler'\n",
    "# scaler , preprocessing_type = preprocessing.RobustScaler().fit(X_classification) , 'RobustScaler'\n",
    "scaler, preprocessing_type = preprocessing.FunctionTransformer(lambda x:x), 'None'\n",
    "\n",
    "# ##########################\n",
    "# classifier = 'K-Means'\n",
    "# n_clusters = 2\n",
    "# # Kmeans estimator instance and Classify scaled data\n",
    "# k_means = cluster.KMeans(n_clusters=n_clusters, random_state=0).fit(scaler.transform(X_classification))\n",
    "# labels = k_means.labels_\n",
    "# print('k_means.inertia_ : ',k_means.inertia_)\n",
    "\n",
    "##########################\n",
    "classifier = 'AgglomerativeClustering'\n",
    "n_clusters = 13\n",
    "aggclustering = cluster.AgglomerativeClustering(linkage='complete',\n",
    "                                                affinity='l2',\n",
    "                                                n_clusters=n_clusters).fit(scaler.transform(X_classification))\n",
    "labels = aggclustering.labels_\n",
    "\n",
    "##########################\n",
    "# classifier = 'DBSCAN'\n",
    "# dbscan = cluster.DBSCAN(eps=0.9, min_samples=5).fit(X_classification)\n",
    "# labels = dbscan.labels_\n",
    "\n",
    "##########################\n",
    "\n",
    "# ##########\n",
    "# classifier = 'HDBSCAN'\n",
    "# import hdbscan\n",
    "# hdbscan = hdbscan.HDBSCAN(\n",
    "#             min_cluster_size=X_classification.shape[0]//2000,\n",
    "#             min_samples=1,\n",
    "#             cluster_selection_epsilon=0.75,\n",
    "# #             allow_single_cluster=False,\n",
    "#             )\n",
    "# hdbscan.fit(scaler.transform(X_classification))\n",
    "# labels = hdbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:23:12.889934Z",
     "start_time": "2020-02-05T14:23:12.836303Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### Statistics\n",
    "clust_stat_df = pd.DataFrame([{\n",
    "         'size': labels[labels == val].size,\n",
    "         'clAss_mean_fetures_delta': np.mean(np.max(X_classification[labels == val,:],axis=0)-np.min(X_classification[labels == val,:],axis=0)),\n",
    "         'class_mean_features_std':np.max(X_classification[labels == val,:].std(axis=0)),\n",
    "            }\n",
    "            for val in np.unique(labels)],\n",
    "          index=[val for val in np.unique(labels)]).sort_values('size',ascending=False)\n",
    "print(clust_stat_df.shape[0])\n",
    "display(clust_stat_df)\n",
    "display(clust_stat_df[['size']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:23:13.082724Z",
     "start_time": "2020-02-05T14:23:12.895163Z"
    },
    "code_folding": [],
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#####\n",
    "# colors: relabelling the classes using the first centroids values\n",
    "\n",
    "# calculate all the class centers in data space\n",
    "y = X.groupby(labels).mean().values\n",
    "# position of the data feature used to sort lables\n",
    "feature_index = find_nearest(700)\n",
    "# here the sorting index\n",
    "centroids_sorting_index = np.argsort(y[:, feature_index])\n",
    "# here the sorting labels, not the index!!\n",
    "centroids_sorted_labels = np.argsort(centroids_sorting_index) \n",
    "# # use pd.Series.map(dict) di directly change values in place \n",
    "labels = pd.Series(labels).map(dict(zip(np.arange(n_clusters),centroids_sorted_labels))).values\n",
    "print('index for label sort :',feature_index)\n",
    "# print(' features y[:,index] :',y[:, feature_index])\n",
    "# print(centroids_sorting_index)\n",
    "# print(centroids_sorted_labels)\n",
    "print(f'ind:y_feat  > new_index')\n",
    "for i,yf,ni in zip(range(len(y[:, feature_index])),y[:, feature_index],centroids_sorted_labels):\n",
    "    print(f'{i:3}:{yf:.5f} > {ni:>4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:23:23.445111Z",
     "start_time": "2020-02-05T14:23:13.091541Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\")\n",
    "\n",
    "scatter_df = pd.DataFrame(scaler.transform(X_classification), columns = [f'feature_{x}' for x in range(n_classification_features)])\n",
    "scatter_df['class'] = labels\n",
    "scatter_df['class'] = scatter_df['class'].apply(lambda x: 'classs_{}'.format(x))\n",
    "\n",
    "if X_classification.shape[1] > 2:\n",
    "    g = sns.PairGrid(scatter_df,hue=\"class\",height=4);\n",
    "    # g = g.map_offdiag(sns.kdeplot,lw=1)\n",
    "    g = g.map_offdiag(plt.scatter, s=0.1 , alpha=0.5);\n",
    "    g;\n",
    "else: \n",
    "    plt.figure(figsize=[8,8])\n",
    "    # inverselly scale scatteplot size to clas size \n",
    "    classes , s = np.unique(scatter_df['class'],return_counts=True)\n",
    "    min_size, max_size = 20, 100\n",
    "    sizes = (((s-s.min())/(s.max()-s.min()))*(max_size-min_size))+min_size\n",
    "    sns.scatterplot(x=\"feature_0\", y=\"feature_1\",hue=\"class\", size='class', sizes=dict(zip(classes,sizes)), data=scatter_df, alpha=0.1);\n",
    "\n",
    "save_plot( f'Classification-scatter-features-n_clusters_{n_clusters}_classifier-{classifier}', out_format='png',save=save_plots_bool)\n",
    "\n",
    "#   hv.extension('bokeh')\n",
    "# hv.extension('matplotlib')\n",
    "\n",
    "# df_ds = hv.Dataset(scatter_df,kdims=[f'feature_{x}' for x in range(n_classification_features)], vdims='class')\n",
    "\n",
    "# sampling = (df.max()-df.min()).describe().mean()/150\n",
    "\n",
    "# def local_datashade ( X,\n",
    "#                      aggregator=ds.mean(),\n",
    "#                      cmap=plt.cm.Spectral_r,\n",
    "#                      x_sampling=sampling,\n",
    "#                      y_sampling=sampling,\n",
    "#                      **kwargs):\n",
    "#     return hds.datashade(X,aggregator=aggregator,cmap=cmap,x_sampling=x_sampling,y_sampling=y_sampling, **kwargs)\n",
    "\n",
    "# point_grid = hv.operation.gridmatrix(\n",
    "#                                     df_ds, diagonal_operation=hv.operation.histogram.instance(num_bins=40)\n",
    "#                                     ).map(hds.datashade, hv.Scatter)\n",
    "# # # .opts(hv.opts.RGB(interpolation='bilinear',aspect=1,fig_size=200))\n",
    "\n",
    "# point_grid.opts(fig_size=300).opts(hv.opts.RGB(interpolation='bilinear',aspect=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "\n",
    "print(np.unique(labels))\n",
    "outdf_gdf.loc[spectral_df_nona_index,'R'] = labels\n",
    "outdf_gdf.loc[outdf_gdf['R'] != 11,'R'] = np.nan\n",
    "\n",
    "outdf_gdf[['x','y','R']].hvplot.scatter(x='x',y='y',c='R',\n",
    "                    rasterize=True,aggregator='mean',dynamic=True,\n",
    "                    x_sampling=2,y_sampling=1,cmap='rainbow',cnorm='eq_hist'\n",
    "                    ).opts(height=600,width=1200,alpha=1)#*background.opts(cmap='Gray',alpha=.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "heading_collapsed=true tags=[] jp-MarkdownHeadingCollapsed=true"
   },
   "source": [
    "#### DBSCAN hypeparamter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T14:36:22.794041Z",
     "start_time": "2020-01-27T14:34:45.097230Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## DBSCAN hypeparamter search\n",
    "\n",
    "eps = np.arange(0.9,1.1,.025)\n",
    "min_samples = [5,10,20,30]\n",
    "# min_samples = [10] \n",
    "\n",
    "dbscan_stats = []\n",
    "\n",
    "for e in eps:\n",
    "    for ms in min_samples:\n",
    "#         dbscan = cluster.DBSCAN(eps=e).fit(X_classification)\n",
    "        dbscan = cluster.DBSCAN(eps=e,min_samples=ms).fit(X_classification)\n",
    "        unique, counts = np.unique(dbscan.labels_, return_counts=True)\n",
    "        stats = {'eps':e,\n",
    "                 'min_samples': ms,\n",
    "                 'n_clusters':len(unique),\n",
    "                 'pop_mean':np.mean(counts),\n",
    "                 'pop_std':np.std(counts),\n",
    "                 'pop_min':np.min(counts),\n",
    "                 'pop_max':np.max(counts)\n",
    "                }\n",
    "        \n",
    "        stats_df = pd.DataFrame([{\n",
    "                 'delta_feat_mean': np.mean(np.max(X_classification[dbscan.labels_ == val,:],axis=0)-np.min(X_classification[dbscan.labels_ == val,:],axis=0)),\n",
    "                 'delta_feat_std' :  np.std(np.max(X_classification[dbscan.labels_ == val,:],axis=0)-np.min(X_classification[dbscan.labels_ == val,:],axis=0)),\n",
    "                    }\n",
    "                    for val in np.unique(dbscan.labels_)],\n",
    "                  index=[val for val in np.unique(dbscan.labels_)])\n",
    "\n",
    "        stats.update(dict(zip([x+'_min' for x in stats_df.std().to_dict().keys()],stats_df.min().to_dict().values())))\n",
    "        stats.update(dict(zip([x+'_max' for x in stats_df.std().to_dict().keys()],stats_df.max().to_dict().values())))\n",
    "        stats.update(dict(zip([x+'_mean' for x in stats_df.std().to_dict().keys()],stats_df.mean().to_dict().values())))\n",
    "        stats.update( dict(zip( [x+'_std' for x in stats_df.std().to_dict().keys()], stats_df.std().to_dict().values())) )\n",
    "\n",
    "        dbscan_stats.append(stats)\n",
    "        print(e,ms)\n",
    "\n",
    "# stats_df = pd.DataFrame(dbscan_stats).set_index('eps')\n",
    "# display(stats_df.T)\n",
    "# stats_df.plot(figsize=[20,20],subplots=True);\n",
    "\n",
    "values='n_clusters'\n",
    "print(values)\n",
    "display(pd.DataFrame(dbscan_stats).pivot(index='eps', columns='min_samples', values=values).T)\n",
    "\n",
    "values='pop_mean'\n",
    "print(values)\n",
    "display(pd.DataFrame(dbscan_stats).pivot(index='eps', columns='min_samples', values=values).T)\n",
    "\n",
    "values='pop_std'\n",
    "print(values)\n",
    "display(pd.DataFrame(dbscan_stats).pivot(index='eps', columns='min_samples', values=values).T)\n",
    "\n",
    "values='pop_max'\n",
    "print(values)\n",
    "display(pd.DataFrame(dbscan_stats).pivot(index='eps', columns='min_samples', values=values).T)\n",
    "\n",
    "values='pop_min'\n",
    "print(values)\n",
    "display(pd.DataFrame(dbscan_stats).pivot(index='eps', columns='min_samples', values=values).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "heading_collapsed=true tags=[] jp-MarkdownHeadingCollapsed=true"
   },
   "source": [
    "####  AgglomerativeClustering plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T16:57:31.339453Z",
     "start_time": "2020-02-04T16:56:32.120781Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "linkage_array = linkage(aggclustering.children_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T11:05:45.614634Z",
     "start_time": "2020-02-05T11:04:12.591508Z"
    },
    "hidden": true,
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,8])\n",
    "dendrogram(linkage_array,\n",
    "    p=40,  # show only the last p merged clusters\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    orientation='top',\n",
    "    no_labels=False,\n",
    "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=16.,\n",
    ");\n",
    "plt.xlabel('sample index or (cluster size)')\n",
    "plt.ylabel('distance')\n",
    "# plt.ylim([3000,6200])\n",
    "# plt.xlim([-0.5,10.5])\n",
    "# plt.hlines(4500,0,100,color='red')\n",
    "plt.show()\n",
    "save_plot(f'{classifier}_dendrogram.png', out_format='png',save=save_plots_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T16:57:31.577796Z",
     "start_time": "2020-02-04T16:57:31.348245Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import inconsistent\n",
    "depth = 3\n",
    "incons = inconsistent(linkage_array, depth)\n",
    "incons[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T10:34:32.862981Z",
     "start_time": "2020-02-05T10:32:58.922780Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# see [SciPy Hierarchical Clustering and Dendrogram Tutorial | Jörn's Blog](https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/)\n",
    "def fancy_dendrogram(*args, **kwargs):\n",
    "    max_d = kwargs.pop('max_d', None)\n",
    "    if max_d and 'color_threshold' not in kwargs:\n",
    "        kwargs['color_threshold'] = max_d\n",
    "    annotate_above = kwargs.pop('annotate_above', 0)\n",
    "\n",
    "    ddata = dendrogram(*args, **kwargs)\n",
    "\n",
    "    if not kwargs.get('no_plot', False):\n",
    "        plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "        plt.xlabel('sample index or (cluster size)')\n",
    "        plt.ylabel('distance')\n",
    "        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n",
    "            x = 0.5 * sum(i[1:3])\n",
    "            y = d[1]\n",
    "            if y > annotate_above:\n",
    "                plt.plot(x, y, 'o', c=c)\n",
    "                plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -5),\n",
    "                             textcoords='offset points',\n",
    "                             va='top', ha='center')\n",
    "        if max_d:\n",
    "            plt.axhline(y=max_d, c='k')\n",
    "    return ddata\n",
    "\n",
    "plt.figure(figsize=[10,8])\n",
    "fancy_dendrogram(\n",
    "    linkage_array,\n",
    "    truncate_mode='lastp',\n",
    "    p=20,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=30.,\n",
    "    show_contracted=True,\n",
    "    annotate_above=10,  # useful in small plots so annotations don't overlap\n",
    ")\n",
    "plt.ylim([2000,6500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T17:01:15.952166Z",
     "start_time": "2020-02-04T17:01:15.732098Z"
    },
    "hidden": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "last = linkage_array[-10:, 2]\n",
    "last_rev = last[::-1]\n",
    "idxs = np.arange(1, len(last) + 1)\n",
    "plt.plot(idxs, last_rev)\n",
    "\n",
    "acceleration = np.diff(last, 2)  # 2nd derivative of the distances\n",
    "acceleration_rev = acceleration[::-1]\n",
    "plt.plot(idxs[:-2] + 1, acceleration_rev)\n",
    "plt.show()\n",
    "k = acceleration_rev.argmax() + 2  # if idx 0 is the max of this we want 2 clusters\n",
    "print(\"clusters:\", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "####  Classification Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T13:29:56.396864Z",
     "start_time": "2020-02-05T13:29:56.389982Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "[c for c in hv.Cycle.default_cycles.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:32:02.164400Z",
     "start_time": "2020-02-05T14:32:01.034210Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "\n",
    "vdims = ['component']\n",
    "kdims=['wavelenght']\n",
    "width=1000\n",
    "height=500\n",
    "\n",
    "colors = cm(np.linspace(0,1,len(np.unique(labels))))\n",
    "matplotlib.colors.LinearSegmentedColormap.from_list('Spectral',colors , N=len(np.unique(labels)))\n",
    "cm = plt.cm.Spectral_r\n",
    "cm_cycle = hv.Cycle([cm(c) for c in np.linspace(0,1,len(np.unique(labels)))])\n",
    "hv.Cycle.default_cycles['default_colors'] = cm_cycle\n",
    "\n",
    "overlay_dict = {ind:hv.Curve((spectral_df.columns.to_numpy(),cmp)) for ind,cmp in X.groupby(labels).mean().iterrows()}\n",
    "\n",
    "overlay_dict['Mean'] = hv.Curve((\n",
    "                    spectral_df.columns.to_numpy(),\n",
    "                    spectral_df.mean()\n",
    "                    ),vdims=vdims,kdims=kdims).opts(line_width=0.5,line_dash='dashed',color='black',alpha=1).relabel('mean')\n",
    "\n",
    "hv.NdOverlay(overlay_dict).opts(\n",
    "                hv.opts.Curve(line_width=4,color=cm_cycle),\n",
    "                hv.opts.Curve('mean',color='black'),\n",
    "                hv.opts.NdOverlay(width=width,height=height)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:45.199294Z",
     "start_time": "2020-02-05T14:30:44.920406Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('matplotlib')\n",
    "\n",
    "hv.Cycle.default_cycles['default_colors'] = hv.Cycle([cm(c) for c in np.linspace(0,255,len(np.unique(labels)))])\n",
    "\n",
    "vdims = ['component']\n",
    "kdims=['wavelenght']\n",
    "\n",
    "overlay_dict = {ind:hv.Curve((spectral_df.columns.to_numpy(),cmp)) for ind,cmp in X.groupby(labels).mean().iterrows()}\n",
    "\n",
    "overlay_dict['Mean'] = hv.Curve((\n",
    "                    spectral_df.columns.to_numpy(),\n",
    "                    spectral_df.mean()\n",
    "                    ),vdims=vdims,kdims=kdims).relabel('mean')\n",
    "\n",
    "overlay_dict['Median'] = hv.Curve((\n",
    "                    spectral_df.columns.to_numpy(),\n",
    "                    spectral_df.median()\n",
    "                    ),vdims=vdims,kdims=kdims).relabel('median')\n",
    "\n",
    "out = hv.NdOverlay(overlay_dict).opts(\n",
    "                hv.opts.Curve(linewidth=4, show_grid=False),\n",
    "                hv.opts.Curve('mean',linewidth=6,linestyle=':',color='black',alpha =0.5),\n",
    "                hv.opts.Curve('median',linewidth=3,color='black',alpha =0.5),\n",
    "                hv.opts.NdOverlay(fig_size=600, aspect=2)\n",
    "                )\n",
    "\n",
    "out\n",
    "\n",
    "# # hv.save(out,out_figure_path / f'Spectral-centroids_n_clusters-{n_clusters}_classifier-{classifier}.png', dpi = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:45.199294Z",
     "start_time": "2020-02-05T14:30:44.920406Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## normalised to global mean\n",
    "\n",
    "hv.extension('matplotlib')\n",
    "\n",
    "hv.Cycle.default_cycles['default_colors'] = hv.Cycle([cm(c) for c in np.linspace(0,255,len(np.unique(labels)))])\n",
    "\n",
    "vdims = ['component']\n",
    "kdims=['wavelenght']\n",
    "\n",
    "overlay_dict = {ind:hv.Curve((spectral_df.columns.to_numpy(),cmp)) for ind,cmp in (X.groupby(labels).mean()/spectral_df.median()).iterrows()}\n",
    "\n",
    "out = hv.NdOverlay(overlay_dict).opts(\n",
    "                hv.opts.Curve(linewidth=4, show_grid=False),\n",
    "                hv.opts.NdOverlay(fig_size=600, aspect=2)\n",
    "                )\n",
    "\n",
    "out\n",
    "\n",
    "# # hv.save(out,out_figure_path / f'Spectral-centroids_n_clusters-{n_clusters}_classifier-{classifier}.png', dpi = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T13:21:47.104924Z",
     "start_time": "2020-02-05T13:21:43.211596Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "# opts(height=600,width=1000,alpha=0.7)\n",
    "\n",
    "\n",
    "out = background.opts(cmap='Gray')*df_shader(labels,cmap=cm).opts(hv.opts.RGB(aspect=2,fig_size=400,alpha=0.9,interpolation='bicubic'))\n",
    "out\n",
    "# hv.save(out,out_figure_path / f'Classification-map_n_clusters-{n_clusters}_classifier-{classifier}.png', dpi = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outdf_gdf['labels'] = np.nan\n",
    "outdf_gdf.loc[spectral_df_nona_index,'labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "background.opts(cmap='Gray')*outdf_gdf[['x','y','labels']].hvplot.scatter(x='x',y='y',c='labels',\n",
    "                    rasterize=True,aggregator='mean',dynamic=False,\n",
    "                    x_sampling=1,y_sampling=1,cmap='rainbow',cnorm='eq_hist'\n",
    "                    ).opts(height=600,width=1200,alpha=.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T13:21:47.104924Z",
     "start_time": "2020-02-05T13:21:43.211596Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "\n",
    "out = background.opts(cmap='Gray')*df_shader(labels,cmap=cm).opts(height=600,width=1000,alpha=0.7)\n",
    "out\n",
    "# hv.save(out,out_figure_path / f'Classification-map_n_clusters-{n_clusters}_classifier-{classifier}.png', dpi = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T17:03:41.128063Z",
     "start_time": "2020-02-04T17:02:11.639344Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.silhouette_score(X_classification, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Neural Network - Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T12:50:03.942765Z",
     "start_time": "2020-02-05T12:50:03.753437Z"
    },
    "lines_to_next_cell": 2,
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# small_data == spectral_df[spectral_df_nonan]\n",
    "X = spectral_df.loc[spectral_df_nona_index]\n",
    "\n",
    "print(f'        outdf_gdf : {outdf_gdf.shape}')\n",
    "print(f'      spectral_df : {spectral_df.shape}')\n",
    "print(f'spectral_df_nonan : {spectral_df_nona_index.shape}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "# mdalibpy.ml.CovarianceThreshold\n",
    "\n",
    "mdalibpy.ml = importlib.reload(mdalibpy.ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X is [observation, features]\n",
    "X_corr = X.values\n",
    "print(f'{X_corr.shape=}')\n",
    "threshold=0.99\n",
    "cov_thres = mdalibpy.ml.CovarianceThreshold(threshold=threshold).fit(X_corr)\n",
    "print('threshold : {}\\n'\n",
    "      'Original feat. / Low Corr. feat. : {}/{}\\n'\n",
    "      'Ratio (Original / Low Corr) feat. : {:4.2f}'\n",
    "      .format(cov_thres.threshold,\n",
    "      X_corr.shape[1],\n",
    "      cov_thres.n_features_,\n",
    "      cov_thres.n_features_/X_corr.shape[1]))\n",
    "print('Correltation matrix statistics: ')\n",
    "print(cov_thres.get_statistics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "il1 = np.tril_indices(X_corr.shape[1])\n",
    "frequencies, edges = np.histogram(cov_thres.get_corr_matrix()[il1], 128)\n",
    "hv.extension('bokeh')\n",
    "(\n",
    " hv.Image(cov_thres.get_corr_matrix(),label='Covariance Matrix')+\\\n",
    " hv.Image(cov_thres.get_corr_matrix(masked=True),label='Covariance Matrix (x>{} masked)'.format(threshold))+\\\n",
    " hv.Histogram((edges, frequencies),kdims=['values'])*hv.VLine(threshold)\n",
    ").opts(\n",
    "   hv.opts.Image(width=600,height=600,tools=['hover'],cmap='viridis'),\n",
    "   hv.opts.Points (color='black', marker='x', size=20),\n",
    "   hv.opts.RGB(width=600,height=600,tools=['hover']),\n",
    "   hv.opts.Histogram(width=1200,height=200),\n",
    "   hv.opts.VLine(color='red', line_width=4)\n",
    ").cols(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input for learning \n",
    "X = spectral_df.loc[spectral_df_nona_index]\n",
    "\n",
    "# X = cov_thres.transform(X)\n",
    "# X = X_pca # PCA\n",
    "\n",
    "original_dim = X.shape[1] # original 784\n",
    "intermediate_dim = original_dim // 3 # original 256\n",
    "latent_dim = 2\n",
    "\n",
    "print(f'{X.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # train the VAE on spectral data\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# x_train, x_test, y_train, y_test = train_test_split( X, y, test_size=1/10, random_state=42)\n",
    "\n",
    "# x_train = x_train.reshape(-1, original_dim) / 255.\n",
    "# x_test = x_test.reshape(-1, original_dim) / 255.\n",
    "# x_train.shape, y_train.shape, x_test.shape, y_test.shape, type(x_train), type(y_train), type(x_test), type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Build the encoder\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(original_dim,))\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(encoder_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Build the decoder\n",
    "from keras.models import Sequential\n",
    "\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "decoder_outputs = keras.Sequential([\n",
    "    layers.Dense(intermediate_dim, input_dim=latent_dim, activation='relu'),\n",
    "    layers.Dense(original_dim, activation='sigmoid')\n",
    "])(latent_inputs)\n",
    "\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the VAE as a Model with a custom train_step\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction),axis=-1\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vae.fit(X,\n",
    "        epochs=15,\n",
    "        batch_size=1280)\n",
    "# display a 2D plot of the digit classes in the latent space\n",
    "z_mean, _, _ = vae.encoder.predict(X)\n",
    "print(f'{z_mean.std(axis=0)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
    "# plt.scatter(z_mean[:, 0], z_mean[:, 1])\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"z[0]\")\n",
    "plt.ylabel(\"z[1]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "z_mean_df = pd.DataFrame(data=z_mean)\n",
    "\n",
    "hds.spread(\n",
    "z_mean_df.hvplot.scatter(x='0',y='1',\n",
    "                    rasterize=True,aggregator='count',dynamic=True,\n",
    "                    x_sampling=1,y_sampling=1,cmap='rainbow'\n",
    "                    )\n",
    "          ,px=3)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
