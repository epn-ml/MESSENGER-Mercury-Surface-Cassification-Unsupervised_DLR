{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Title \n",
    "\n",
    "**Automated surface mapping via unsupervised learning and classification of Mercury Visible--Near-Infrared reflectance spectra**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T14:24:33.808982Z",
     "iopub.status.busy": "2022-12-20T14:24:33.808666Z",
     "iopub.status.idle": "2022-12-20T14:24:33.814773Z",
     "shell.execute_reply": "2022-12-20T14:24:33.814260Z",
     "shell.execute_reply.started": "2022-12-20T14:24:33.808957Z"
    }
   },
   "source": [
    "**Abstract**\n",
    "\n",
    "In this work we apply unsupervised learning techniques for  dimensionality reduction and clustering to remote sensing  hyperspectral Visible-Near Infrared (VNIR) reflectance spectra  datasets of the planet Mercury obtained by the MErcury Surface, Space  ENvironment, GEochemistry, and Ranging (MESSENGER) mission.\n",
    "This  approach produces cluster maps, which group different regions of the  surface based on the properties of their spectra as inferred during  the learning process.\n",
    "While results depend on the choice of model  parameters and available data, comparison to expert-generated geologic  maps shows that some clusters correspond to expert-mapped classes such  as smooth plains on Mercury.\n",
    "These automatically generated maps can  serve as a starting point or comparison for traditional methods of  creating geologic maps based on spectral patterns.\n",
    "\n",
    "\n",
    "The code and data  used in this work is available as python jupyter notebook on the  github public repository  [MESSENGER-Mercury-Surface-Cassification-Unsupervised_DLR](https://github.com/epn-ml/MESSENGER-Mercury-Surface-Cassification-Unsupervised_DLR) funded by the European Union's Horizon 2020 grant No 871149.\n",
    "\n",
    "Authors:\n",
    "- Mario D'Amore$^1$\n",
    "- Sebastiano Padovan$^{1,2,3}$\n",
    "\n",
    "Affiliations : \n",
    "\n",
    "-  $^1$German Aerospace Center (DLR), Rutherfordstra√üe 2, 12489 Berlin,Germany\n",
    "-  $^2$EUMETSAT, Eumetsat Allee 1, 64295 Darmstadt, Germany\n",
    "-  $^3$WGS, Berliner Allee 47, 64295 Darmstadt, Germany\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports\n",
    "\n",
    "Generic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:29:29.298095Z",
     "start_time": "2020-02-04T12:29:23.562215Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import fiona\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg') # non interactive\n",
    "#%matplotlib qt # for not-notebook\n",
    "%matplotlib inline\n",
    "from   matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "matplotlib.rcParams['xtick.labelsize'] = 16\n",
    "matplotlib.rcParams['ytick.labelsize'] = 16\n",
    "matplotlib.rcParams['axes.titlesize'] = 24\n",
    "matplotlib.rcParams['axes.labelsize'] = 20\n",
    "matplotlib.rcParams['font.size'] = 16\n",
    "\n",
    "pd.set_option('display.width',150)\n",
    "pd.set_option('display.max_colwidth',150)\n",
    "pd.set_option('display.max_rows',150)\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:29:29.298095Z",
     "start_time": "2020-02-04T12:29:23.562215Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datashader as ds\n",
    "import holoviews as hv\n",
    "import holoviews.operation.datashader as hds\n",
    "import hvplot.pandas\n",
    "# hv.extension('bokeh','matplotlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore watnings, some holoviews calls should be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define auxiliary functions & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:29:29.376411Z",
     "start_time": "2020-02-04T12:29:29.367525Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_path = pathlib.Path('..')                     # local base path\n",
    "input_data_path = base_path / 'data/processed'  # input data location \n",
    "out_figure_path = base_path / 'reports/figures' # output location <- CHANGE THIS TO YOUR LIKING\n",
    "out_models_path = base_path / 'models' # output location <- CHANGE THIS TO YOUR LIKING\n",
    "\n",
    "print(f'{base_path=}')\n",
    "print(f'{input_data_path=}')\n",
    "print(f'{out_models_path=}')\n",
    "\n",
    "\n",
    "# this globally saves all generated plot with save_plot defind below\n",
    "save_plots_bool = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:29:29.341273Z",
     "start_time": "2020-02-04T12:29:29.305706Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a figure\n",
    "def make_map(in_data,alpha,norm,interpolation=None):\n",
    "    '''\n",
    "    outdf_gdf.loc[spectral_df_nona_index,'R'] = spectral_df[in_wav]\n",
    "    '''\n",
    "    \n",
    "    outdf_gdf.loc[spectral_df_nona_index,'R'] = in_data\n",
    "\n",
    "    map_crs = ccrs.PlateCarree(central_longitude=0.0)\n",
    "    fig, ax = plt.subplots(nrows=1,ncols=1,figsize=[18,18],subplot_kw={'projection': map_crs})\n",
    "    background = ax.imshow(img,\n",
    "                            cmap=plt.cm.gray,\n",
    "                            extent=img_extent,\n",
    "                            origin='upper',\n",
    "                            transform=ccrs.PlateCarree(central_longitude=0.0)\n",
    "                          );\n",
    "    # Here we are using the numpy reshape because we now the final image shape: it is not always the case!!\n",
    "    # This is FASTER then Geopandas.GeoDataFrame.plot!!!!\n",
    "    im = ax.imshow(outdf_gdf.sort_index()['R'].values.reshape(360,180).T,\n",
    "               interpolation= interpolation,\n",
    "               extent= data_img_extent,\n",
    "               cmap=plt.cm.Spectral_r,\n",
    "               transform=ccrs.PlateCarree(central_longitude=0.0),\n",
    "               origin='upper',\n",
    "               alpha=alpha,\n",
    "               # vmax=0.065,\n",
    "               norm=norm,\n",
    "                  );\n",
    "    return im\n",
    "\n",
    "def save_plot(out_file,\n",
    "              output_dir = out_figure_path,\n",
    "              dpi=150,\n",
    "              out_format='jpg',\n",
    "              save=False):\n",
    "    ''' helper function to save previous plot\n",
    "    '''\n",
    "    \n",
    "    out_path = output_dir / (out_file +f'.{out_format}' )\n",
    "    if save :\n",
    "        plt.savefig(out_path,dpi=dpi)\n",
    "        return (f'Saving image to {out_path}')\n",
    "    else:\n",
    "        return (f'NOT saving image to {out_path}')  \n",
    "\n",
    "def df_shader(in_data,**kwargs):\n",
    "    '''\n",
    "    accept input : spectral_df[in_wav] \n",
    "    add it to outdf_gdf:\n",
    "    outdf_gdf.loc[spectral_df_nona_index,'R'] = spectral_df[in_wav]\n",
    "    \n",
    "    returns shader(hv.Points(outdf_gdf.loc[spectral_df_nona_index,'R']))\n",
    "    '''\n",
    "    vdims = 'R'\n",
    "    kdims=[('x','longitude'),('y','latitude')]\n",
    "    outdf_gdf.loc[spectral_df_nona_index,'R'] = in_data\n",
    "\n",
    "    return shader(hv.Points(outdf_gdf.loc[spectral_df_nona_index,['x','y','R']],kdims=kdims,vdims=vdims),**kwargs)\n",
    "\n",
    "\n",
    "def df_rasterer(in_data,**kwargs):\n",
    "    '''\n",
    "    accept input : spectral_df[in_wav] \n",
    "    add it to outdf_gdf:\n",
    "    outdf_gdf.loc[spectral_df_nona_index,'R'] = spectral_df[in_wav]\n",
    "    \n",
    "    returns shader(hv.Points(outdf_gdf.loc[spectral_df_nona_index,'R']))\n",
    "    '''\n",
    "    vdims = 'R'\n",
    "    kdims=[('x','longitude'),('y','latitude')]\n",
    "    outdf_gdf.loc[spectral_df_nona_index,'R'] = in_data\n",
    "\n",
    "    return rasterer(hv.Points(outdf_gdf.loc[spectral_df_nona_index,['x','y','R']],kdims=kdims,vdims=vdims).opts(),**kwargs).opts()\n",
    "\n",
    "\n",
    "def shader(ppoints,aggregator=ds.mean(),x_sampling=1,y_sampling=1,cmap=plt.cm.Spectral_r, dynamic=True):\n",
    "    return hds.shade(\n",
    "                hds.rasterize(ppoints,\n",
    "                              aggregator=aggregator,\n",
    "                              x_sampling=x_sampling,\n",
    "                              y_sampling=y_sampling),\n",
    "                              cmap=cmap,\n",
    "                              dynamic=dynamic,\n",
    "                              )\n",
    "\n",
    "def rasterer(ppoints,aggregator=ds.mean(),x_sampling=2,y_sampling=2,dynamic=True):\n",
    "    return hds.rasterize(ppoints,\n",
    "                         aggregator=aggregator,\n",
    "                         x_sampling=x_sampling,\n",
    "                         y_sampling=y_sampling,\n",
    "                         dynamic=dynamic,\n",
    "                        )\n",
    "\n",
    "\n",
    "def colorbar_img_shader(in_data,cmap=plt.cm.Spectral_r,**kwargs):\n",
    "    \n",
    "    raster = df_rasterer(in_data,**kwargs).opts(alpha=1,colorbar=True,cmap=cmap)\n",
    "    \n",
    "    return hv.Overlay([raster,\n",
    "                        hds.shade(raster,cmap=cmap,group='datashaded'),\n",
    "                        background.opts(cmap='Gray',alpha=0.5),\n",
    "                      ])\n",
    "# .collate()\n",
    "\n",
    "def find_nearest_in_array(\n",
    "                 value,\n",
    "                 array,\n",
    "                 return_value=False):\n",
    "    \"\"\"Find nearest value in a numpy array\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    value : value to search for\n",
    "    array : array to search in, should be sorted.\n",
    "    return_value : bool, if to return the actual values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "##TODO add sorting check/option to sort ? make sense?\n",
    "    import numpy as np\n",
    "\n",
    "    closest_index = (np.abs(array - value)).argmin()\n",
    "\n",
    "    if value > np.nanmax(array):\n",
    "        import warnings\n",
    "        warnings.warn(f'value > np.nanmax(array) : {value} > {np.nanmax(array)}')\n",
    "\n",
    "    if value < np.nanmin(array):\n",
    "        import warnings\n",
    "        warnings.warn(f'value < np.nanmin(array) : {value} < {np.nanmax(array)}')\n",
    "\n",
    "    if not return_value: \n",
    "        return closest_index\n",
    "    else:\n",
    "        return closest_index, array[closest_index]\n",
    "\n",
    "\n",
    "def get_mascs_geojson(file, gzipped=True):\n",
    "    \"\"\"return a geopandas.DataFrame from a geojson, could be compressed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    file : path of geojson compressed file to geopandas dataframe\n",
    "    gzipped : bool, if compressed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    geopandas.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    import geopandas as gpd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import gzip\n",
    "    from geopandas import GeoDataFrame\n",
    "\n",
    "    if gzipped:\n",
    "        # get the compressed geojson\n",
    "        with gzip.GzipFile(file, 'r') as fin:\n",
    "            geodata = json.loads(fin.read().decode('utf-8'))\n",
    "    else:\n",
    "        # get the uncompressed geojson\n",
    "        with open(file, 'r') as fin:\n",
    "            geodata = json.load(fin)\n",
    "\n",
    "    import shapely\n",
    "    # extract geometries\n",
    "    geometries = [shapely.geometry.Polygon(g['geometry']['coordinates'][0]) for g in geodata['features']]\n",
    "    # extract id\n",
    "    ids = [int(g['id']) for g in geodata['features']]\n",
    "    # generate GeoDataFrame\n",
    "    out_gdf = gpd.GeoDataFrame(data=[g['properties'] for g in geodata['features']], geometry=geometries, index=ids).sort_index()\n",
    "    # cast arrays to numpy\n",
    "    if 'array' in out_gdf:\n",
    "        out_gdf['array'] = out_gdf['array'].apply(lambda x: np.array(x).astype(float))\n",
    "\n",
    "    return out_gdf\n",
    "\n",
    "#CRS from  https://github.com/Melown/vts-registry/blob/master/registry/registry/srs.json\n",
    "mercury_crs = {\n",
    "    \"geographic-dmercury2000\": {\n",
    "        \"comment\": \"Geographic, DMercury2000 (iau2000:19900)\",\n",
    "        \"srsDef\": \"+proj=longlat +a=2439700 +b=2439700 +no_defs\",\n",
    "        \"type\": \"geographic\"\n",
    "    },\n",
    "    \"geocentric-dmercury2000\": {\n",
    "        \"comment\": \"Geocentric, Mercury\",\n",
    "        \"srsDef\": \"+proj=geocent +a=2439700 +b=2439700 +lon_0=0 +units=m +no_defs\",\n",
    "        \"type\": \"cartesian\"\n",
    "    },\n",
    "    \"eqc-dmercury2000\": {\n",
    "        \"comment\": \"Equidistant Cylindrical, DMercury2000 (iau2000:19911)\",\n",
    "        \"srsDef\": \"+proj=eqc +lat_ts=0 +lat_0=0 +lon_0=0 +x_0=0 +y_0=0 +a=2439700 +b=2439700 +units=m +no_defs\",\n",
    "        \"type\": \"projected\"\n",
    "    },\n",
    "    \"merc-dmercury2000\": {\n",
    "        \"comment\": \"Mercator, DMercury2000 (iau2000:19974)\",\n",
    "        \"srsDef\": \"+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +a=2439700 +b=2439700 +units=m +no_defs\",\n",
    "        \"type\": \"projected\"\n",
    "    },\n",
    "    \"steren-dmercury2000\": {\n",
    "        \"comment\": \"Polar Sterographic North, DMercury2000 (iau2000:19918)\",\n",
    "        \"srsDef\": \"+proj=stere +lat_0=90 +lat_ts=90 +lon_0=0 +k=1 +x_0=0 +y_0=0 +a=2439700 +b=2439700 +units=m +no_defs\",\n",
    "        \"type\": \"projected\"\n",
    "    },\n",
    "    \"steres-dmercury2000\": {\n",
    "        \"comment\": \"Polar Stereographic South, DMercury2000 (iau2000:19920)\",\n",
    "        \"srsDef\": \"+proj=stere +lat_0=-90 +lat_ts=-90 +lon_0=0 +k=1 +x_0=0 +y_0=0 +a=2439700 +b=2439700 +units=m +no_defs\",\n",
    "        \"type\": \"projected\"\n",
    "}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:29:29.387455Z",
     "start_time": "2020-02-04T12:29:29.381055Z"
    },
    "tags": []
   },
   "source": [
    "## Load Data\n",
    "\n",
    "Data are too big to be included in this repo, user can find it on [Zenodo](https://zenodo.org/record/7433033) at [https://zenodo.org/record/7433033](https://zenodo.org/record/7433033).\n",
    "\n",
    "Download the datafile `grid_2D_-180_+180_-90_+90_1deg_st_median_photom_iof_sp_2nm.geojson.gz` in  `data/processed` with some variation of\n",
    "\n",
    "```bash\n",
    "curl https://zenodo.org/record/7433033/files/grid_2D_0_360_-90_%2B90_1deg_st_median_photom_iof_sp_2nm.png --output data/processed/grid_2D_-180_+180_-90_+90_1deg_st_median_photom_iof_sp_2nm.geojson.gz\n",
    "```\n",
    "\n",
    "This is a preview of the data cube from Zenodo.\n",
    "\n",
    "![Preview od the data cube from Zenodo](https://zenodo.org/api/iiif/v2/c98bb0bc-cfa1-449e-94f7-95f9d074543e:f4cc114a-fac9-42b8-b5a8-380901fe8dba:grid_2D_0_360_-90_%2B90_1deg_st_median_photom_iof_sp_2nm.png/full/750,/0/default.png)\n",
    "\n",
    "Create the wavelenghts array and its helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:29:29.387455Z",
     "start_time": "2020-02-04T12:29:29.381055Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the wavelenghts array\n",
    "wav_grid_2nm = np.arange(260,1052,2)\n",
    "\n",
    "# define find_nearest, with wav_grid_2nm as default array\n",
    "find_nearest = lambda x : find_nearest_in_array(x,wav_grid_2nm)\n",
    "\n",
    "# this is to index an array based on wav_grid_2nm\n",
    "wavelenght = 415\n",
    "print(f'         wavelenght = {wavelenght:5d} <-- number we search for')\n",
    "print(f'find_nearest({wavelenght:5d}) = {find_nearest(wavelenght):5d} <-- index of {wavelenght} in wav_grid_2nm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:29:29.399368Z",
     "start_time": "2020-02-04T12:29:29.394878Z"
    },
    "execution": {
     "iopub.execute_input": "2022-05-24T08:38:14.625138Z",
     "iopub.status.busy": "2022-05-24T08:38:14.624914Z",
     "iopub.status.idle": "2022-05-24T08:38:14.628307Z",
     "shell.execute_reply": "2022-05-24T08:38:14.627826Z",
     "shell.execute_reply.started": "2022-05-24T08:38:14.625122Z"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Select input data. From MASCS documentation :  \n",
    "\n",
    "```\n",
    "PHOTOM_IOF_SPECTRUM_DATA : \n",
    "   Derived column of photometrically normalized \n",
    "   reflectance-at-sensor spectra. One row per spectrum. NIR spectrum has up \n",
    "   to 256 values (depending on binning and windowing), VIS has up to 512. \n",
    "   Reflectance is a unitless parameter. Reflectance from saturated pixels, \n",
    "   or binned pixels with one saturated element, are set to 1e32. PER \n",
    "   SPECTRUM column.\"\n",
    "\n",
    "IOF_SPECTRUM_DATA : \n",
    "   DESCRIPTION = \"Derived column of reflectance-at-sensor spectra. One \n",
    "   row per spectrum. NIR spectrum has up to 256 values (depending on binning \n",
    "   and windowing), VIS has up to 512. Reflectance is a unitless parameter. \n",
    "   Reflectance from saturated pixels, or binned pixels with one saturated \n",
    "   element, are set to 1e32. PER SPECTRUM column.\"\n",
    "```\n",
    "\n",
    "define the datafile with filename structure:\n",
    "\n",
    "```\n",
    "[description from database]_[function applied to the spectra for each pixel]_[data array used]\n",
    "    [description from database] = grid_2D_0_360_-90_+90\n",
    "    [function applied to the spectra for each pixel] = avg or st_median\n",
    "    [data array used] = iof_sp_2nm or photom_iof_sp_2nm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:29:29.399368Z",
     "start_time": "2020-02-04T12:29:29.394878Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_data_name = 'grid_2D_-180_+180_-90_+90_1deg_st_median_photom_iof_sp_2nm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are in a gzipped geojson to reduce size, but geopandas doesn't like it.\n",
    "\n",
    "The function below accept a path and return a GeoDataFrame.\n",
    "\n",
    "An optional `gzipped[=True default]` keywords take care of compressed geojson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:29:47.380230Z",
     "start_time": "2020-02-04T12:29:29.404990Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "outdf_gdf = get_mascs_geojson( input_data_path / (input_data_name+'.geojson.gz'), gzipped=True)\n",
    "# outdf_gdf = get_mascs_geojson( input_data_path / (input_data_name+'.geojson.gz'), gzipped=True, cast_to_numeric=False)\n",
    "\n",
    "# this is to be sure that the cells are ordered in natural way == reshape with numpy\n",
    "outdf_gdf = outdf_gdf.set_index('natural_index',drop=True).sort_index()\n",
    "import fiona.crs\n",
    "\n",
    "# set Mercury Lat/Lon as crs\n",
    "outdf_gdf.crs = fiona.crs.from_string(mercury_crs['geographic-dmercury2000']['srsDef'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unravel spectral reflectance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:29:58.859511Z",
     "start_time": "2020-02-04T12:29:47.385237Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create wavelenghts columns: this create empy columns with np.nan (nice!)\n",
    "# use a separate df, because mixed types columns are crazy. and buggy\n",
    "spectral_df = pd.DataFrame(index=outdf_gdf.index,columns = wav_grid_2nm).fillna(np.nan)\n",
    "## assign single wavelenght to columns, only where array vectors len !=0\n",
    "spectral_df.loc[outdf_gdf['array'].apply(lambda x : len(x)) != 0, wav_grid_2nm] = np.stack(outdf_gdf.loc[outdf_gdf['array'].apply(lambda x : len(x)) != 0,'array'], axis=0).astype(np.float64)\n",
    "## drop array column\n",
    "outdf_gdf.drop(columns=['array'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:13.478705Z",
     "start_time": "2020-02-04T12:29:58.864249Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create x and y cols = lon and lat\n",
    "outdf_gdf['x'] = outdf_gdf.apply(lambda x: x['geometry'].centroid.x , axis=1)\n",
    "outdf_gdf['y'] = outdf_gdf.apply(lambda x: x['geometry'].centroid.y , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:19.913044Z",
     "start_time": "2020-02-04T12:30:13.491986Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop outlier : this clean up further noisy data, instrumental effect, etc\n",
    "print(spectral_df.shape)\n",
    "low = .02\n",
    "high = .999\n",
    "quant_df = spectral_df.quantile([low, high])\n",
    "spectral_df =  spectral_df[spectral_df >= 0].apply(lambda x: x[(x>quant_df.loc[low,x.name]) &\\\n",
    "                                       (x < quant_df.loc[high,x.name])], axis=0)\\\n",
    "                                       \n",
    "print(spectral_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:19.923399Z",
     "start_time": "2020-02-04T12:30:19.917834Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cut to stop_wav\n",
    "# iloc doesn't support int columns indexing!!!!\n",
    "start_wav = 268 # below all NaN\n",
    "stop_wav  = 975 # above a bump in NaN \n",
    "\n",
    "spectral_df = spectral_df.iloc[:,find_nearest(start_wav):find_nearest(stop_wav)+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:20.736248Z",
     "start_time": "2020-02-04T12:30:19.928238Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count nan\n",
    "print(f'{spectral_df.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:20.736248Z",
     "start_time": "2020-02-04T12:30:19.928238Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2,figsize=(19,1.5))\n",
    "\n",
    "spectral_df.isna().sum(axis=0).plot(ax=axs[0]);\n",
    "spectral_df.isna().sum(axis=1).plot(ax=axs[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(outdf_gdf.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:04:34.277075Z",
     "start_time": "2020-02-04T13:04:14.474084Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "print(spectral_df.shape, spectral_df.dropna(axis=0,how='any').shape)\n",
    "\n",
    "display(spectral_df.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:04:34.277075Z",
     "start_time": "2020-02-04T13:04:14.474084Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# whole data distribution \n",
    "fig, axarr = plt.subplots(nrows=1, ncols=2,figsize=(19,2))\n",
    "sns.histplot(spectral_df.dropna(how='any').values.flatten(), ax= axarr[0],bins = 255,alpha=0.4, kde=True,edgecolor='none');\n",
    "sns.histplot(spectral_df.dropna(how='any').values.flatten(), ax= axarr[1],bins = 255,alpha=0.4, kde=True,edgecolor='none',log_scale=(False, True),);\n",
    "\n",
    "save_plot('whole_data_distribution_seaborn',save=save_plots_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:22.611537Z",
     "start_time": "2020-02-04T12:29:21.322Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = spectral_df.dropna(how='any').values.T\n",
    "\n",
    "from skimage import exposure\n",
    "\n",
    "# Equalization : paramterless\n",
    "plt.figure(figsize=[24,8]);\n",
    "plt.imshow(exposure.equalize_hist(img),\n",
    "           interpolation='bicubic',\n",
    "           aspect='auto',\n",
    "           cmap=plt.cm.Spectral_r);\n",
    "# [Colorbar Tick Labelling Demo ‚Äî Matplotlib 3.1.2 documentation](https://matplotlib.org/3.1.1/gallery/ticks_and_spines/colorbar_tick_labelling_demo.html)\n",
    "cbar = plt.colorbar(ticks=[0.01, 0.5, 1], orientation='vertical')\n",
    "cbar.ax.set_yticklabels(\n",
    "    np.around([np.percentile(img,0.01), np.median(img), np.nanmax(img)],decimals=3)\n",
    "    );\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_plot('spectrogram',save=save_plots_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:04:34.533701Z",
     "start_time": "2020-02-04T13:04:34.282887Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keep the old name? naa\n",
    "spectral_df_nona_index = spectral_df.dropna(how='any').index\n",
    "\n",
    "print(f'        outdf_gdf : {outdf_gdf.shape}')\n",
    "print(f'      spectral_df : {spectral_df.shape}')\n",
    "print(f'spectral_df_nonan : {spectral_df_nona_index.shape}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:04:34.569542Z",
     "start_time": "2020-02-04T13:04:34.538642Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define 2 wavelenghts and calculate something \n",
    "in_wav = 450\n",
    "en_wav = 750\n",
    "\n",
    "idx_in = find_nearest(in_wav)\n",
    "idx_en = find_nearest(en_wav)\n",
    "\n",
    "# store one wavelenght in the geometry table\n",
    "outdf_gdf.loc[spectral_df_nona_index,'refl'] = spectral_df[en_wav]/spectral_df[in_wav]\n",
    "\n",
    "print(f'(wav[{idx_in}],wav[{idx_en}]) = ({wav_grid_2nm[idx_in]}, {wav_grid_2nm[idx_en]}) \\nspectral[:,idx_in:idx_en].shape : {spectral_df.loc[:,in_wav:en_wav].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T08:47:32.012197Z",
     "iopub.status.busy": "2023-01-18T08:47:32.011862Z",
     "iopub.status.idle": "2023-01-18T08:47:32.398997Z",
     "shell.execute_reply": "2023-01-18T08:47:32.398496Z",
     "shell.execute_reply.started": "2023-01-18T08:47:32.012168Z"
    },
    "tags": []
   },
   "source": [
    "<hr>\n",
    "**Test some plot** using [Introduction ‚Äî hvPlot 0.8.2 documentation](https://hvplot.holoviz.org/user_guide/Introduction.html) \n",
    "\n",
    "Seting holoview backend mess with matplotlib inline, I must set it again after some holoview plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select some spectra and extract the indexes\n",
    "test_spectra_index = spectral_df.loc[spectral_df_nona_index].sample(25).index\n",
    "\n",
    "#create a plot with 2 panel\n",
    "fig, axs = fig, axs = plt.subplots(ncols=2, figsize=[20,6])\n",
    "\n",
    "# plot spectra\n",
    "spectral_df.loc[test_spectra_index].T.plot(ax=axs[0],legend=False)\n",
    "\n",
    "# plot the average \n",
    "spectral_df.loc[test_spectra_index].mean(axis=0).plot(ax=axs[0],lw=10,color='black',alpha=0.5)\n",
    "\n",
    "outdf_gdf.loc[test_spectra_index].reset_index(drop=False).plot.scatter(x='x',y='y',ax=axs[1],size=30,color='refl',cmap=plt.cm.Spectral_r);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(spectral_df.loc[test_spectra_index].T.hvplot()*\\\n",
    "spectral_df.loc[test_spectra_index].mean(axis=0).hvplot().opts(line_width=10,line_dash='solid',color='black',alpha=0.5)\n",
    ")\\\n",
    "+outdf_gdf.loc[test_spectra_index].drop(columns=['geometry']).hvplot.scatter(x='x',y='y',c='refl',cmap=plt.cm.Spectral_r).opts(hv.opts.Scatter(marker='circle',size=20,alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:04:34.580107Z",
     "start_time": "2020-02-04T13:04:34.575398Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate rows & cols from grid properties files, assuming regular grid\n",
    "rows, cols = 360, 180\n",
    "rows_half_step, cols_half_step = 1, 1\n",
    "\n",
    "data_img_extent = [-180.0, 180.0, -90.0, 90.0]\n",
    "\n",
    "# extent = [outdf_gdf.total_bounds[i] for i in [0,2,1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:22.626542Z",
     "start_time": "2020-02-04T12:29:21.337Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specific wavelengths data distribution \n",
    "\n",
    "fig, axarr = plt.subplots(nrows=1, ncols=1,figsize=(20,4))\n",
    "\n",
    "plot_wav = 300\n",
    "ax = sns.histplot(spectral_df[plot_wav].dropna(),ax= axarr,bins = 255,stat=\"density\", alpha=0.4, kde=True,edgecolor='none');\n",
    "ax.set_xlim([0.005,0.075])\n",
    "plot_wav = 700\n",
    "sns.histplot(spectral_df[plot_wav].dropna(),ax= axarr,bins = 255,stat=\"density\", alpha=0.4, kde=True,edgecolor='none');\n",
    "plot_wav = 900\n",
    "sns.histplot(spectral_df[plot_wav].dropna(),ax= axarr,bins = 255,stat=\"density\", alpha=0.4, kde=True,edgecolor='none');\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_plot('global_300nm_700nm_900nm_distribution', out_format='png',save=save_plots_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:04:39.721458Z",
     "start_time": "2020-02-04T13:04:39.578146Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "# from scipy import misc\n",
    "import imageio\n",
    "from skimage import transform \n",
    "# read background image\n",
    "img = imageio.imread( input_data_path / '1280x640_20120330_monochrome_basemap_1000mpp_equirectangular.png')\n",
    "img_extent = (-180, 180, -90, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:04:39.993323Z",
     "start_time": "2020-02-04T13:04:39.837695Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "\n",
    "from PIL import Image, ImageEnhance\n",
    "kdims=[('x','latitude'),('y','longitude')]\n",
    "\n",
    "backimage = Image.open( input_data_path / '1280x640_20120330_monochrome_basemap_1000mpp_equirectangular.png')\n",
    "## from help :  as four-tuple defining the (left, bottom, right and top) edges.\n",
    "hv_img_extent = (-180, -90,180, 90)\n",
    "\n",
    "background = hv.Image(\n",
    "            np.array(backimage),\n",
    "            bounds=hv_img_extent,\n",
    "            kdims=kdims,\n",
    "            group='backplane',\n",
    "            ).opts(cmap='Gray',clone=False)\n",
    "\n",
    "# # show the background image\n",
    "# background.options(width=800,height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "map_crs = ccrs.PlateCarree(central_longitude=0.0)\n",
    "fig, ax = plt.subplots(nrows=1,ncols=1,figsize=[18,18],subplot_kw={'projection': map_crs})\n",
    "background_mpl = ax.imshow(img,\n",
    "                        cmap=plt.cm.gray,\n",
    "                        extent=img_extent,\n",
    "                        origin='upper',\n",
    "                        transform=ccrs.PlateCarree(central_longitude=0.0)\n",
    "                      );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:22.641152Z",
     "start_time": "2020-02-04T12:29:21.354Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('matplotlib')\n",
    "# hv.output(fig='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#     vdims = 'R'\n",
    "#     kdims=[('x','longitude'),('y','latitude')]\n",
    "#     outdf_gdf.loc[spectral_df_nona_index,'R'] = in_data\n",
    "\n",
    "#     return shader(hv.Points(outdf_gdf.loc[spectral_df_nona_index,['x','y','R']],kdims=kdims,vdims=vdims).opts(),**kwargs).opts()\n",
    "\n",
    "spectral_df[970]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:22.641152Z",
     "start_time": "2020-02-04T12:29:21.354Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('matplotlib')\n",
    "\n",
    "# spectral slope R[970]-R[270] / 270 -970\n",
    "(background.opts(cmap='Gray',alpha=0.5)*\\\n",
    " df_shader((spectral_df[970]-spectral_df[270])/700.,cmap=plt.cm.Spectral_r).opts(interpolation='bilinear',alpha=0.5)).\\\n",
    "    opts(\n",
    "    fig_inches=4,\n",
    "    aspect=2,\n",
    "    fig_size=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:22.648536Z",
     "start_time": "2020-02-04T12:29:21.363Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('matplotlib')\n",
    "plot_wav = 700\n",
    "\n",
    "out = colorbar_img_shader(spectral_df[plot_wav])\n",
    "_ = out.DynamicMap.II.opts(interpolation='None',aspect=1.8,fig_size=200,alpha=0.7)\n",
    "out.collate()\n",
    "# hv.save(out,out_figure_path / '1b_mascs_700nm_refl.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:30:22.652101Z",
     "start_time": "2020-02-04T12:29:21.367Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "background.opts(cmap='Gray')*df_shader(spectral_df[plot_wav],cmap=plt.cm.inferno).opts(height=600,width=1000,alpha=0.7)\n",
    "\n",
    "out = colorbar_img_shader(spectral_df[plot_wav])\n",
    "_ = out.DynamicMap.II.opts(height=400,width=800,alpha=0.75)\n",
    "out.collate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T12:50:03.942765Z",
     "start_time": "2020-02-05T12:50:03.753437Z"
    },
    "lines_to_next_cell": 2,
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# small_data == spectral_df[spectral_df_nonan]\n",
    "X = spectral_df.loc[spectral_df_nona_index]\n",
    "\n",
    "print(f'        outdf_gdf : {outdf_gdf.shape}')\n",
    "print(f'      spectral_df : {spectral_df.shape}')\n",
    "print(f'spectral_df_nonan : {spectral_df_nona_index.shape}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction\n",
    "\n",
    "[2.5. Decomposing signals in components (matrix factorization problems) ‚Äî scikit-learn 0.20.2 documentation](https://scikit-learn.org/stable/modules/decomposition.html#decompositions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:04:52.140269Z",
     "start_time": "2020-02-04T13:04:52.136044Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from sklearn import model_selection\n",
    "from sklearn import pipeline, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T09:42:52.968069Z",
     "start_time": "2020-01-30T09:42:44.955965Z"
    },
    "hidden": true,
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Principal components analysis : which is the data dimensionality?\n",
    "\n",
    "# explicitely set number of PCA components to comput\n",
    "n_components = 8\n",
    "pca = decomposition.PCA(n_components=n_components)\n",
    "\n",
    "# # let PCA decide the number of components to reconstruct 95% of the total variance \n",
    "# pca = decomposition.PCA(0.99)\n",
    "\n",
    "pca.fit(X)\n",
    "# n_components = pca.n_components_\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "print('X.shape               : {}\\n'\n",
    "      'X_pca.shape           : {}\\n'\n",
    "      'pca.components_.shape : {}'.format(X.shape, X_pca.shape, pca.components_.shape))\n",
    "\n",
    "print(\"              variance       var_ratio      cum_var_ratio\")\n",
    "for i in range(n_components):\n",
    "    print(\"Component %2s: %12.10f   %12.10f   %12.10f\" % (i, pca.explained_variance_[i], pca.explained_variance_ratio_[i], np.cumsum(pca.explained_variance_ratio_)[i]))\n",
    "\n",
    "\n",
    "# As we can see, only the 2 first components are useful\n",
    "# pca.n_components = 2\n",
    "# small_data_pca = pca.fit_transform(small_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### PCA residual error estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T11:32:04.860423Z",
     "start_time": "2020-01-23T11:14:22.223170Z"
    },
    "hidden": true,
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca_error = []\n",
    "\n",
    "cmp_index = np.linspace(0, pca.n_components_, num=pca.n_components_//2, endpoint=True, dtype=int)\n",
    "cmp_index[-1] -= 1\n",
    "\n",
    "for i in cmp_index:\n",
    "# for i in range(pca.n_components):\n",
    "    print(\"Component %2s: %12.10f   %12.10f   %12.10f\" % (i, pca.explained_variance_[i], pca.explained_variance_ratio_[i], np.cumsum(pca.explained_variance_ratio_)[i]))\n",
    "    img = (X-np.dot(X_pca[:,:i],pca.components_[:i])-X.mean()).values\n",
    "    pca_error.append({\n",
    "        'min': img.min(),\n",
    "        'max': img.max(),\n",
    "        'mean' : img.mean(),\n",
    "        'median' : np.median(img),\n",
    "        'std' : np.std(img),\n",
    "        'pca_scores' : np.mean(model_selection.cross_val_score(decomposition.PCA(n_components=n_components).fit(X), X,cv=2))\n",
    "    })\n",
    "    print(pca_error[-1])\n",
    "\n",
    "pca_errors_df = pd.DataFrame.from_dict(pca_error)\n",
    "pca_errors_df['delta'] = pca_errors_df['max']-pca_errors_df['min']\n",
    "pca_errors_df['explained_variance'] = pca.explained_variance_[cmp_index]\n",
    "pca_errors_df['explained_variance_ratio'] = pca.explained_variance_ratio_[cmp_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T14:15:03.216655Z",
     "start_time": "2020-01-23T14:15:03.149687Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca_errors_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T15:13:01.109953Z",
     "start_time": "2020-01-23T15:13:01.000537Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import hvplot.pandas\n",
    "a = (pca_errors_df[['std']]).hvplot().opts(width=1200,height=400)\n",
    "b = (pca_errors_df['delta']).hvplot().opts(width=1200,height=400)\n",
    "c = pca_errors_df['pca_scores'].hvplot().opts(width=1200,height=400)\n",
    "d = pca_errors_df[['max','min']].hvplot().opts(width=1200,height=400)\n",
    "\n",
    "c\n",
    "# ((a+b+c)*hv.HLine(10000*(0.005*0.005)).opts(color='red')).cols(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T14:17:27.025978Z",
     "start_time": "2020-01-23T14:17:07.789533Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from skimage import exposure\n",
    "img = (X-np.dot(X_pca,pca.components_)-X.mean()).values\n",
    "\n",
    "plt.figure(figsize=[20,8]);\n",
    "plt.imshow(exposure.equalize_hist(img,nbins=512),\n",
    "           aspect='auto',\n",
    "           interpolation='bilinear',\n",
    "           cmap=plt.cm.Spectral_r,\n",
    "           extent = [X.columns.min(),X.columns.max(),  X.index.min(), X.index.max()],\n",
    "          );\n",
    "# cbar = plt.colorbar(ticks=[0.01, 0.5, 1], orientation='vertical')\n",
    "# cbar.ax.set_yticklabels(\n",
    "#     np.around([np.percentile(img,0.01), np.median(img), np.nanmax(img)],decimals=3)\n",
    "#     );\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T14:17:32.664128Z",
     "start_time": "2020-01-23T14:17:27.032431Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-20T13:49:04.069103Z",
     "iopub.status.busy": "2022-12-20T13:49:04.068876Z",
     "iopub.status.idle": "2022-12-20T13:49:05.160376Z",
     "shell.execute_reply": "2022-12-20T13:49:05.159672Z",
     "shell.execute_reply.started": "2022-12-20T13:49:04.069089Z"
    },
    "hidden": true,
    "tags": []
   },
   "source": [
    "reconstruct initial data with choosen PCA components  and look at the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T14:17:32.664128Z",
     "start_time": "2020-01-23T14:17:27.032431Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff_img = (X-np.dot(X_pca,pca.components_)-X.mean())\n",
    "fig, axs = plt.subplots(nrows=2,ncols=2,figsize=[20,8])\n",
    "\n",
    "ax = axs.flatten()\n",
    "\n",
    "ax[0].set_title('max')\n",
    "diff_img.max().plot(ax=ax[0])\n",
    "\n",
    "ax[1].set_title('median')\n",
    "diff_img.median().plot(ax=ax[1])\n",
    "\n",
    "ax[2].set_title('min')\n",
    "diff_img.min().plot(ax=ax[2])\n",
    "\n",
    "ax[3].set_title('std')\n",
    "diff_img.std().plot(ax=ax[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T16:27:28.819556Z",
     "start_time": "2020-01-23T16:27:16.088047Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hv.extension('matplotlib')\n",
    "# plot_wav = 500\n",
    "# out = colorbar_img_shader((X-np.dot(X_pca,pca.components_)-X.mean())[plot_wav])\n",
    "# _ = out.DynamicMap.II.opts(interpolation='bicubic',aspect=2,fig_size=400,alpha=0.7)\n",
    "# out\n",
    "\n",
    "# spectral_df.columns.min(),np.quantile(spectral_df.columns,0.25),np.quantile(spectral_df.columns,0.75),spectral_df.columns.max()\n",
    "# (268, 444, 797.5, 974)\n",
    "\n",
    "hv.extension('matplotlib')\n",
    "    \n",
    "NdLayout = hv.NdLayout(\n",
    "            {f'{plot_wav}nm': df_shader( (X-np.dot(X_pca,pca.components_)-X.mean())[plot_wav],x_sampling=2,y_sampling=2).\\\n",
    "            opts(interpolation='bicubic',aspect=2,alpha=0.8)\n",
    "                 for ind,plot_wav in enumerate([270, 470, 770, 970])}\n",
    "            ,kdims='Wav')\n",
    "\n",
    "(background.opts(cmap='Gray',alpha=1)*NdLayout.cols(2)).opts(fig_size=200,tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### PCA visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T09:44:51.703714Z",
     "start_time": "2020-01-30T09:44:50.924263Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "components_shift = 0\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "ax = plt.subplot()\n",
    "ax.plot(np.arange(pca.n_components_-components_shift)+components_shift, pca.explained_variance_ratio_[components_shift:])\n",
    "ax.plot(np.arange(pca.n_components_-components_shift)+components_shift, pca.explained_variance_ratio_[components_shift:], \"r.\")\n",
    "ax.set_title(\"PCA explained variance\")\n",
    "ax.set_yscale('log')\n",
    "# ax.set_xlim([components_shift-1, pca.n_components_])\n",
    "# ax.set_ylim([np.min(pca.explained_variance_ratio_),np.max(pca.explained_variance_ratio_)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.Curve(\n",
    "                   (np.arange(pca.n_components_-components_shift)+components_shift, pca.explained_variance_ratio_[components_shift:]),\n",
    "                   kdims='components',vdims='var. ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T15:11:54.548356Z",
     "start_time": "2020-01-30T15:11:53.888181Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "components_shift = 0\n",
    "hv.extension('bokeh')\n",
    "\n",
    "overlay = hv.Curve(\n",
    "                   (np.arange(pca.n_components_-components_shift)+components_shift, pca.explained_variance_ratio_[components_shift:]),\n",
    "                   kdims='components',vdims='var. ratio')\n",
    "\n",
    "overlay2 = hv.Curve(\n",
    "                    (np.arange(pca.n_components_-components_shift)+components_shift, np.cumsum(pca.explained_variance_ratio_[components_shift:])),\n",
    "                    kdims='components',vdims='var. cumsum')\n",
    "\n",
    "overlay3 = hv.Curve(\n",
    "                    (np.arange(pca.n_components_-components_shift)+components_shift, np.gradient(np.cumsum(pca.explained_variance_ratio_[components_shift:]))),\n",
    "                    kdims='components',vdims='gradient(var. cumsum)')\n",
    "\n",
    "overlay4 = hv.Curve(\n",
    "                    (np.arange(pca.n_components_-components_shift)+components_shift, np.gradient(np.gradient(np.cumsum(pca.explained_variance_ratio_[components_shift:])))) ,\n",
    "                    kdims='components',vdims='$gradient^2$(var. cumsum)' )\n",
    "\n",
    "\n",
    "layout = overlay+overlay2+overlay3+overlay4\n",
    "\n",
    "layout.opts(\n",
    "    hv.opts.Curve(line_width=3,height=500, width=600),\n",
    "    hv.opts.Points(alpha=0.5, size=10),\n",
    ").cols(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T16:27:29.814860Z",
     "start_time": "2020-01-23T16:27:28.824873Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "\n",
    "vdims = ['~Reflectance']\n",
    "kdims=['Wavelenght (um)']\n",
    "\n",
    "width=1100\n",
    "height=500\n",
    "\n",
    "shift = 0.3\n",
    "overlay_dict = {'PCA.{}'.format(ind):hv.Curve((spectral_df.columns.to_numpy(),cmp + shift*(ind+1)),vdims=vdims,kdims=kdims) for ind,cmp in \n",
    "                enumerate(preprocessing.MinMaxScaler().fit_transform(pca.components_.T).T[:4,:])}\n",
    "\n",
    "overlay_dict['Mean'] = hv.Curve((spectral_df.columns.to_numpy(),\n",
    "                  preprocessing.MinMaxScaler().fit_transform(spectral_df.mean().values.reshape(-1, 1)).squeeze()\n",
    "                                ),vdims=vdims,kdims=kdims).opts(line_width=1,line_dash='solid',color='black',alpha=1)\n",
    "\n",
    "# shift_dict = {'PCA.{}-shift'.format(ind):hv.HLine(0.5*ind).opts(\n",
    "#                 line_width=0.25,line_dash='dashed',color='black') for ind in range(4)}\n",
    "\n",
    "\n",
    "hv.NdOverlay(overlay_dict).opts(\n",
    "                hv.opts.Curve(line_width=4, show_grid=True),\n",
    "                hv.opts.NdOverlay(width=width,height=height,legend_cols=4,legend_position='bottom')\n",
    "                ) #* hv.NdOverlay(shift_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T14:18:52.438837Z",
     "start_time": "2020-01-23T14:18:50.705967Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "\n",
    "max_pca_comp = 4\n",
    "\n",
    "NdLayout = hv.NdLayout(\n",
    "            {ind: df_shader(cmp[:,np.newaxis],x_sampling=2,y_sampling=2).\\\n",
    "             options(height=350,width=550,alpha=0.8)\n",
    "                 for ind,cmp in enumerate(X_pca[:,:max_pca_comp].T)}\n",
    "            ,kdims='Component')\n",
    "\n",
    "background.options(cmap='Gray',alpha=0.9)*NdLayout.cols(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T16:27:41.828831Z",
     "start_time": "2020-01-23T16:27:29.820210Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hv.extension('matplotlib')\n",
    "\n",
    "max_pca_comp = 16\n",
    "\n",
    "for ind,cmp in enumerate(X_pca[:,:max_pca_comp].T):\n",
    "    print(f'PCA.{ind:02} min:{cmp.min():10.5} , max:{cmp.max():10.5}, delta:{cmp.max()-cmp.min():10.5}')\n",
    "\n",
    "NdLayout = hv.NdLayout(\n",
    "            {ind: df_shader(cmp[:,np.newaxis],x_sampling=2,y_sampling=2).\\\n",
    "            opts(interpolation='bicubic',aspect=2,alpha=0.7)\n",
    "                 for ind,cmp in enumerate(X_pca[:,:max_pca_comp].T)}\n",
    "            ,kdims='Component')\n",
    "\n",
    "(background.opts(cmap='Gray',alpha=1)*NdLayout.cols(4)).opts(tight=True, vspace=0.01, hspace=0.01, fig_size=150).cols(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T16:27:44.151949Z",
     "start_time": "2020-01-23T16:27:41.834742Z"
    },
    "hidden": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "\n",
    "import itertools\n",
    "gridplot = {}\n",
    "for x,y in itertools.combinations(range(3), 2):\n",
    "    print(x,y,X_pca[:,[x,y]].shape)\n",
    "    gridplot[f'PCA.{x} vs PCA.{y}'] = hv.Points(X_pca[:,[x,y]],kdims=[f'PCA.{x}',f'PCA.{y}'])\n",
    "\n",
    "\n",
    "# # hds.dynspread(\n",
    "hds.datashade(hv.NdLayout(gridplot),aggregator=ds.count(),cmap=plt.cm.viridis, x_sampling=0.003, y_sampling=0.003).\\\n",
    "opts(height=1000,width=1200,tight=True).cols(3)\n",
    "# opts(fig_size=200,aspect_weight=True,tight=True).cols(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T10:51:49.591134Z",
     "start_time": "2020-01-24T10:51:49.565420Z"
    }
   },
   "source": [
    "#### ICA\n",
    "\n",
    "[2.5. Decomposing signals in components (matrix factorization problems) ‚Äî scikit-learn 0.20.2 documentation](https://scikit-learn.org/stable/modules/decomposition.html#independent-component-analysis-ica)\n",
    "\n",
    "From the documentation :\n",
    "\n",
    "Independent component analysis separates a multivariate signal into additive subcomponents that are maximally independent. It is implemented in scikit-learn using the Fast ICA algorithm. Typically, ICA is not used for reducing dimensionality but for separating superimposed signals. Since the ICA model does not include a noise term, for the model to be correct, whitening must be applied. This can be done internally using the whiten argument or manually using one of the PCA variants.\n",
    "\n",
    "It is classically used to separate mixed signals (a problem known as blind source separation).\n",
    "\n",
    "Calculate the reconstruction error for increasing number of ICA components :\n",
    "\n",
    "    print(np.concatenate((np.arange(1,5),np.arange(0,160,20)[1:])))\n",
    "    array([  1,   2,   3,   4,  20,  40,  60,  80, 100, 120, 140])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### ICA residual error estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T10:51:12.198839Z",
     "start_time": "2020-02-04T10:51:12.184046Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ica_rec_error_path = pathlib.Path(out_models_path / 'ica_rec_error_df.csv')\n",
    "\n",
    "# check if we already run and stored this \n",
    "if ica_rec_error_path.is_file():\n",
    "    # load reconstruction error\n",
    "    ica_rec_error_df = pd.read_csv(ica_rec_error_path, index_col='ICA components n.')\n",
    "else:\n",
    "    # calculate reconstruction error\n",
    "    ica_rec_error = {}\n",
    "    for ica_n_components in np.concatenate((np.arange(1,5),np.arange(0,160,20)[1:])) : \n",
    "    #     print(ica_n_components)\n",
    "        ica = decomposition.FastICA(n_components=ica_n_components,random_state=4)\n",
    "        S_  = ica.fit_transform(X)\n",
    "        # evaluate overall reconstruction error\n",
    "        ica_rec_error[ica_n_components] = np.std((X-ica.inverse_transform(S_)).values.flatten())\n",
    "        print(ica_n_components,ica_rec_error[ica_n_components])\n",
    "\n",
    "    ica_rec_error_df = pd.DataFrame.from_dict(ica_rec_error,orient='index')\n",
    "    ica_rec_error_df.index.name = 'ICA components n.'\n",
    "    ica_rec_error_df.columns = ['reconstruction error']\n",
    "    ica_rec_error_df.to_csv(ica_rec_error_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T10:48:14.491420Z",
     "start_time": "2020-02-04T10:48:14.118800Z"
    }
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "display(ica_rec_error_df.sort_index())\n",
    "import hvplot.pandas\n",
    "\n",
    "(hv.HLine(0.0015,group='line')*\\\n",
    " hv.VLine(4,group='line')*\\\n",
    " ica_rec_error_df.sort_index().hvplot()*\\\n",
    " ica_rec_error_df.sort_index().hvplot(kind='scatter')).\\\n",
    "opts(\n",
    "    hv.opts.Points(marker='circle',alpha=0.5),\n",
    "    hv.opts.Curve(color='red') \n",
    "     )\n",
    "\n",
    "# (hv.RGB(np.random.rand(10, 10, 4), group='A') * hv.RGB(np.random.rand(10, 10, 4), group='B')).opts(\n",
    "#     hv.opts.RGB('A', alpha=0.1), hv.opts.RGB('B', alpha=0.5)\n",
    "# )\n",
    "\n",
    "# # ica_rec_error_df\n",
    "# hv.extension('bokeh')\n",
    "\n",
    "# (ica_rec_error_df/X.min().min()).hvplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T11:26:24.283625Z",
     "start_time": "2020-02-04T11:26:23.643364Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes,mark_inset\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[12,6])\n",
    "ica_rec_error_df.sort_index().plot(ax = ax,label=False,marker='o',legend=False)\n",
    "ax.hlines(0.0015,xmin = ica_rec_error_df.index.min(), xmax = ica_rec_error_df.index.max(), color='red')\n",
    "ax.vlines(4,ymin = ica_rec_error_df.min(), ymax = ica_rec_error_df.max(), color='green')\n",
    "ax.set_ylim( [ ica_rec_error_df.min().values[0], ica_rec_error_df.max().values[0]] )\n",
    "\n",
    "axin = inset_axes(ax, width='60%', height='60%', loc=1)\n",
    "ica_rec_error_df.sort_index().plot(ax = axin,label=False,marker='o',legend=False)\n",
    "axin.hlines(0.0015,xmin = ica_rec_error_df.index.min(), xmax = ica_rec_error_df.index.max(), color='red')\n",
    "axin.vlines(4,ymin = ica_rec_error_df.min(), ymax = ica_rec_error_df.max(), color='green')\n",
    "\n",
    "axin.set_ylim([0.00144, 0.00165])\n",
    "axin.set_xlim([2.5, 5])\n",
    "axin.axes.get_xaxis().set_visible(False)\n",
    "axin.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "mark_inset(ax, axin, loc1=2, loc2=3, fc=\"gray\", alpha=0.3, ec=\"0.5\");\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "save_plot('ICA_reconstruction_error_zoom_included', out_format='png',save=save_plots_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:50:36.298750Z",
     "start_time": "2020-02-04T13:50:30.137453Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute ICA\n",
    "\n",
    "ica_n_components= 4\n",
    "\n",
    "# for ica_n_components in range(2,16):\n",
    "ica = decomposition.FastICA(n_components=ica_n_components,random_state=4)\n",
    "S_ = ica.fit_transform(X)  # Reconstruct signals\n",
    "A_ = ica.mixing_           # Get estimated mixing matrix\n",
    "\n",
    "# coefficients matrix\n",
    "# S_ /= S_.std(axis=0)\n",
    "\n",
    "# vector components = signal\n",
    "# A_ -= A_.mean(axis=0)\n",
    "# X ~= S_ x A_.T\n",
    "\n",
    "print(f'ica_n_components : {ica_n_components}')\n",
    "print(f'X : {X.shape}')\n",
    "print(f'A_ : {A_.shape}')\n",
    "print(f'S_ : {S_.shape}')\n",
    "# print(f'square(sum(X-ICA^-1(X)) : {np.sum((X-ica.inverse_transform(S_))**2)}')\n",
    "# print(f'np.norm(X-ICA^-1(X),2) : {np.linalg.norm(X-ica.inverse_transform(S_),2)}')\n",
    "print(((X-ica.inverse_transform(S_)).max()-(X-ica.inverse_transform(S_)).min()).describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:05:46.562192Z",
     "start_time": "2020-02-04T13:05:38.803057Z"
    }
   },
   "outputs": [],
   "source": [
    "## reconstruction error at specific wav ma\n",
    "# h.extension('matplotlib')\n",
    "# plot_wav = 500\n",
    "# out = colorbar_img_shader((X-np.dot(X_pca,pca.components_)-X.mean())[plot_wav])\n",
    "# _ = out.DynamicMap.II.opts(interpolation='bicubic',aspect=2,fig_size=400,alpha=0.7)\n",
    "# out\n",
    "\n",
    "# spectral_df.columns.min(),np.quantile(spectral_df.columns,0.25),np.quantile(spectral_df.columns,0.75),spectral_df.columns.max()\n",
    "# (268, 444, 797.5, 974)\n",
    "\n",
    "hv.extension('matplotlib')\n",
    "    \n",
    "NdLayout = hv.NdLayout(\n",
    "#           difference maps\n",
    "            {f'{plot_wav}nm': df_shader( (X-ica.inverse_transform(S_))[plot_wav],x_sampling=1,y_sampling=1).\\\n",
    "#           only reoconstructed vectors maps\n",
    "#              {plot_wav: df_shader( ica.inverse_transform(S_)[:,plot_wav//2-X.columns[0]],x_sampling=2,y_sampling=2).\\\n",
    "\n",
    "            opts(interpolation='bicubic',aspect=2,alpha=0.8)\n",
    "                 for ind,plot_wav in enumerate([270, 470, 770, 970])}\n",
    "            ,kdims='Wav')\n",
    "\n",
    "(background.opts(cmap='Gray',alpha=1)*NdLayout.cols(2)).opts(fig_size=200,tight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:50:02.592603Z",
     "start_time": "2020-02-04T13:49:59.873165Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.sqrt(((X-ica.inverse_transform(S_)).max()-(X-ica.inverse_transform(S_))).apply(np.square).sum()/X.size).plot(figsize=[20,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:57:26.433936Z",
     "start_time": "2020-02-04T13:57:25.509592Z"
    }
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "\n",
    "vdims = ['Reflectance']\n",
    "kdims=['wavelenght (nm)']\n",
    "width=1100\n",
    "height=500\n",
    "\n",
    "overlay_dict = {f'ICA comp. n.{ind}':hv.Curve((spectral_df.columns.to_numpy(),cmp),vdims=vdims,kdims=kdims) for ind,cmp in enumerate(A_.T) }\n",
    "\n",
    "overlay_dict['Mean'] = hv.Curve((spectral_df.columns.to_numpy(),\n",
    "                  preprocessing.MinMaxScaler().fit_transform(spectral_df.mean().values.reshape(-1, 1)).squeeze()\n",
    "                                ),vdims=vdims,kdims=kdims).opts(line_width=1,line_dash='dashed',color='black',alpha=1)\n",
    "\n",
    "\n",
    "hv.NdOverlay(overlay_dict).opts(\n",
    "                hv.opts.Curve(line_width=4, show_grid=True),\n",
    "                hv.opts.NdOverlay(width=width,height=height)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T14:02:11.036577Z",
     "start_time": "2020-02-04T14:02:10.474330Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('matplotlib')\n",
    "\n",
    "overlay_dict['Mean'].opts(linewidth=1, color='black',alpha =0.25)\n",
    "\n",
    "out = hv.NdOverlay(overlay_dict).opts(\n",
    "                hv.opts.Curve(linewidth=4, show_grid=False),\n",
    "                hv.opts.NdOverlay(fig_size=500, aspect=2.5))\n",
    "out\n",
    "save_plot('ICA_components', out_format='png',save=save_plots_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:07:07.349955Z",
     "start_time": "2020-02-04T13:07:07.345921Z"
    }
   },
   "outputs": [],
   "source": [
    "# [python - Holoviews change datashader colormap - Stack Overflow](https://stackoverflow.com/a/59837074)\n",
    "from holoviews.plotting.util import process_cmap\n",
    "# [Colormaps ‚Äî HoloViews 1.12.7 documentation](http://holoviews.org/user_guide/Colormaps.html)\n",
    "# process_cmap(\"Plasma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T14:52:54.608599Z",
     "start_time": "2020-02-04T14:52:49.677891Z"
    }
   },
   "outputs": [],
   "source": [
    "hv.extension('matplotlib')\n",
    "\n",
    "max_comp = ica_n_components\n",
    "\n",
    "# sampling = 2\n",
    "# cmp = S_[:,2]\n",
    "# out = colorbar_img_shader(cmp[:,np.newaxis],x_sampling=sampling,y_sampling=sampling)\n",
    "# _ = out.DynamicMap.II.opts(interpolation='bilinear',aspect=2,fig_size=400,alpha=1)\n",
    "# out\n",
    "\n",
    "for ind,cmp in enumerate(S_[:,:max_comp].T):\n",
    "    print(f'ICA.{ind:02} min:{cmp.min():10.5} , max:{cmp.max():10.5}, delta:{cmp.max()-cmp.min():10.5}')\n",
    "\n",
    "sampling = 1\n",
    "NdLayout = hv.NdLayout(\n",
    "            {ind: df_shader(cmp[:,np.newaxis],x_sampling=sampling,y_sampling=sampling).\\\n",
    "            opts(interpolation='bicubic',aspect=2,alpha=0.85)\n",
    "                for ind,cmp in enumerate(S_.T)}\n",
    "            ,kdims='ICA Component')\n",
    "\n",
    "out = (background.opts(cmap='Gray',alpha=1)*NdLayout.cols(4)).opts(tight=True, vspace=0.01, hspace=0.01, fig_size=200).cols(2)\n",
    "out\n",
    "# hv.save(out, out_figure_path / 'ICA_components_map.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T15:18:23.422514Z",
     "start_time": "2020-02-04T15:18:17.680624Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hv.extension('bokeh')\n",
    "hv.extension('matplotlib')\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data = S_,\n",
    "    columns=[f'ICA.{ica_c}' for ica_c in range(ica_n_components)]\n",
    ")\n",
    "\n",
    "# # limit to the two \"major components\"\n",
    "# df_ds = hv.Dataset(df[['ICA.1','ICA.2']])\n",
    "df_ds = hv.Dataset(df)\n",
    "\n",
    "sampling = (df.max()-df.min()).describe().mean()/250\n",
    "\n",
    "def local_datashade ( X,\n",
    "                     aggregator=ds.count(),\n",
    "                     cmap=plt.cm.Spectral_r,\n",
    "                     x_sampling=sampling,\n",
    "                     y_sampling=sampling,\n",
    "                     **kwargs):\n",
    "    return hds.datashade(X,aggregator=aggregator,cmap=cmap,x_sampling=x_sampling,y_sampling=y_sampling, **kwargs)\n",
    "\n",
    "point_grid = hv.operation.gridmatrix(df_ds, diagonal_operation=hv.operation.histogram.instance(num_bins=50) ).map(local_datashade, hv.Scatter)\n",
    "# .opts(hv.opts.RGB(interpolation='bilinear',aspect=1,fig_size=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T15:18:23.422514Z",
     "start_time": "2020-02-04T15:18:17.680624Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = point_grid.opts(fig_size=200).opts(hv.opts.RGB(interpolation='bilinear',aspect=1))\n",
    "out\n",
    "# hv.save(out, out_figure_path / 'ICA_coefficients_gridplot_density_C1_C2.png', dpi=200)\n",
    "# hv.save(out, out_figure_path / 'ICA_coefficients_gridplot_density.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold Learning\n",
    "\n",
    "[2.2. Manifold learning ‚Äî scikit-learn 0.22.1 documentation](https://scikit-learn.org/stable/modules/manifold.html)\n",
    "\n",
    "Let's load cached embedding data.\n",
    "Filenames are splitted to retrive some paramters.\n",
    "\n",
    "**WARNING :those methods are time/CPU consuming!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T15:18:24.609499Z",
     "start_time": "2020-02-04T15:18:24.594017Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_name(path):\n",
    "    name=path.stem.split('_')\n",
    "    out_dict = {'path':path,'kind':name[1],'algorithm':name[0]}\n",
    "    for el in name[2:]:\n",
    "        out_dict[el.split('-')[0]]=el.split('-')[1]\n",
    "    return out_dict\n",
    "\n",
    "cache_df = pd.DataFrame.from_dict(\n",
    "                [\n",
    "                split_name(p) for p in pathlib.Path('../models/').glob('*.npy')\n",
    "                ]\n",
    "            )\n",
    "\n",
    "cache_df=cache_df.sort_values(by=cache_df.columns[1:].tolist())\n",
    "\n",
    "cache_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### T-distributed stochastic neighbor (T-SNE) embedding \n",
    "\n",
    "See [sklearn.manifold.TSNE ‚Äî (scikit-learn documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select T-SNE cache and sample 1 random \n",
    "cache_df.query('algorithm == \"tsne\"').dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_df.query('algorithm == \"tsne\"').dropna(axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# set cache file pathlib.Path\n",
    "cached_tsne_dict = cache_df.query('algorithm == \"tsne\"').iloc[1].to_dict() \n",
    "\n",
    "cached_tsne   = cached_tsne_dict['path']\n",
    "pcacomponets  = cached_tsne_dict['path']\n",
    "pcacomponents = cached_tsne_dict['pcacomponents']\n",
    "perplexity    = cached_tsne_dict['perplexity']\n",
    "\n",
    "# load file \n",
    "if cached_tsne.is_file():\n",
    "    print(f'Loading: {cached_tsne}')\n",
    "    X_tsne = np.load(cached_tsne)\n",
    "else:\n",
    "    from sklearn.manifold import TSNE\n",
    "    X_tsne = TSNE(n_components=2).fit_transform(X)\n",
    "    np.save(cached_tsne,X_tsne)    \n",
    "\n",
    "print(X.shape, X_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T16:09:42.569870Z",
     "start_time": "2020-01-16T16:09:42.491002Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hv.extension('matplotlib')\n",
    "\n",
    "vdims = ['label']\n",
    "kdims=['TSNE.0','TSNE.1']\n",
    "\n",
    "perplexities = [5, 30, 50, 100]\n",
    "\n",
    "### RANDOM SAMPPLING\n",
    "# randomize = np.random.choice(X_pca.shape[0], size=1000)\n",
    "# label=labels[randomize]\n",
    "# X = X_pca[randomize,:]\n",
    "\n",
    "overwrite=True\n",
    "label= None\n",
    "X = X_pca\n",
    "print(f'TSN X.shape : {X.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T16:09:42.569870Z",
     "start_time": "2020-01-16T16:09:42.491002Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filenameextra ='_pcacomponents-{}'.format(n_components)\n",
    "\n",
    "def get_tsne(perplexity=None,filename_extra=filenameextra, overwrite=False):\n",
    "    from sklearn.manifold import TSNE\n",
    "    cached_tsne = pathlib.Path('../models/tsne_embedding_perplexity-{}{}.npy'.format(perplexity,filename_extra))\n",
    "    if cached_tsne.is_file():\n",
    "        print(f'Loading: {cached_tsne}')\n",
    "        x_tsne = np.load(cached_tsne)\n",
    "    else:\n",
    "        print(f'file not found: {cached_tsne} - Calculating!')\n",
    "        if overwrite:\n",
    "            print(f'Not found, calculating {cached_tsne}')\n",
    "            x_tsne = TSNE(n_components=2).fit_transform(X)\n",
    "            np.save(cached_tsne,x_tsne)\n",
    "        else:\n",
    "            print(f'overwrite set to {overwrite} stopping')\n",
    "    print('X.shape : {}, X_tsne.shape : {}, perplexity : {}'.format(X.shape, x_tsne.shape,perplexity))\n",
    "    return x_tsne\n",
    "\n",
    "\n",
    "def tsne_to_holocurve(*argv, **kwargs):\n",
    "    xtsne = get_tsne(perplexity=kwargs['perplexity'],filename_extra=kwargs['filename_extra'], overwrite=kwargs['overwrite'])\n",
    "    print(kwargs)\n",
    "    if 'label' in kwargs and kwargs.get('label') is not None:\n",
    "#         print('Label')\n",
    "#         print(np.unique(kwargs.get('label'),return_counts=True))\n",
    "        return hv.Scatter(np.hstack([xtsne ,kwargs.get('label')[:,np.newaxis]]),vdims=kwargs['vdims'],kdims=kwargs['kdims'])\n",
    "    else:\n",
    "        print('Nolabel')\n",
    "        return hv.Scatter(xtsne,kdims=kwargs['kdims'][0], vdims=kwargs['kdims'][1])\n",
    "    \n",
    "curve_dict = {p:tsne_to_holocurve(label=label,\n",
    "                                  perplexity=p,\n",
    "                                  overwrite=overwrite,\n",
    "                                  filename_extra=filenameextra,\n",
    "                                  vdims=vdims,\n",
    "                                  kdims=kdims)\n",
    "              for p in perplexities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T15:55:22.409670Z",
     "start_time": "2020-01-16T15:55:22.379733Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "label= None\n",
    "min_size, max_size = 20, 30\n",
    "\n",
    "if label is not None:\n",
    "    print('labels = ',label.shape)\n",
    "    classes , s = np.unique(label, return_counts=True)\n",
    "    print('classes size:',dict(zip(classes,s)))\n",
    "    # marker size inverse proportional to population size\n",
    "    sizes = ((1-(s-s.min())/(s.max()-s.min()))*(max_size-min_size))+min_size\n",
    "    NdLayout = hv.NdLayout(curve_dict, kdims='perplexity').opts(hv.opts.Scatter(s= [dict(zip(classes,sizes)).get(l) for l in label]),hv.opts.NdLayout(fig_inches=6))\n",
    "    NdLayout.opts(hv.opts.Scatter(color=vdims[0]))\n",
    "else:\n",
    "    print('no labels = ')\n",
    "    NdLayout = hv.NdLayout(curve_dict, kdims='perplexity').opts(hv.opts.Scatter(s=min_size),hv.opts.NdLayout(fig_inches=6))\n",
    "\n",
    "NdLayout.opts(hv.opts.Scatter(alpha=0.25, cmap='Set1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T15:11:44.876035Z",
     "start_time": "2020-01-15T15:11:44.869922Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perplexity = 30\n",
    "# X_tsne = get_tsne(perplexity=perplexity,filename_extra=filenameextra, overwrite=False)\n",
    "X_tsne = curve_dict.get(perplexity).data[:,:-1]\n",
    "print('X.shape : {}, X_tsne.shape : {}, perplexity : {}'.format(X.shape, X_tsne.shape,perplexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UMAP embedding\n",
    "\n",
    "[Basic UMAP Parameters ‚Äî (umap documentation)](https://umap-learn.readthedocs.io/en/latest/parameters.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T14:54:59.376413Z",
     "start_time": "2020-02-04T14:54:57.471405Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import umap\n",
    "\n",
    "vdims = ['label']\n",
    "kdims=['UMAP.0','UMAP.1']\n",
    "\n",
    "X_embedd = X.values\n",
    "\n",
    "filenameextra ='_icacomponents-{}'.format(ica_n_components)\n",
    "\n",
    "### RANDOM SAMPPLING\n",
    "randomize = np.random.choice(X.shape[0], size=100)\n",
    "try:\n",
    "    label=labels[randomize]\n",
    "except NameError:\n",
    "    label=None\n",
    "# X_embedd = X_embedd[randomize,:]\n",
    "\n",
    "overwrite=True\n",
    "label= None\n",
    "\n",
    "# neighbors = np.arange(5,X_embedd.shape[0]//6,X_embedd.shape[0]//20) # 5 to a quarter of the data each 1/8 o the data\n",
    "neighbors     = [   100, 4000, 7000]\n",
    "min_distances = (0.0, 0.5, 0.99)\n",
    "\n",
    "print('X_embedd.shape : ',X_embedd.shape)\n",
    "print('     neighbors : ',neighbors)\n",
    "print(' min_distances : ',min_distances)\n",
    "\n",
    "import itertools\n",
    "print(list(itertools.product(neighbors,min_distances)))\n",
    "\n",
    "def get_umap(neighbors, mindistances,filename_extra=filenameextra, label=None, overwrite=False):\n",
    "\n",
    "    cached_umap = pathlib.Path('../models/umap_embedding_neighbors-{}_mindist-{}{}.npy'.format(neighbors,mindistances,filename_extra))\n",
    "    if cached_umap.is_file():\n",
    "        print(f'Loading: {cached_umap}')\n",
    "        x_umap = np.load(cached_umap)\n",
    "    else:\n",
    "        print(f'file not found: {cached_umap} - Calculating!')\n",
    "        if overwrite: \n",
    "            x_umap = umap.UMAP(n_neighbors=neighbors, min_dist = mindistances).fit_transform(X_embedd)\n",
    "            np.save(cached_umap,x_umap)    \n",
    "        else:\n",
    "            print(f'overwrite set to {overwrite} stopping')\n",
    "    print('X_embedd.shape : {}, x_umap.shape : {}, neighbors : {}, min_dist: {}'.format(X_embedd.shape, x_umap.shape,neighbors, mindistances))\n",
    "    if label is not None:\n",
    "        return hv.Scatter(np.hstack([x_umap,label[:,np.newaxis]]),vdims=vdims,kdims=kdims)\n",
    "    else:\n",
    "        return hv.Scatter(x_umap)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T14:55:00.588391Z",
     "start_time": "2020-02-04T14:55:00.487630Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hv.extension('matplotlib')\n",
    "\n",
    "curve_dict_2D = {(n,d):get_umap(n,d,label=label, overwrite=overwrite) for n in neighbors for d in min_distances}\n",
    "\n",
    "gridspace = hv.GridSpace(curve_dict_2D, kdims=['neighbors, local > global structure', 'minimum distance in representation']).opts(hv.opts.Scatter(s=25,alpha=0.25),hv.opts.GridSpace(fig_inches=16))\n",
    "\n",
    "# if labels.any(): \n",
    "#     gridspace.opts(hv.opts.Scatter(cmap=plt.cm.Spectral_r,c='label'))\n",
    "\n",
    "# gridspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T15:24:55.538495Z",
     "start_time": "2020-02-04T15:24:54.253178Z"
    }
   },
   "outputs": [],
   "source": [
    "gridspace\n",
    "\n",
    "hv.extension('matplotlib')\n",
    "# hv.extension('bokeh')\n",
    "\n",
    "out = hds.dynspread(\n",
    "hds.datashade(gridspace,\n",
    "              aggregator=ds.count(),\n",
    "              cmap=plt.cm.Spectral_r,\n",
    "              x_sampling=0.25,\n",
    "              y_sampling=0.25,\n",
    "             ).\\\n",
    "opts(aspect=1,fig_size=50).opts(hv.opts.RGB(interpolation='bilinear')))\n",
    "out\n",
    "\n",
    "# save_plot(f'UMAP_gridspace_ICA_{ica_n_components}components', out_format='png',save=save_plots_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T15:24:57.743887Z",
     "start_time": "2020-02-04T15:24:57.730498Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "neigh_mindist =  (4000, 0.99)\n",
    "\n",
    "# print('X.shape : {}, X_umap.shape : {}, (n_neighbors,min_dist) : {}'.format(X.shape, X_umap.shape,neigh_mindist))\n",
    "X_umap_scatter = get_umap(neighbors=neigh_mindist[0], mindistances=neigh_mindist[1],filename_extra=filenameextra, label=None, overwrite=True)\n",
    "\n",
    "X_umap = curve_dict_2D.get(neigh_mindist).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T15:24:59.150353Z",
     "start_time": "2020-02-04T15:24:58.697765Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "vdims = ['label']\n",
    "kdims=['UMAP.0','UMAP.1']\n",
    "\n",
    "hv.extension('matplotlib')\n",
    "# hv.extension('bokeh')\n",
    "hds.datashade(X_umap_scatter,\n",
    "              aggregator=ds.count(),\n",
    "              cmap=plt.cm.Spectral_r,\n",
    "              x_sampling=0.4,\n",
    "              y_sampling=0.3,\n",
    "             ).\\\n",
    "opts(interpolation='bilinear',aspect=1,fig_size=200)\n",
    "# opts(height=600,width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T15:25:09.309518Z",
     "start_time": "2020-02-04T15:25:08.421360Z"
    }
   },
   "outputs": [],
   "source": [
    "a = hds.rasterize(X_umap_scatter,aggregator=ds.count(),dynamic=False,x_sampling=0.4, y_sampling=0.3).data.to_dataframe()\n",
    "display(a.describe())\n",
    "\n",
    "ax = a.plot.hist(bins=45)\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "See : \n",
    "- Preprocessing is **very** important for unbalanced features : [6.3. Preprocessing data ‚Äî scikit-learn 1.2.0 documentation](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "- General clustering documentation : [2.3. Clustering ‚Äî scikit-learn 1.2.0 documentation](https://scikit-learn.org/stable/modules/clustering.html)\n",
    "\n",
    "Different kinds of clustering on the same dataset :\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T15:25:27.347442Z",
     "start_time": "2020-02-04T15:25:27.306610Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "from sklearn import preprocessing\n",
    "\n",
    "############################\n",
    "##\n",
    "## data for classificatiom from different sources, choose one of those to see the differences\n",
    "## WARNING : you should have calculated the correspndent X to use it!\n",
    "##\n",
    "\n",
    "# X_classification = X_pca # PCA\n",
    "# X_classification = S_ # ICA\n",
    "# X_classification = X_tsne # tsne embedding\n",
    "X_classification = X_umap # umap embedding\n",
    "\n",
    "n_classification_features = X_classification.shape[1]\n",
    "\n",
    "print('X_classification shape : ',X_classification.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T15:16:46.847222Z",
     "start_time": "2020-02-05T15:16:46.305828Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Scalers : use one of those \n",
    "##########\n",
    "# scaler , preprocessing_type = preprocessing.StandardScaler().fit(X_classification)    , 'StandardScaler'\n",
    "# scaler , preprocessing_type = preprocessing.MinMaxScaler().fit(X_classification) , 'MinMaxScaler'\n",
    "scaler , preprocessing_type = preprocessing.RobustScaler().fit(X_classification) , 'RobustScaler'\n",
    "# scaler, preprocessing_type = preprocessing.FunctionTransformer(lambda x:x), 'None'\n",
    "\n",
    "##########################\n",
    "classifier = 'AgglomerativeClustering'\n",
    "n_clusters = 3\n",
    "aggclustering = cluster.AgglomerativeClustering(linkage='complete',\n",
    "                                                affinity='l2',\n",
    "                                                n_clusters=n_clusters).fit(scaler.transform(X_classification))\n",
    "labels = aggclustering.labels_\n",
    "\n",
    "############################\n",
    "##\n",
    "## those are other clustering algorithms, comment the above and play with those here below\n",
    "##\n",
    "############################\n",
    "\n",
    "# ##########################\n",
    "# classifier = 'K-Means'\n",
    "# n_clusters = 3\n",
    "# # Kmeans estimator instance and Classify scaled data\n",
    "# k_means = cluster.KMeans(n_clusters=n_clusters, random_state=0).fit(scaler.transform(X_classification))\n",
    "# labels = k_means.labels_\n",
    "# print('k_means.inertia_ : ',k_means.inertia_)\n",
    "\n",
    "##########################\n",
    "# classifier = 'DBSCAN'\n",
    "# dbscan = cluster.DBSCAN(eps=0.9, min_samples=5).fit(X_classification)\n",
    "# labels = dbscan.labels_\n",
    "\n",
    "##########################\n",
    "\n",
    "# ##########\n",
    "# classifier = 'HDBSCAN'\n",
    "# import hdbscan\n",
    "# hdbscan = hdbscan.HDBSCAN(\n",
    "#             min_cluster_size=X_classification.shape[0]//2000,\n",
    "#             min_samples=1,\n",
    "#             cluster_selection_epsilon=0.75,\n",
    "# #             allow_single_cluster=False,\n",
    "#             )\n",
    "# hdbscan.fit(scaler.transform(X_classification))\n",
    "# labels = hdbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:23:12.889934Z",
     "start_time": "2020-02-05T14:23:12.836303Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### Statistics of clustering\n",
    "clust_stat_df = pd.DataFrame([{\n",
    "         'size': labels[labels == val].size,\n",
    "         'clAss_mean_fetures_delta': np.mean(np.max(X_classification[labels == val,:],axis=0)-np.min(X_classification[labels == val,:],axis=0)),\n",
    "         'class_mean_features_std':np.max(X_classification[labels == val,:].std(axis=0)),\n",
    "            }\n",
    "            for val in np.unique(labels)],\n",
    "          index=[val for val in np.unique(labels)]).sort_values('size',ascending=False)\n",
    "print(clust_stat_df.shape[0])\n",
    "display(clust_stat_df)\n",
    "display(clust_stat_df[['size']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:23:13.082724Z",
     "start_time": "2020-02-05T14:23:12.895163Z"
    },
    "code_folding": [],
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#####\n",
    "# colors: relabelling the classes using the first centroids values\n",
    "\n",
    "# calculate all the class centers in data space\n",
    "y = X.groupby(labels).mean().values\n",
    "# position of the data feature used to sort lables\n",
    "feature_index = find_nearest(700)\n",
    "# here the sorting index\n",
    "centroids_sorting_index = np.argsort(y[:, feature_index])\n",
    "# here the sorting labels, not the index!!\n",
    "centroids_sorted_labels = np.argsort(centroids_sorting_index) \n",
    "# # use pd.Series.map(dict) di directly change values in place \n",
    "labels = pd.Series(labels).map(dict(zip(np.arange(n_clusters),centroids_sorted_labels))).values\n",
    "print('index for label sort :',feature_index)\n",
    "# print(' features y[:,index] :',y[:, feature_index])\n",
    "# print(centroids_sorting_index)\n",
    "# print(centroids_sorted_labels)\n",
    "print(f'ind:y_feat  > new_index')\n",
    "for i,yf,ni in zip(range(len(y[:, feature_index])),y[:, feature_index],centroids_sorted_labels):\n",
    "    print(f'{i:3}:{yf:.5f} > {ni:>4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holoview import breaks matplotlib inline \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:23:23.445111Z",
     "start_time": "2020-02-05T14:23:13.091541Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\")\n",
    "\n",
    "scatter_df = pd.DataFrame(scaler.transform(X_classification), columns = [f'feature_{x}' for x in range(n_classification_features)])\n",
    "scatter_df['class'] = labels\n",
    "scatter_df['class'] = scatter_df['class'].apply(lambda x: 'classs_{}'.format(x))\n",
    "\n",
    "if X_classification.shape[1] > 2:\n",
    "    g = sns.PairGrid(scatter_df,hue=\"class\",height=4);\n",
    "    # g = g.map_offdiag(sns.kdeplot,lw=1)\n",
    "    g = g.map_offdiag(plt.scatter, s=0.1 , alpha=0.5);\n",
    "    g;\n",
    "else: \n",
    "    plt.figure(figsize=[8,8])\n",
    "    # inverselly scale scatteplot size to clas size \n",
    "    classes , s = np.unique(scatter_df['class'],return_counts=True)\n",
    "    min_size, max_size = 20, 100\n",
    "    sizes = (((s-s.min())/(s.max()-s.min()))*(max_size-min_size))+min_size\n",
    "    sns.scatterplot(x=\"feature_0\", y=\"feature_1\",hue=\"class\", size='class', sizes=dict(zip(classes,sizes)), data=scatter_df, alpha=0.1)\n",
    "\n",
    "# save_plot( f'Classification-scatter-features-n_clusters_{n_clusters}_classifier-{classifier}', out_format='png',save=save_plots_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "\n",
    "print(np.unique(labels))\n",
    "outdf_gdf.loc[spectral_df_nona_index,'R'] = labels\n",
    "outdf_gdf.loc[outdf_gdf['R'] != 2,'R'] = np.nan\n",
    "\n",
    "outdf_gdf[['x','y','R']].hvplot.scatter(x='x',y='y',c='R',\n",
    "                    rasterize=True,aggregator='mean',dynamic=True,\n",
    "                    x_sampling=2,y_sampling=1,cmap='rainbow',cnorm='eq_hist'\n",
    "                    ).opts(height=300,width=800,alpha=1)#*background.opts(cmap='Gray',alpha=.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "heading_collapsed=true tags=[] jp-MarkdownHeadingCollapsed=true"
   },
   "source": [
    "####  AgglomerativeClustering plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T16:57:31.339453Z",
     "start_time": "2020-02-04T16:56:32.120781Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "linkage_array = linkage(aggclustering.children_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T11:05:45.614634Z",
     "start_time": "2020-02-05T11:04:12.591508Z"
    },
    "hidden": true,
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,8])\n",
    "dendrogram(linkage_array,\n",
    "    p=40,  # show only the last p merged clusters\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    orientation='top',\n",
    "    no_labels=False,\n",
    "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=16.,\n",
    ");\n",
    "plt.xlabel('sample index or (cluster size)')\n",
    "plt.ylabel('distance')\n",
    "# plt.ylim([3000,6200])\n",
    "# plt.xlim([-0.5,10.5])\n",
    "# plt.hlines(4500,0,100,color='red')\n",
    "plt.show()\n",
    "save_plot(f'{classifier}_dendrogram.png', out_format='png',save=save_plots_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [scipy.cluster.hierarchy.inconsistent (SciPy v1.10.0)](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.inconsistent.html).\n",
    "\n",
    "Calculate inconsistency statistics on a linkage matrix.\n",
    "\n",
    "This function behaves similarly to the MATLAB(TM) inconsistent function.\n",
    "\n",
    "    Y = inconsistent(Z) returns the inconsistency coefficient for each link of the hierarchical cluster tree Z generated by the linkage function. inconsistent calculates the inconsistency coefficient for each link by comparing its height with the average height of other links at the same level of the hierarchy. The larger the coefficient, the greater the difference between the objects connected by the link. For more information, see Algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T16:57:31.577796Z",
     "start_time": "2020-02-04T16:57:31.348245Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import inconsistent\n",
    "depth = 3\n",
    "incons = inconsistent(linkage_array, depth)\n",
    "incons[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T10:34:32.862981Z",
     "start_time": "2020-02-05T10:32:58.922780Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# see [SciPy Hierarchical Clustering and Dendrogram Tutorial | J√∂rn's Blog](https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/)\n",
    "def fancy_dendrogram(*args, **kwargs):\n",
    "    max_d = kwargs.pop('max_d', None)\n",
    "    if max_d and 'color_threshold' not in kwargs:\n",
    "        kwargs['color_threshold'] = max_d\n",
    "    annotate_above = kwargs.pop('annotate_above', 0)\n",
    "\n",
    "    ddata = dendrogram(*args, **kwargs)\n",
    "\n",
    "    if not kwargs.get('no_plot', False):\n",
    "        plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "        plt.xlabel('sample index or (cluster size)')\n",
    "        plt.ylabel('distance')\n",
    "        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n",
    "            x = 0.5 * sum(i[1:3])\n",
    "            y = d[1]\n",
    "            if y > annotate_above:\n",
    "                plt.plot(x, y, 'o', c=c)\n",
    "                plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -5),\n",
    "                             textcoords='offset points',\n",
    "                             va='top', ha='center')\n",
    "        if max_d:\n",
    "            plt.axhline(y=max_d, c='k')\n",
    "    return ddata\n",
    "\n",
    "plt.figure(figsize=[10,8])\n",
    "fancy_dendrogram(\n",
    "    linkage_array,\n",
    "    truncate_mode='lastp',\n",
    "    p=20,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=30.,\n",
    "    show_contracted=True,\n",
    "    annotate_above=10,  # useful in small plots so annotations don't overlap\n",
    ")\n",
    "plt.ylim([2000,6500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "####  Classification Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:32:02.164400Z",
     "start_time": "2020-02-05T14:32:01.034210Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "\n",
    "vdims = ['component']\n",
    "kdims=['wavelenght']\n",
    "width=1000\n",
    "height=500\n",
    "\n",
    "cm = plt.cm.Spectral_r\n",
    "colors = cm(np.linspace(0,1,len(np.unique(labels))))\n",
    "matplotlib.colors.LinearSegmentedColormap.from_list('Spectral',colors , N=len(np.unique(labels)))\n",
    "cm_cycle = hv.Cycle([cm(c) for c in np.linspace(0,1,len(np.unique(labels)))])\n",
    "hv.Cycle.default_cycles['default_colors'] = cm_cycle\n",
    "\n",
    "overlay_dict = {ind:hv.Curve((spectral_df.columns.to_numpy(),cmp)) for ind,cmp in X.groupby(labels).mean().iterrows()}\n",
    "\n",
    "overlay_dict['Mean'] = hv.Curve((\n",
    "                    spectral_df.columns.to_numpy(),\n",
    "                    spectral_df.mean()\n",
    "                    ),vdims=vdims,kdims=kdims).opts(line_width=0.5,line_dash='dashed',color='black',alpha=1).relabel('mean')\n",
    "\n",
    "hv.NdOverlay(overlay_dict).opts(\n",
    "                hv.opts.Curve(line_width=4,color=cm_cycle),\n",
    "                hv.opts.Curve('mean',color='black'),\n",
    "                hv.opts.NdOverlay(width=width,height=height)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:45.199294Z",
     "start_time": "2020-02-05T14:30:44.920406Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('matplotlib')\n",
    "\n",
    "hv.Cycle.default_cycles['default_colors'] = hv.Cycle([cm(c) for c in np.linspace(0,255,len(np.unique(labels)))])\n",
    "\n",
    "vdims = ['component']\n",
    "kdims=['wavelenght']\n",
    "\n",
    "overlay_dict = {ind:hv.Curve((spectral_df.columns.to_numpy(),cmp)) for ind,cmp in X.groupby(labels).mean().iterrows()}\n",
    "\n",
    "overlay_dict['Mean'] = hv.Curve((\n",
    "                    spectral_df.columns.to_numpy(),\n",
    "                    spectral_df.mean()\n",
    "                    ),vdims=vdims,kdims=kdims).relabel('mean')\n",
    "\n",
    "overlay_dict['Median'] = hv.Curve((\n",
    "                    spectral_df.columns.to_numpy(),\n",
    "                    spectral_df.median()\n",
    "                    ),vdims=vdims,kdims=kdims).relabel('median')\n",
    "\n",
    "out = hv.NdOverlay(overlay_dict).opts(\n",
    "                hv.opts.Curve(linewidth=4, show_grid=False),\n",
    "                hv.opts.Curve('mean',linewidth=6,linestyle=':',color='black',alpha =0.5),\n",
    "                hv.opts.Curve('median',linewidth=3,color='black',alpha =0.5),\n",
    "                hv.opts.NdOverlay(fig_size=400, aspect=2)\n",
    "                )\n",
    "\n",
    "out\n",
    "\n",
    "# # hv.save(out,out_figure_path / f'Spectral-centroids_n_clusters-{n_clusters}_classifier-{classifier}.png', dpi = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:45.199294Z",
     "start_time": "2020-02-05T14:30:44.920406Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## normalised to global mean\n",
    "\n",
    "hv.extension('matplotlib')\n",
    "\n",
    "hv.Cycle.default_cycles['default_colors'] = hv.Cycle([cm(c) for c in np.linspace(0,255,len(np.unique(labels)))])\n",
    "\n",
    "vdims = ['component']\n",
    "kdims=['wavelenght']\n",
    "\n",
    "overlay_dict = {ind:hv.Curve((spectral_df.columns.to_numpy(),cmp)) for ind,cmp in (X.groupby(labels).mean()/spectral_df.median()).iterrows()}\n",
    "\n",
    "out = hv.NdOverlay(overlay_dict).opts(\n",
    "                hv.opts.Curve(linewidth=4, show_grid=False),\n",
    "                hv.opts.NdOverlay(fig_size=400, aspect=2)\n",
    "                )\n",
    "\n",
    "out\n",
    "\n",
    "# # hv.save(out,out_figure_path / f'Spectral-centroids_n_clusters-{n_clusters}_classifier-{classifier}.png', dpi = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T13:21:47.104924Z",
     "start_time": "2020-02-05T13:21:43.211596Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outdf_gdf['labels'] = np.nan\n",
    "outdf_gdf.loc[spectral_df_nona_index,'labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T13:21:47.104924Z",
     "start_time": "2020-02-05T13:21:43.211596Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data layer  on top, transparent \n",
    "# basemap on the bottom, opaque.\n",
    "\n",
    "hv.extension('bokeh')\n",
    "\n",
    "out = background.opts(cmap='Gray')*df_shader(labels,cmap=cm).opts(height=600,width=1000,alpha=0.7)\n",
    "out\n",
    "\n",
    "# hv.save(out,out_figure_path / f'Classification-map_n_clusters-{n_clusters}_classifier-{classifier}.png', dpi = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following uses [hvPlot 0.8.2](https://hvplot.holoviz.org/). \n",
    "\n",
    "From the documentation : \n",
    "\n",
    "\n",
    "A familiar and high-level API for data exploration and visualization hvPlot diagram\n",
    "\n",
    "<img src=\"https://hvplot.holoviz.org/assets/diagram.svg\" width=\"600\">\n",
    "\n",
    "`.hvplot()` is a powerful and interactive Pandas-like `.plot()` API\n",
    "\n",
    "By replacing `.plot()` with `.hvplot()` you get an interactive figure. Try it out below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# basemap on top , transparent \n",
    "# data layer on the bottom, opaque.\n",
    "\n",
    "outdf_gdf[['x','y','labels']].hvplot.scatter(x='x',y='y',c='labels',\n",
    "                    rasterize=True,aggregator='mean',dynamic=False,\n",
    "                    x_sampling=1,y_sampling=1,cmap='Spectral',cnorm='eq_hist'\n",
    "                    ).opts(height=500,width=1000,alpha=1)*\\\n",
    "background.opts(cmap='Gray',alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [sklearn.metrics.silhouette_score (scikit-learn 1.2.0)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)\n",
    "\n",
    "Compute the mean Silhouette Coefficient of all samples.\n",
    "\n",
    "The Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. \n",
    "\n",
    "The Silhouette Coefficient for a sample is $(b - a) / max(a, b)$. To clarify, $b$ is the distance between $a$ sample and the nearest cluster that the sample is not a part of. Note that Silhouette Coefficient is only defined if number of labels is $2 <= n_labels <= n_samples - 1$.\n",
    "\n",
    "This function returns the mean Silhouette Coefficient over all samples. To obtain the values for each sample, use silhouette_samples.\n",
    "\n",
    "The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T17:03:41.128063Z",
     "start_time": "2020-02-04T17:02:11.639344Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.silhouette_score(X_classification, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not that bad actually..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
